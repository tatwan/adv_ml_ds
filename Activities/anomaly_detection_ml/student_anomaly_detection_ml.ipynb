{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6e0e9c-88ae-49c4-a90f-b0cc90c1c21f",
   "metadata": {},
   "source": [
    "# Outlier Detection Using Unsupervised Machine Learning\n",
    "\n",
    "## Introduction to Anomaly Detection with PyOD\n",
    "\n",
    "Welcome to this hands-on lab on **anomaly detection** using the **Python Outlier Detection (PyOD)** library. This notebook will guide you through various machine learning-based approaches to detect anomalies in time series data.\n",
    "\n",
    "### What is Anomaly Detection?\n",
    "\n",
    "**Anomaly detection** (also called outlier detection) is the process of identifying data points, events, or observations that deviate significantly from the majority of the data. Unlike statistical methods that rely on simple thresholds and distributions, machine learning approaches can:\n",
    "\n",
    "- Handle **high-dimensional data** with multiple features\n",
    "- Detect **complex patterns** that simple statistics might miss\n",
    "- Learn **contextual relationships** between features\n",
    "- Adapt to **non-linear patterns** in the data\n",
    "\n",
    "### Types of Anomalies\n",
    "\n",
    "1. **Point Anomalies**: Individual data points that are anomalous (e.g., an unusually high taxi ridership on a random Tuesday)\n",
    "2. **Contextual Anomalies**: Data points that are anomalous in a specific context (e.g., low ridership on a holiday is normal, but low ridership on a regular workday is anomalous)\n",
    "3. **Collective Anomalies**: A collection of related data points that are anomalous together (e.g., a sequence of unusual events)\n",
    "\n",
    "### About PyOD\n",
    "\n",
    "PyOD (Python Outlier Detection) is a comprehensive Python toolkit for detecting outliers in multivariate data. It provides:\n",
    "- **30+ algorithms** from different families (statistical, proximity-based, clustering, neural networks)\n",
    "- A **unified API** similar to scikit-learn\n",
    "- **Scalable** implementations for large datasets\n",
    "- Tools for **model evaluation and comparison**\n",
    "\n",
    "In this lab, we'll explore algorithms from different families and learn when to use each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e9fd2-6a92-414a-8512-cd59d6cec153",
   "metadata": {},
   "source": [
    "# Technical Requirements\n",
    "\n",
    "Before we begin, let's verify that we have all the necessary libraries installed. These libraries form the foundation of our anomaly detection toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9a87b-b531-4c7a-acaf-09886135c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import pandas as pd\n",
    "import pyod \n",
    "import statsmodels\n",
    "\n",
    "print(f'''\n",
    "Matplotlib -> {matplotlib.__version__}\n",
    "pandas -> {pd.__version__}   \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a4e443-e6cb-4a6c-a86d-540464831574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod import version\n",
    "version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f5a99-e47c-44b3-9668-21fabc57ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff46e8e-16e4-4556-95c6-85d1290558f7",
   "metadata": {},
   "source": [
    "## Dataset: NYC Taxi Ridership\n",
    "\n",
    "We'll use NYC taxi ridership data, which contains the number of passengers over time. This is an excellent dataset for learning anomaly detection because:\n",
    "\n",
    "- It has **clear seasonal patterns** (daily, weekly, yearly)\n",
    "- It contains **known anomalies** (holidays, special events)\n",
    "- It's a **real-world use case** (anomaly detection in transportation systems)\n",
    "\n",
    "The dataset has observations every 30 minutes, showing how many taxi passengers were recorded during each time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f85e2b-0941-4bd0-8f53-f95ee15dc731",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"data/nyc_taxi.csv\")\n",
    "nyc_taxi = pd.read_csv(file, \n",
    "                     index_col='timestamp', \n",
    "                     parse_dates=True)\n",
    "nyc_taxi.index.freq = '30min'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822866e6-eb9d-44d4-82c6-ff1d560d737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(outliers, data, method='KNN', halignment='right', valignment='bottom', labels=False):\n",
    "    \"\"\"\n",
    "    Plot time series data with highlighted outliers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    outliers : pandas.DataFrame or pandas.Series\n",
    "        The DataFrame or Series containing the outlier data points.\n",
    "    data : pandas.DataFrame or pandas.Series\n",
    "        The complete time series data.\n",
    "    method : str, default='KNN'\n",
    "        The outlier detection method used, displayed in the plot title.\n",
    "    halignment : str, default='right'\n",
    "        Horizontal alignment for the date labels ('left', 'center', or 'right').\n",
    "    valignment : str, default='bottom'\n",
    "        Vertical alignment for the date labels ('top', 'center', or 'bottom').\n",
    "    labels : bool, default=False\n",
    "        If True, displays date labels for each outlier point.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function shows the plot but does not return any value.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "    data.plot(ax=ax, alpha=0.6)\n",
    "    \n",
    "    # Plot outliers\n",
    "    if labels:\n",
    "        outliers.plot(ax=ax, style='rx', markersize=8, legend=False)\n",
    "        \n",
    "        # Add text labels for each outlier\n",
    "        for idx, value in outliers['value'].items():\n",
    "            ax.text(idx, value, f'{idx.date()}', \n",
    "                   horizontalalignment=halignment, \n",
    "                   verticalalignment=valignment)\n",
    "    else:\n",
    "        outliers.plot(ax=ax, style='rx', legend=False)\n",
    "    \n",
    "    ax.set_title(f'NYC Taxi - {method}')\n",
    "    ax.set_xlabel('date')\n",
    "    ax.set_ylabel('# of passengers')\n",
    "    ax.legend(['nyc taxi', 'outliers'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5733e-bb8f-4046-a039-e240f7a8d4f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize the Data and Known Anomalies\n",
    "\n",
    "Before applying any algorithms, let's visualize the data and identify some **known anomalies**. These known anomalies will serve as our ground truth to evaluate how well our algorithms perform.\n",
    "\n",
    "**Known Anomaly Dates:**\n",
    "- **November 1, 2014**: NYC Marathon\n",
    "- **November 27, 2014**: Thanksgiving\n",
    "- **December 25, 2014**: Christmas\n",
    "- **January 1, 2015**: New Year's Day\n",
    "- **January 27, 2015**: Major snowstorm (Blizzard)\n",
    "\n",
    "These dates should show unusual taxi ridership patterns compared to regular days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a12cb-7b1f-48da-aa9d-18818df6b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d94bbe-adc5-4a60-9af6-4861358dea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_dates =  [\n",
    "        \"2014-11-01\",\n",
    "        \"2014-11-27\",\n",
    "        \"2014-12-25\",\n",
    "        \"2015-01-01\",\n",
    "        \"2015-01-27\"\n",
    "]\n",
    "tx = nyc_taxi.resample('D').mean()\n",
    "known_outliers = tx.loc[nyc_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a39078-a165-4f6f-8c37-720811cbf4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(known_outliers, tx, 'Known Outliers', labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5e02f-9728-4a60-a2fd-8362ffd5f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(df, window_size):\n",
    "    \"\"\"Transform time series data into sliding windows for anomaly detection.\n",
    "    \n",
    "    Creates a DataFrame where each row represents a sliding window of observations,\n",
    "    allowing anomaly detection algorithms to identify unusual temporal patterns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Univariate time series data with values in a single column\n",
    "        window_size (int): Number of time steps in each sliding window\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame where each row is a complete window of observations,\n",
    "                     and columns represent the position within the window\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if not isinstance(window_size, int) or window_size < 1:\n",
    "        raise ValueError(\"Window size must be a positive integer\")\n",
    "    \n",
    "    # Convert DataFrame to 1D array\n",
    "    d = df.values.squeeze()\n",
    "    \n",
    "    # Create sliding windows using numpy's efficient implementation\n",
    "    windows = np.lib.stride_tricks.sliding_window_view(d, window_shape=window_size)[:-1]\n",
    "    \n",
    "    # Create column names for positions within the window\n",
    "    cols = [f'pos_{i}' for i in range(window_size)]\n",
    "    \n",
    "    # Create DataFrame with windows\n",
    "    windows_df = pd.DataFrame(windows, columns=cols, index=df.index[window_size:])\n",
    "    \n",
    "    return windows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc090a6-494a-4875-8e36-b93d594ae151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "def add_time_features(df):\n",
    "    \"\"\"Add time-based and exogenous features to a daily time series dataset.\n",
    "    \n",
    "    Creates features useful for time series anomaly detection by encoding \n",
    "    temporal patterns and external factors that might influence the data.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Time series DataFrame with DatetimeIndex\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Original DataFrame with additional time-based features\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Cyclical encoding for day of week (weekly seasonality)\n",
    "    result['dow_sin'] = np.sin(2 * np.pi * result.index.dayofweek / 7)\n",
    "    result['dow_cos'] = np.cos(2 * np.pi * result.index.dayofweek / 7)\n",
    "    \n",
    "    # Cyclical encoding for month (yearly seasonality)\n",
    "    result['month_sin'] = np.sin(2 * np.pi * result.index.month / 12)\n",
    "    result['month_cos'] = np.cos(2 * np.pi * result.index.month / 12)\n",
    "    \n",
    "    # Keep year for trend analysis\n",
    "    result['year'] = result.index.year\n",
    "    \n",
    "    # Trend feature (simple incremental counter)\n",
    "    result['time'] = np.arange(1, len(result)+1)\n",
    "    \n",
    "    # US holidays - fix the datetime comparison warning\n",
    "    us_holidays = holidays.US(years=result.index.year.unique())\n",
    "    result['is_holiday'] = result.index.map(lambda x: x in us_holidays).astype(int)\n",
    "    \n",
    "    # Weekend feature\n",
    "    result['is_weekend'] = (result.index.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # Month start/end features (can be important for taxi data)\n",
    "    result['is_month_start'] = result.index.is_month_start.astype(int)\n",
    "    result['is_month_end'] = result.index.is_month_end.astype(int)\n",
    "    \n",
    "    # For NYC taxi data: check if it's a typical commuting day\n",
    "    result['is_commuting_day'] = ((~result['is_holiday'].astype(bool)) & \n",
    "                                 (~result['is_weekend'].astype(bool))).astype(int)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdcaeb-bc7a-4f02-9556-93f102eb667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sliding_windows(tx, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78470bba-e5a0-4849-89a6-4cceaf26881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_time_features(tx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eab690c7-2b8c-41a4-91dc-a8b31255f64c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Detecting Outliers Using Distance-Based Algorithms (PyOD)\n",
    "\n",
    "## What are Distance-Based Algorithms?\n",
    "\n",
    "Distance-based algorithms identify anomalies by measuring how **far** data points are from their neighbors. The intuition is simple:\n",
    "\n",
    "- **Normal points** are surrounded by many neighbors (they're in dense regions)\n",
    "- **Anomalous points** are isolated and far from other points (they're in sparse regions)\n",
    "\n",
    "These algorithms work well when anomalies are in low-density regions of the feature space.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 1: K-Nearest Neighbors (KNN)\n",
    "\n",
    "### How KNN Works for Anomaly Detection\n",
    "\n",
    "The **KNN algorithm** identifies outliers based on the distance to their k-nearest neighbors:\n",
    "\n",
    "1. **For each data point**, find its k closest neighbors\n",
    "2. **Calculate a distance metric** (e.g., mean, median, or largest distance to the k neighbors)\n",
    "3. **Assign an anomaly score** based on this distance\n",
    "4. **Points with large distances** to their neighbors are considered outliers\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `n_neighbors` (k): How many neighbors to consider (default: 5)\n",
    "- `method`: How to aggregate distances ('mean', 'median', 'largest')\n",
    "- `contamination`: Expected proportion of outliers in the data (e.g., 0.03 = 3%)\n",
    "\n",
    "**When to use KNN:**\n",
    "- ✅ Simple to understand and interpret\n",
    "- ✅ Works well when outliers are far from normal points\n",
    "- ❌ Computationally expensive for large datasets\n",
    "- ❌ Sensitive to the choice of k\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 2: Local Outlier Factor (LOF)\n",
    "\n",
    "### How LOF Works for Anomaly Detection\n",
    "\n",
    "**LOF** is an improvement over KNN that considers **local density** rather than just distance:\n",
    "\n",
    "1. **Calculate local density** for each point based on distances to its k neighbors\n",
    "2. **Compare each point's density** to the density of its neighbors\n",
    "3. **Compute LOF score**: Ratio of the point's density to its neighbors' average density\n",
    "4. **Points in low-density regions** relative to their neighbors are outliers\n",
    "\n",
    "**Key Differences from KNN:**\n",
    "- LOF considers **relative density** (compares local densities)\n",
    "- KNN uses **absolute distance** (compares distances directly)\n",
    "- LOF handles **varying density regions** better than KNN\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `n_neighbors`: Number of neighbors to use for density estimation (default: 20)\n",
    "- `contamination`: Expected proportion of outliers\n",
    "\n",
    "**When to use LOF:**\n",
    "- ✅ Data has **clusters with different densities**\n",
    "- ✅ Need to detect **local** anomalies (outliers within a cluster)\n",
    "- ✅ More robust than KNN for complex data distributions\n",
    "- ❌ Still computationally expensive for very large datasets\n",
    "\n",
    "---\n",
    "\n",
    "Let's apply both algorithms to detect point outliers in our taxi data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dac0e0-fdd2-42e0-b7d7-297490788dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ebc4157-8de7-4bbc-905e-c39fa2ce5e6c",
   "metadata": {},
   "source": [
    "## Detecting Point Outliers\n",
    "\n",
    "In this section, we apply KNN and LOF directly to the raw time series values. This approach detects **point anomalies** - individual observations that have unusual values compared to other observations, without considering temporal context or trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0a6ae-c3b5-45a8-b1c5-34ee5c33e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(contamination=0.03,\n",
    "          method='mean',\n",
    "          n_neighbors=5)\n",
    "print(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513bf01d-eed4-4b81-8d91-c10cde676163",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LOF(contamination=0.03, \n",
    "          n_neighbors=20) \n",
    "print(lof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e23679-1017-441f-aad2-b2842ba928b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn.fit(tx)\n",
    "lof.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470e8ac-7285-4795-9511-f45409de069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = pd.Series(knn.predict(tx), \n",
    "                      index=tx.index)\n",
    "\n",
    "lof_pred = pd.Series(lof.predict(tx), \n",
    "                      index=tx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d271fe-a287-47dd-b01f-e3d12de0c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of KNN outliers = ', knn_pred.sum())\n",
    "print('Number of LOF outliers = ', lof_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca351b-f530-4f72-baac-449aee3f4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# known outliers dates\n",
    "print(tx.loc[nyc_dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12088cc2-008e-4484-a500-1858eeb39385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from knn_pred\n",
    "knn_outliers = knn_pred[knn_pred == 1]\n",
    "knn_outliers = tx.loc[knn_outliers.index] \n",
    "print(knn_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d504bb0-3c16-4c45-9dc5-30267acfd9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from lof_pred\n",
    "lof_outliers = lof_pred[lof_pred == 1]\n",
    "lof_outliers = tx.loc[lof_outliers.index] \n",
    "print(lof_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6eaf5-a229-4a38-b893-cfd7bbcfeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(knn_outliers, tx, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6aa32e-d40b-4f12-99d3-807abe4d9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(lof_outliers, tx, 'LOF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72555b6-f19b-4677-a6e4-1aef25010860",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(known_outliers, tx, 'Known Outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc3621-9aff-451d-a316-9207f972f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(knn_outliers, \n",
    "              tx, \n",
    "              'KNN',\n",
    "              labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7bcb9b-2ebd-4805-9fdd-966552839de0",
   "metadata": {},
   "source": [
    "## Detecting Outliers After Time Series Decomposition\n",
    "\n",
    "### Why Decompose Time Series?\n",
    "\n",
    "When detecting anomalies in time series, we often face a challenge: **seasonal patterns can mask true anomalies**. For example:\n",
    "- Low taxi ridership on Christmas is **normal** (expected pattern)\n",
    "- Low taxi ridership on a regular Tuesday is **anomalous** (unexpected)\n",
    "\n",
    "**Time Series Decomposition** separates the data into components:\n",
    "1. **Trend**: Long-term increase or decrease\n",
    "2. **Seasonal**: Repeating patterns (daily, weekly, yearly)\n",
    "3. **Residual**: What's left after removing trend and seasonality\n",
    "\n",
    "### The Approach\n",
    "\n",
    "By applying anomaly detection to the **residuals** instead of the raw data:\n",
    "- We **remove expected patterns** (trend and seasonality)\n",
    "- We focus on **genuine deviations** from normal behavior\n",
    "- We detect **contextual anomalies** that would be hidden by seasonal effects\n",
    "\n",
    "**STL (Seasonal and Trend decomposition using Loess)** is a robust method that works well even with missing data and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e9b4c-67b3-4b53-822c-3bc78307fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# For daily data with weekly seasonality\n",
    "stl = STL(tx, seasonal=7)\n",
    "result = stl.fit()\n",
    "residuals = result.resid.dropna().to_frame()\n",
    "\n",
    "residuals.plot(title='Residuals after STL decomposition');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09859e1e-aef6-4128-9426-e113b5e1adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_decomp = KNN(contamination=0.03,\n",
    "          method='mean',\n",
    "          n_neighbors=5)\n",
    "lof_decomp = LOF(contamination=0.03, \n",
    "          n_neighbors=20) \n",
    "\n",
    "# Apply PyOD to residuals\n",
    "knn_r = knn_decomp.fit(residuals)\n",
    "lof_r = lof_decomp.fit(residuals)\n",
    "\n",
    "knn_pred_r = pd.Series(knn_decomp.predict(residuals), \n",
    "                      index=residuals.index)\n",
    "\n",
    "lof_pred_r = pd.Series(lof_decomp.predict(residuals), \n",
    "                      index=residuals.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed8ec6-2f0d-48e9-94ef-abe04a4d0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of KNN outliers after Decomposition = ', knn_pred_r.sum())\n",
    "print('Number of LOF outliers  after Decomposition = ', lof_pred_r.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43521bd3-c05d-4c1c-8c7d-b8379e53717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(known_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cf041-8b10-46be-9d46-e7f0115be411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from knn_pred_r\n",
    "knn_outliers_r = knn_pred_r[knn_pred_r == 1]\n",
    "knn_outliers_r = tx.loc[knn_outliers_r.index] \n",
    "print(knn_outliers_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70605f11-5002-45bd-adc3-5baab104ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from lof_pred_r\n",
    "lof_outliers_r = lof_pred_r[lof_pred_r == 1]\n",
    "lof_outliers_r = tx.loc[lof_outliers_r.index] \n",
    "print(lof_outliers_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4071a304-35af-49ef-ac0c-4875c4f71f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(knn_outliers_r, tx, 'KNN on residuals after decomposition - Original Taxi Plot', labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7796c-bd29-4484-b60e-da98c7d696e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = residuals[knn_pred_r == 1]\n",
    "res.columns = ['value']\n",
    "plot_outliers(res, residuals, 'KNN on residuals after decomposition - Residual Plot', labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899572c3-f222-4b98-9077-df197c9186e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(lof_outliers_r, tx, 'LOF on residuals after decomposition')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4b29366-901f-4b6c-a5d9-db662508af32",
   "metadata": {},
   "source": [
    "## Detecting Contextual Outliers with Sliding Windows\n",
    "\n",
    "### What are Contextual Outliers?\n",
    "\n",
    "A **contextual outlier** is a data point that's anomalous **in its specific context** but might be normal in a different context. For time series:\n",
    "- A value might be normal by itself\n",
    "- But **unusual given recent history**\n",
    "\n",
    "Example: A taxi count of 10,000 might be normal, but if the previous 6 days all had 30,000, then 10,000 becomes anomalous.\n",
    "\n",
    "### The Sliding Window Approach\n",
    "\n",
    "Instead of looking at individual values, we create **windows of consecutive observations**:\n",
    "\n",
    "1. **Create windows**: Each row contains a sequence of values (e.g., the last 7 days)\n",
    "2. **Transform the problem**: Instead of 1 feature (value), we now have 7 features (pos_0, pos_1, ..., pos_6)\n",
    "3. **Apply algorithms**: KNN and LOF now compare entire patterns, not just single values\n",
    "4. **Detect anomalies**: Points with unusual patterns compared to their neighbors are flagged\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ Captures **temporal patterns** and sequences\n",
    "- ✅ Detects anomalies that manifest over **multiple time steps**\n",
    "- ✅ Considers **recent history** when evaluating current values\n",
    "\n",
    "**Window Size Selection:**\n",
    "- Too small: Misses important patterns\n",
    "- Too large: Computationally expensive, may include irrelevant information\n",
    "- Common choices: 7 (weekly pattern), 30 (monthly pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51923e17-d238-42e0-b8b0-8b122e7d1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(df, window_size):\n",
    "    \"\"\"Transform time series data into sliding windows for anomaly detection.\n",
    "    \n",
    "    Creates a DataFrame where each row represents a sliding window of observations,\n",
    "    allowing anomaly detection algorithms to identify unusual temporal patterns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Univariate time series data with values in a single column\n",
    "        window_size (int): Number of time steps in each sliding window\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame where each row is a complete window of observations,\n",
    "                     and columns represent the position within the window\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if not isinstance(window_size, int) or window_size < 1:\n",
    "        raise ValueError(\"Window size must be a positive integer\")\n",
    "    \n",
    "    # Convert DataFrame to 1D array\n",
    "    d = df.values.squeeze()\n",
    "    \n",
    "    # Create sliding windows using numpy's efficient implementation\n",
    "    windows = np.lib.stride_tricks.sliding_window_view(d, window_shape=window_size)[:-1]\n",
    "    \n",
    "    # Create column names for positions within the window\n",
    "    cols = [f'pos_{i}' for i in range(window_size)]\n",
    "    \n",
    "    # Create DataFrame with windows\n",
    "    windows_df = pd.DataFrame(windows, columns=cols, index=df.index[window_size:])\n",
    "    \n",
    "    return windows_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e87c8-65c2-4dca-aae7-4f907a53364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_sw = create_sliding_windows(tx, window_size=7)\n",
    "print(tx_sw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecbd84-5993-4c39-ba48-ec29593015ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_sw = KNN(contamination=0.03,\n",
    "          method='mean',\n",
    "          n_neighbors=5)\n",
    "\n",
    "lof_sw = LOF(contamination=0.03, \n",
    "          n_neighbors=20)\n",
    "\n",
    "\n",
    "knn_sw.fit(tx_sw)\n",
    "lof_sw.fit(tx_sw)\n",
    "\n",
    "knn_pred_sw = pd.Series(knn_sw.predict(tx_sw), \n",
    "                      index=tx_sw.index)\n",
    "\n",
    "lof_pred_sw = pd.Series(lof_sw.predict(tx_sw), \n",
    "                      index=tx_sw.index)\n",
    "\n",
    "print('Number of KNN outliers = ', knn_pred_sw.sum())\n",
    "print('Number of LOF outliers = ', lof_pred_sw.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3043754-1158-4905-a301-9222e7b26b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# known outliers dates\n",
    "print(tx.loc[nyc_dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7c547-30ee-49f0-a930-d402b7e56ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54480fd8-d057-45d7-94a3-9a7c8b21627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from knn_pred_sw\n",
    "knn_outliers_sw = knn_pred_sw[knn_pred_sw == 1]\n",
    "knn_outliers_sw = tx.loc[knn_outliers_sw.index] \n",
    "print(knn_outliers_sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14acdb02-a69d-4f82-af6a-b9e96b8b6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from lof_pred_sw\n",
    "lof_outliers_sw = lof_pred_sw[lof_pred_sw == 1]\n",
    "lof_outliers_sw = tx.loc[lof_outliers_sw.index] \n",
    "print(lof_outliers_sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c5c84-852b-4603-9623-6879519584ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(knn_outliers_sw, tx, 'KNN Sliding Window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385db3e-bf9e-41c8-80b2-23eeb5972f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(lof_outliers_sw, tx, 'LOF Sliding Window')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8592c-1a94-4641-a759-c6b458439124",
   "metadata": {},
   "source": [
    "## Detecting Contextual Outliers with Feature Engineering\n",
    "\n",
    "### The Power of Feature Engineering\n",
    "\n",
    "While sliding windows capture recent history, **feature engineering** adds explicit **contextual information**:\n",
    "\n",
    "- **Temporal features**: Day of week, month, year, time trends\n",
    "- **Cyclical encoding**: Sine/cosine transformations for periodic patterns\n",
    "- **Domain knowledge**: Holidays, weekends, special events\n",
    "\n",
    "### Why Use Both Sliding Windows AND Features?\n",
    "\n",
    "Combining both approaches gives us the best of both worlds:\n",
    "\n",
    "1. **Sliding windows**: Capture raw sequential patterns\n",
    "2. **Engineered features**: Add explicit context (e.g., \"this is a Monday\", \"this is a holiday\")\n",
    "\n",
    "This creates a **rich feature space** where algorithms can learn that:\n",
    "- Low ridership on a holiday is **normal**\n",
    "- Low ridership on a regular Tuesday is **anomalous**\n",
    "\n",
    "### Cyclical Encoding Explained\n",
    "\n",
    "For periodic features like day of week (0-6) or month (1-12), we use **sine and cosine transformations**:\n",
    "\n",
    "- **Why?** Prevents the algorithm from thinking day 6 (Saturday) is \"far\" from day 0 (Sunday)\n",
    "- **How?** Maps the circular nature: sin(2π × day / 7) and cos(2π × day / 7)\n",
    "\n",
    "This approach is more powerful than simply adding raw time features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041d2d8-9a81-4a98-b9f5-d790cbec8525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "def add_time_features(df):\n",
    "    \"\"\"Add time-based and exogenous features to a daily time series dataset.\n",
    "    \n",
    "    Creates features useful for time series anomaly detection by encoding \n",
    "    temporal patterns and external factors that might influence the data.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Time series DataFrame with DatetimeIndex\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Original DataFrame with additional time-based features\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Cyclical encoding for day of week (weekly seasonality)\n",
    "    result['dow_sin'] = np.sin(2 * np.pi * result.index.dayofweek / 7)\n",
    "    result['dow_cos'] = np.cos(2 * np.pi * result.index.dayofweek / 7)\n",
    "    \n",
    "    # Cyclical encoding for month (yearly seasonality)\n",
    "    result['month_sin'] = np.sin(2 * np.pi * result.index.month / 12)\n",
    "    result['month_cos'] = np.cos(2 * np.pi * result.index.month / 12)\n",
    "    \n",
    "    # Keep year for trend analysis\n",
    "    result['year'] = result.index.year\n",
    "    \n",
    "    # Trend feature (simple incremental counter)\n",
    "    result['time'] = np.arange(1, len(result)+1)\n",
    "    \n",
    "    # US holidays - fix the datetime comparison warning\n",
    "    us_holidays = holidays.US(years=result.index.year.unique())\n",
    "    result['is_holiday'] = result.index.map(lambda x: x in us_holidays).astype(int)\n",
    "    \n",
    "    # Weekend feature\n",
    "    result['is_weekend'] = (result.index.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # Month start/end features (can be important for taxi data)\n",
    "    result['is_month_start'] = result.index.is_month_start.astype(int)\n",
    "    result['is_month_end'] = result.index.is_month_end.astype(int)\n",
    "    \n",
    "    # For NYC taxi data: check if it's a typical commuting day\n",
    "    result['is_commuting_day'] = ((~result['is_holiday'].astype(bool)) & \n",
    "                                 (~result['is_weekend'].astype(bool))).astype(int)\n",
    "    \n",
    "    return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb08b5-22ee-42f9-986e-a8236aaa9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with sliding windows for more context\n",
    "windows = create_sliding_windows(tx, 7)\n",
    "features = add_time_features(tx)\n",
    "# Join the windows with the features at the corresponding dates\n",
    "combined = pd.merge(\n",
    "    windows, \n",
    "    features.drop(columns=['value']),  # Exclude the target value\n",
    "    left_index=True, \n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c510d-0b7c-486e-9c10-055a3d1a1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fe = KNN(contamination=0.03,\n",
    "          method='mean',\n",
    "          n_neighbors=5)\n",
    "\n",
    "lof_fe = LOF(contamination=0.03, \n",
    "          n_neighbors=20)\n",
    "\n",
    "\n",
    "knn_fe.fit(combined)\n",
    "lof_fe.fit(combined)\n",
    "\n",
    "knn_pred_fe = pd.Series(knn_fe.predict(combined), \n",
    "                      index=combined.index)\n",
    "\n",
    "lof_pred_fe = pd.Series(lof_fe.predict(combined), \n",
    "                      index=combined.index)\n",
    "\n",
    "# extract the outlier values and dates from knn_pred_fe\n",
    "knn_outliers_fe = knn_pred_fe[knn_pred_fe == 1]\n",
    "knn_outliers_fe = tx.loc[knn_outliers_fe.index] \n",
    "print(knn_outliers_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c628a-394c-431d-a35d-6fe5e9dac5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from lof_pred_fe\n",
    "lof_outliers_fe = lof_pred_fe[lof_pred_fe == 1]\n",
    "lof_outliers_fe = tx.loc[lof_outliers_fe.index] \n",
    "print(lof_outliers_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64692bb2-2a9d-4cd1-8a6a-4858f0acdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(knn_outliers_fe, tx, 'KNN Sliding Window and Feature Engineering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec61fe2-a720-40e3-997a-14907427d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(lof_outliers_fe, tx, 'LOF Sliding Window and Feature Engineering')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00a230-915e-49db-bd54-433c7c0ffab0",
   "metadata": {},
   "source": [
    "## Understanding PyOD's Internal Workflow\n",
    "\n",
    "### From Anomaly Scores to Predictions\n",
    "\n",
    "PyOD algorithms don't just output binary labels (0 = normal, 1 = anomaly). Instead, they follow a sophisticated workflow:\n",
    "\n",
    "1. **Decision Scores**: Each point gets a continuous score indicating \"how anomalous\" it is\n",
    "   - Higher score = more anomalous\n",
    "   - Lower score = more normal\n",
    "\n",
    "2. **Threshold Calculation**: Based on the `contamination` parameter\n",
    "   - If contamination = 0.03, the threshold is set at the 97th percentile\n",
    "   - Points above this threshold are labeled as anomalies\n",
    "\n",
    "3. **Probabilities**: Scores can be converted to probabilities (0 to 1)\n",
    "   - Two methods: 'linear' or 'unify'\n",
    "   - Helps interpret \"how confident\" the model is\n",
    "\n",
    "4. **Confidence Scores**: How certain the model is about its prediction\n",
    "   - Low confidence = borderline case\n",
    "   - High confidence = clear anomaly or clear normal point\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Understanding the internals helps you:\n",
    "- **Adjust sensitivity**: Modify thresholds instead of retraining\n",
    "- **Rank anomalies**: Focus on the most severe cases first\n",
    "- **Debug results**: Understand why certain points were flagged\n",
    "- **Combine models**: Use scores from multiple algorithms\n",
    "\n",
    "Let's explore these concepts with our KNN model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6030a-705c-4886-8275-b37ab1b0b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(contamination=0.03,\n",
    "          method='mean',\n",
    "          n_neighbors=5)\n",
    "\n",
    "knn.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdcdb71-5201-4d31-8187-9744c0546cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores = knn.decision_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939c10f-f327-43b3-a196-20e8f8d3acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores_df = (pd.DataFrame(knn_scores, \n",
    "             index=tx.index, \n",
    "             columns=['score']))\n",
    "\n",
    "knn_scores_df.quantile(0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd3626d-76a4-4dd0-9a9b-87967d6cc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores = knn.decision_function(tx)\n",
    "knn_scores_df = (pd.DataFrame(knn_scores, \n",
    "             index=tx.index, \n",
    "             columns=['score']))\n",
    "knn_scores_df.quantile(0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5f07e-5834-41cc-8b8a-8564f91cec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a315b-506f-4ccc-aefe-3dc040ea91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_scores_df[knn_scores_df['score'] > knn.threshold_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723f432-25af-49e5-8d25-d4868a24012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(tx)*0.03)\n",
    "print(knn_scores_df.nlargest(n, 'score').sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b3346-b24d-4c71-90cd-6786fd233dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores_df.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3f498-4130-401b-9a19-48ee9e6b9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300f6a4-735f-4e8f-8db5-6a831e88b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2474.508333 - 11.745833)*100/(4862.058333-11.745833)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433ef7a-f6eb-48dd-8296-3122feae01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_proba = knn.predict_proba(tx, method='linear')\n",
    "knn_proba_df = (pd.DataFrame(np.round(knn_proba * 100, 3),\n",
    "            index=tx.index,\n",
    "            columns=['Proba_Normal', 'Proba_Anomaly']))\n",
    "print(knn_proba_df.nlargest(n, 'Proba_Anomaly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53fba11-fcf1-4341-9cbf-6f02d9624ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_proba = knn.predict_proba(tx, method='unify')\n",
    "knn_proba_df = (pd.DataFrame(np.round(knn_proba * 100, 3),\n",
    "            index=tx.index,\n",
    "            columns=['Proba_Normal', 'Proba_Anomaly']))\n",
    "print(knn_proba_df.nlargest(n, 'Proba_Anomaly').sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11aa759-da47-41ca-95c2-bd4e01f587a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_conf = knn.predict_confidence(tx)\n",
    "knn_conf_df = (pd.DataFrame(knn_conf,\n",
    "            index=tx.index,\n",
    "            columns=['confidence']))\n",
    "print(knn_conf_df.nsmallest(n, 'confidence').sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368f81a-8535-4e59-8947-2ff0f76c9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# save the model\n",
    "dump(knn, 'knn_outliers.joblib')\n",
    "# load the model\n",
    "knn = load('knn_outliers.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4fa157-6afb-468c-9df4-827a5dab91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109e092-4b11-401b-b3f3-dedd8f5a0678",
   "metadata": {},
   "source": [
    "## Exploring KNN Hyperparameters\n",
    "\n",
    "In this section, we explore how different **method** choices affect KNN's anomaly detection:\n",
    "\n",
    "- **'mean'**: Uses the average distance to k neighbors\n",
    "  - Balanced approach, reduces impact of single extreme neighbor\n",
    "  \n",
    "- **'median'**: Uses the median distance to k neighbors\n",
    "  - More robust to outliers in the neighborhood\n",
    "  \n",
    "- **'largest'**: Uses the maximum distance to k neighbors\n",
    "  - More sensitive, focuses on the farthest neighbor\n",
    "\n",
    "**Try different methods** to see how they affect which points are flagged as anomalies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8815207-313b-44f3-bf2f-f3d8aa3f26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_explore(df, method='mean', contamination=0.03, k=5):\n",
    "    \n",
    "    m = KNN(contamination=contamination,\n",
    "              method=method,\n",
    "              n_neighbors=k)\n",
    "    m.fit(df)\n",
    "    \n",
    "    decision_score = pd.DataFrame(m.decision_scores_, \n",
    "                          index=df.index, columns=['score'])\n",
    "    n = int(len(df)*contamination)\n",
    "    outliers = decision_score.nlargest(n, 'score')\n",
    "    return outliers, m.threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b585a-17ca-4be7-bee4-6b3126b22f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['mean', 'median', 'largest']:\n",
    "    o, t = knn_explore(tx, method=method, contamination=0.05)\n",
    "    print(f'Method= {method}, Threshold= {t}')\n",
    "    print(o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ab71b8c-dcc9-4a6c-b0e0-c7e7168064ff",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Clustering-Based Algorithms (PyOD)\n",
    "\n",
    "## What are Clustering-Based Algorithms?\n",
    "\n",
    "Clustering-based algorithms use a different intuition than distance-based methods:\n",
    "\n",
    "**Core Idea**: \n",
    "- First, **group similar data points** into clusters\n",
    "- Then, identify points that **don't fit well** into any cluster\n",
    "- Points far from cluster centers or in small/sparse clusters are anomalies\n",
    "\n",
    "**Advantage over distance-based methods**:\n",
    "- Can handle **complex data distributions** with multiple modes\n",
    "- More efficient for **large datasets** (cluster first, then evaluate)\n",
    "- Can detect anomalies as **points between clusters**\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm: Cluster-Based Local Outlier Factor (CBLOF)\n",
    "\n",
    "### How CBLOF Works\n",
    "\n",
    "CBLOF combines clustering with outlier scoring:\n",
    "\n",
    "1. **Clustering Phase**: \n",
    "   - Apply clustering (e.g., K-means) to partition the data into `n_clusters` groups\n",
    "   - Classify clusters as **large** (many points) or **small** (few points)\n",
    "\n",
    "2. **Outlier Scoring**:\n",
    "   - For points in **large clusters**: Score based on distance to cluster center\n",
    "   - For points in **small clusters**: Score based on distance to nearest large cluster\n",
    "   - Intuition: Small clusters are likely to contain outliers\n",
    "\n",
    "3. **Final Score Calculation**:\n",
    "   - Uses `alpha` and `beta` parameters to weight the contribution\n",
    "   - `alpha`: Threshold to classify clusters as large vs. small (default: 0.9)\n",
    "   - `beta`: Weight factor for small cluster penalty (default: 5)\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "- **`n_clusters`**: Number of clusters to create\n",
    "  - Too few: Might group anomalies with normal points\n",
    "  - Too many: Normal points might form small clusters\n",
    "  \n",
    "- **`contamination`**: Expected proportion of outliers (e.g., 0.03 = 3%)\n",
    "\n",
    "- **`alpha`**: Ratio to determine large vs. small clusters\n",
    "  - Default 0.9 means: clusters with > 90% of average cluster size are \"large\"\n",
    "  \n",
    "- **`beta`**: Penalty multiplier for small clusters\n",
    "  - Higher values = stronger penalty for points in small clusters\n",
    "\n",
    "### When to Use CBLOF\n",
    "\n",
    "- ✅ Data has **natural groupings** or clusters\n",
    "- ✅ Need better **scalability** than pure distance-based methods\n",
    "- ✅ Outliers are likely to be **isolated** or in sparse regions\n",
    "- ❌ Need to choose the number of clusters (requires domain knowledge)\n",
    "- ❌ Less effective if data doesn't have clear cluster structure\n",
    "\n",
    "Let's see how CBLOF performs on our taxi data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7a7fa-a89e-4b7c-846e-70dca557db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.cblof import CBLOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d9070-0242-4541-b9d4-0777fcbc3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cblof = CBLOF(n_clusters=8, \n",
    "              contamination=0.03,\n",
    "              alpha=0.9,\n",
    "              beta=5)\n",
    "cblof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa259588-f78c-4f3a-b433-ffa1892ddddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cblof.fit(tx)\n",
    "cblof_pred = pd.Series(cblof.predict(tx), \n",
    "                      index=tx.index)\n",
    "print('Number of CBLOF outliers = ', cblof_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678b168-0783-45e1-8be5-d3412c273322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from cblof_pred\n",
    "cblof_outliers = cblof_pred[cblof_pred == 1]\n",
    "cblof_outliers = tx.loc[cblof_outliers.index] \n",
    "print(cblof_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005b3dd-64ab-48a3-a0e8-72fd6550f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(known_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa79732-0e17-4b68-8f17-472446945903",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(cblof_outliers, tx, 'CBLOF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c31ec2-be0c-4164-95df-a2875f4ce981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.utils import evaluate_print\n",
    "# Create binary ground truth labels (1 for known outliers, 0 for normal points)\n",
    "y = tx.index.isin(known_outliers.index).astype(int)\n",
    "evaluate_print('CBLOF', y, cblof.decision_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ca629-3b59-4fdc-911c-943efdc8ee8d",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Probabilistic and Statistical Algorithms\n",
    "\n",
    "## What are Probabilistic Algorithms?\n",
    "\n",
    "Unlike distance or clustering methods, probabilistic algorithms model the **probability distribution** of the data:\n",
    "\n",
    "**Core Idea**:\n",
    "- Learn the **joint probability distribution** of all features\n",
    "- Calculate how **probable** each data point is under this distribution\n",
    "- Points with **low probability** are considered anomalies\n",
    "\n",
    "**Advantages**:\n",
    "- ✅ Theoretically grounded in **statistics and probability theory**\n",
    "- ✅ Can handle **dependencies between features** \n",
    "- ✅ Often **parameter-free** or require minimal tuning\n",
    "- ✅ Fast prediction after training\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 1: COPOD (Copula-Based Outlier Detection)\n",
    "\n",
    "### How COPOD Works\n",
    "\n",
    "COPOD uses **copula theory** from statistics to model dependencies:\n",
    "\n",
    "1. **Marginal Distributions**: \n",
    "   - Model each feature's distribution independently\n",
    "   - Estimate the **empirical cumulative distribution function (ECDF)** for each feature\n",
    "   \n",
    "2. **Tail Probabilities**:\n",
    "   - For each feature, compute left-tail and right-tail probabilities\n",
    "   - Left-tail: P(X ≤ x) - how unusual is this low value?\n",
    "   - Right-tail: P(X ≥ x) - how unusual is this high value?\n",
    "   \n",
    "3. **Copula-Based Combination**:\n",
    "   - Use **copula functions** to combine probabilities across features\n",
    "   - Captures dependencies between features without assuming specific distributions\n",
    "   \n",
    "4. **Outlier Score**:\n",
    "   - Compute final score based on combined tail probabilities\n",
    "   - Lower probability = higher anomaly score\n",
    "\n",
    "### Key Advantages of COPOD\n",
    "\n",
    "- **Parameter-free**: No hyperparameters to tune (except contamination)\n",
    "- **Fast**: Linear time complexity O(n)\n",
    "- **Interpretable**: Based on probability theory\n",
    "- **Distribution-free**: Doesn't assume Gaussian or any specific distribution\n",
    "\n",
    "### When to Use COPOD\n",
    "\n",
    "- ✅ Need a **simple, fast** algorithm with **no tuning**\n",
    "- ✅ Data has **multiple features** with potential dependencies\n",
    "- ✅ Want **probabilistic interpretation** of anomaly scores\n",
    "- ✅ Working with **large datasets** (very efficient)\n",
    "- ❌ Data is highly complex and non-linear (consider deep learning methods)\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 2: ECOD (Empirical Cumulative Distribution Outlier Detection)\n",
    "\n",
    "ECOD is a simpler, even faster variant:\n",
    "\n",
    "### How ECOD Works\n",
    "\n",
    "1. **Empirical CDFs**: Compute ECDF for each feature independently\n",
    "2. **Tail Probabilities**: Calculate left and right tail probabilities\n",
    "3. **Independence Assumption**: Combines probabilities assuming feature independence\n",
    "4. **Outlier Score**: Points in the tails of multiple features score highest\n",
    "\n",
    "### COPOD vs. ECOD\n",
    "\n",
    "| Feature | COPOD | ECOD |\n",
    "|---------|-------|------|\n",
    "| **Speed** | Fast | Even faster |\n",
    "| **Dependencies** | Models feature dependencies | Assumes independence |\n",
    "| **Accuracy** | Higher for correlated features | Good for independent features |\n",
    "| **Complexity** | Uses copula theory | Simple tail probability |\n",
    "\n",
    "Let's apply both algorithms to see how they perform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fad015-34f0-47a3-9d3d-8403400e3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.copod import COPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482212f-4983-4096-bcef-6df71410ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "copod = COPOD(contamination=0.03)\n",
    "copod.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea00716-d818-42af-9d59-302eeb18c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "copod_pred = pd.Series(copod.predict(tx), \n",
    "                      index=tx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c271854-39e0-42a1-a07b-1db5737b0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of COPOD outliers = ', copod_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cce99-a02d-493e-b67c-535736e323ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from copod_pred\n",
    "copod_outliers = copod_pred[copod_pred == 1]\n",
    "copod_outliers = tx.loc[copod_outliers.index]\n",
    "print(copod_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a50f12-63d2-4163-8687-8d6d7bb1e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211f440-e0fd-4952-963a-7679d7768d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(copod_outliers, tx, 'COPOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376dde97-24f5-40fc-88e5-d85b2ed0e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(known_outliers, tx, 'Known')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cfb34f-5133-4289-a54a-c5cd4d2d2e5f",
   "metadata": {},
   "source": [
    "## Additional Probabilistic Methods: ECOD\n",
    "\n",
    "In this section, we explore **ECOD** as another probabilistic approach. Compare its results with COPOD to see how the independence assumption affects anomaly detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc67374-53d0-43fe-9c87-625ddc214580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "# Initialize and fit the ECOD model\n",
    "ecod = ECOD(contamination=0.03)\n",
    "ecod.fit(tx)\n",
    "\n",
    "# Predict outliers\n",
    "ecod_pred = pd.Series(ecod.predict(tx), \n",
    "                     index=tx.index)\n",
    "print('Number of ECOD outliers = ', ecod_pred.sum())\n",
    "\n",
    "# extract the outlier values and dates from ecod_pred\n",
    "ecod_outliers = ecod_pred[ecod_pred == 1]\n",
    "ecod_outliers = tx.loc[ecod_outliers.index]\n",
    "print(ecod_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef7510-b756-4e8a-b353-6f0558ea6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ecod_outliers, tx, 'ECOD')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0fce3fc-8d37-4d11-899d-a79ce6b9fd3e",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Kernel-Based Algorithms (PyOD)\n",
    "\n",
    "## What are Kernel-Based Methods?\n",
    "\n",
    "Kernel-based algorithms use **kernel functions** to transform data into higher-dimensional spaces where separation between normal and anomalous points becomes easier.\n",
    "\n",
    "**Core Concept**:\n",
    "- Data that's **not linearly separable** in the original space...\n",
    "- ...might be **separable in a transformed (kernel) space**\n",
    "- Use the **\"kernel trick\"** to work in high dimensions without explicitly computing the transformation\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm: One-Class SVM (OCSVM)\n",
    "\n",
    "### How One-Class SVM Works\n",
    "\n",
    "Unlike traditional SVM (which learns a boundary between two classes), **One-Class SVM** learns a boundary around **one class** (normal data):\n",
    "\n",
    "1. **Map to Feature Space**:\n",
    "   - Use a kernel function (RBF, linear, polynomial, sigmoid) to implicitly transform data\n",
    "   - In this new space, find a **hyperplane** that separates normal data from the origin\n",
    "\n",
    "2. **Learn the Decision Boundary**:\n",
    "   - Fit a hyperplane that **encloses most normal points**\n",
    "   - Points are allowed to fall outside based on the `nu` parameter\n",
    "   - The hyperplane maximizes margin while allowing some flexibility\n",
    "\n",
    "3. **Classify New Points**:\n",
    "   - Points **inside the boundary** → normal\n",
    "   - Points **outside the boundary** → anomalies\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **`kernel`**: The kernel function to use\n",
    "   - **'rbf' (Radial Basis Function)**: Most popular, handles non-linear patterns\n",
    "     - Creates smooth, circular decision boundaries\n",
    "   - **'linear'**: Fastest, for linearly separable data\n",
    "   - **'poly'**: Polynomial boundaries, can overfit\n",
    "   - **'sigmoid'**: Neural network-like transformation\n",
    "\n",
    "2. **`gamma`**: Kernel coefficient (for 'rbf', 'poly', 'sigmoid')\n",
    "   - **'auto'**: Uses 1 / n_features\n",
    "   - **High gamma**: Model focuses on nearby points (complex, can overfit)\n",
    "   - **Low gamma**: Model considers distant points (simpler, smoother boundary)\n",
    "\n",
    "3. **`nu`**: Upper bound on fraction of outliers and lower bound on fraction of support vectors\n",
    "   - Similar to contamination, but interpreted differently\n",
    "   - Range: (0, 1), typically 0.5\n",
    "   - Higher nu → more flexible boundary, more points classified as outliers\n",
    "\n",
    "### The Importance of Scaling\n",
    "\n",
    "⚠️ **Critical**: OCSVM is **sensitive to feature scales**!\n",
    "\n",
    "- Features with large ranges dominate the distance calculations\n",
    "- **Always standardize/normalize** features before applying OCSVM\n",
    "- Use `StandardScaler` or PyOD's `standardizer` utility\n",
    "\n",
    "### When to Use OCSVM\n",
    "\n",
    "- ✅ Data has **complex, non-linear patterns**\n",
    "- ✅ Need a **robust** decision boundary\n",
    "- ✅ Have **only normal data** for training (one-class problem)\n",
    "- ✅ Can afford to **scale your features** properly\n",
    "- ❌ Have very large datasets (can be slow)\n",
    "- ❌ Need easy interpretability (kernel methods are \"black box\")\n",
    "\n",
    "### Kernel Selection Guide\n",
    "\n",
    "- **Start with 'rbf'**: Works well in most cases\n",
    "- **Try 'linear'**: If data seems linearly separable or for baseline\n",
    "- **Use 'poly'**: If you suspect polynomial relationships\n",
    "- **Avoid 'sigmoid'**: Unless you have specific reasons\n",
    "\n",
    "Let's see how OCSVM performs with different kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e069cc-a459-41eb-96e6-28356b5dcd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.ocsvm import OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ed201-4ceb-4998-afde-f57c457d7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocsvm = OCSVM(contamination=0.03, \n",
    "              kernel='rbf',\n",
    "              gamma='auto',\n",
    "              nu=0.5)\n",
    "ocsvm.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bd301-58ed-49be-988c-93cb6a121f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocsvm_pred = pd.Series(ocsvm.predict(tx), \n",
    "                      index=tx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb09ca-1300-4e7e-a024-3b7a0de07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of OCSVM outliers = ', ocsvm_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db406c-6aaf-46bd-a32d-e424e033be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from ocsvm_pred\n",
    "ocsvm_outliers = ocsvm_pred[ocsvm_pred == 1]\n",
    "ocsvm_outliers = tx.loc[ocsvm_outliers.index] \n",
    "print(ocsvm_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a3741-f497-427b-9f5f-0a6fe3bf63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(known_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb88f4a-e3dc-4975-a207-b3805b2e030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(known_outliers, tx, 'Known')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb03e3-c6d3-44e0-9b84-7e79dfac7701",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ocsvm_outliers, tx, 'OCSVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2585a-4c9f-441f-8a67-61e4894b6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.utils.utility import standardizer\n",
    "\n",
    "ocsvm = OCSVM(contamination=0.03, \n",
    "              kernel='rbf',\n",
    "              gamma='auto',\n",
    "              nu=0.5)\n",
    "\n",
    "scaled = standardizer(tx)\n",
    "ocsvm.fit(scaled)\n",
    "ocsvm_pred_sc = pd.Series(ocsvm.predict(scaled), \n",
    "                      index=tx.index)\n",
    "\n",
    "# extract the outlier values and dates from ocsvm_pred_sc\n",
    "ocsvm_outliers_sc = ocsvm_pred_sc[ocsvm_pred_sc == 1]\n",
    "ocsvm_outliers_sc = tx.loc[ocsvm_outliers_sc.index] \n",
    "print(ocsvm_outliers_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5d8de-6939-4ceb-9905-2da7a04d2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ocsvm_outliers_sc, tx, 'OCSVM after scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3acda-6a12-48a3-b497-e34edbcea536",
   "metadata": {},
   "source": [
    "## Exploring Different Kernels\n",
    "\n",
    "Different kernels can capture different types of patterns. Let's compare how 'linear', 'poly', 'rbf', and 'sigmoid' kernels detect anomalies in our taxi data.\n",
    "\n",
    "**Watch for**:\n",
    "- Which kernel finds the most known anomalies?\n",
    "- Which kernel produces the most stable results?\n",
    "- How do the decision boundaries differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082a9c9-fa24-4e46-9e56-b8f79464e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    ocsvm = OCSVM(contamination=0.03, \n",
    "                  kernel=kernel)\n",
    "    ocsvm.fit(scaled)\n",
    "    ocsvm_pred_sc = pd.Series(ocsvm.predict(scaled), \n",
    "                              index=tx.index, \n",
    "                              name=kernel)\n",
    "    ocsvm_outliers_sc = ocsvm_pred_sc[ocsvm_pred_sc == 1]\n",
    "    ocsvm_outliers_sc = tx.loc[ocsvm_outliers_sc.index]\n",
    "    print(f\"Outliers using {kernel} kerenl: \\n{ocsvm_outliers_sc}\")\n",
    "    #plot_outliers(ocsvm_outliers_sc, tx, f\"OCSVM using {kernel} kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aacaa9-f415-4631-a146-75f35bc415b3",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Ensemble Methods (PyOD)\n",
    "\n",
    "## What are Ensemble Methods?\n",
    "\n",
    "**Ensemble methods** combine multiple models or decision strategies to improve overall performance:\n",
    "\n",
    "**Core Principle**: \"Wisdom of the crowd\"\n",
    "- Individual models might make mistakes\n",
    "- But **combining multiple models** often produces better results\n",
    "- Different models capture different aspects of anomalies\n",
    "\n",
    "**Types of Ensembles**:\n",
    "1. **Bagging**: Train multiple models on random subsets of data\n",
    "2. **Boosting**: Train models sequentially, focusing on hard cases\n",
    "3. **Feature bagging**: Train models on random subsets of features\n",
    "4. **Model combination**: Combine different algorithm types\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 1: Isolation Forest (IForest)\n",
    "\n",
    "### How Isolation Forest Works\n",
    "\n",
    "IForest uses a unique approach based on **isolation** rather than distance or density:\n",
    "\n",
    "**Key Insight**: \n",
    "- **Anomalies are rare and different** → easier to isolate\n",
    "- **Normal points are common and similar** → harder to isolate\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "1. **Build Isolation Trees** (`n_estimators` trees):\n",
    "   - Randomly select a feature\n",
    "   - Randomly select a split value between min and max\n",
    "   - Split the data recursively\n",
    "   - Stop when each point is isolated or tree reaches max depth\n",
    "\n",
    "2. **Measure Path Length**:\n",
    "   - For each point, count how many splits needed to isolate it\n",
    "   - **Anomalies** → isolated quickly → **short paths**\n",
    "   - **Normal points** → many splits needed → **long paths**\n",
    "\n",
    "3. **Aggregate Across Trees**:\n",
    "   - Average path length across all trees\n",
    "   - Normalize to get anomaly score\n",
    "   - Shorter average path = higher anomaly score\n",
    "\n",
    "### Visual Intuition\n",
    "\n",
    "Imagine randomly drawing lines to separate points:\n",
    "- An outlier sitting far from others gets separated in 1-2 cuts ✂️\n",
    "- A normal point in a dense cluster needs many cuts ✂️✂️✂️✂️✂️\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **`n_estimators`**: Number of isolation trees (default: 100)\n",
    "   - More trees → more stable, but slower\n",
    "   - Typical range: 50-200\n",
    "\n",
    "2. **`contamination`**: Expected proportion of outliers\n",
    "\n",
    "3. **`bootstrap`**: Whether to use sampling with replacement\n",
    "   - `True`: Each tree trained on a bootstrapped sample (introduces more diversity)\n",
    "   - `False`: Each tree sees all data (faster, but less diverse)\n",
    "\n",
    "4. **`max_samples`**: Number of samples to draw for each tree\n",
    "   - Default: 256 or size of dataset if smaller\n",
    "   - Larger → more accurate but slower\n",
    "   - Smaller → faster but less stable\n",
    "\n",
    "### Why Isolation Forest is Popular\n",
    "\n",
    "- ✅ **Fast**: Linear time complexity O(n)\n",
    "- ✅ **Scalable**: Works well with large datasets\n",
    "- ✅ **Few parameters**: Easy to use\n",
    "- ✅ **No need for distance/density computation**: Different paradigm\n",
    "- ✅ **Handles high-dimensional data** well\n",
    "- ✅ **Interpretable**: Path length has intuitive meaning\n",
    "\n",
    "### When to Use IForest\n",
    "\n",
    "- ✅ **Large datasets** (very efficient)\n",
    "- ✅ **High-dimensional data** (doesn't suffer from curse of dimensionality)\n",
    "- ✅ Need **fast training and prediction**\n",
    "- ✅ Want a **parameter-free** method (works well with defaults)\n",
    "- ✅ Outliers are **globally anomalous** (not local anomalies)\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 2: Deep Isolation Forest (DIF)\n",
    "\n",
    "### What is DIF?\n",
    "\n",
    "**DIF** is an advanced variant that combines:\n",
    "- Isolation Forest's isolation principle\n",
    "- Deep learning's representation learning\n",
    "\n",
    "### How DIF Improves on IForest\n",
    "\n",
    "1. **Feature Representation**:\n",
    "   - Uses neural networks to learn better feature representations\n",
    "   - Can capture **non-linear relationships** automatically\n",
    "\n",
    "2. **Ensemble of Representations**:\n",
    "   - Each tree uses a different learned representation\n",
    "   - More diverse ensemble → better performance\n",
    "\n",
    "### When to Use DIF\n",
    "\n",
    "- ✅ Data has **complex, non-linear patterns**\n",
    "- ✅ Have **sufficient training data** for deep learning\n",
    "- ✅ Standard IForest doesn't perform well\n",
    "- ❌ Need fast training (DIF is slower than IForest)\n",
    "- ❌ Need interpretability (neural networks are less interpretable)\n",
    "\n",
    "Let's apply both IForest and DIF to our taxi data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5ccf0-fac5-440d-9309-4a46942fcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc6f47-5c1c-4574-bbac-ab07eae19e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest = IForest(contamination=0.03,\n",
    "                 n_estimators=100,\n",
    "                 bootstrap=False,\n",
    "                 random_state=45)\n",
    "iforest.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdb25c-de65-4fef-8258-d7d031c6d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_pred = pd.Series(iforest.predict(tx), \n",
    "                      index=tx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99a794-718f-4559-bc10-6b4d08c569e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of IForest outliers = ', iforest_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43abcdc-6aa0-4d04-9448-fdceb029221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from iforest_pred\n",
    "iforest_outliers = iforest_pred[iforest_pred == 1]\n",
    "iforest_outliers = tx.loc[iforest_outliers.index] \n",
    "print(iforest_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee733d11-43f4-4058-9f8b-61bfeb843f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(iforest_outliers, tx, 'IForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f794c41-fbb5-4de3-b64e-fe0f63c25681",
   "metadata": {},
   "source": [
    "## Advanced Ensemble: Deep Isolation Forest (DIF)\n",
    "\n",
    "Now let's explore DIF, which combines isolation with deep learning for potentially better anomaly detection on complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cc949-ad09-41c3-a891-f5369ff552ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyod.models.dif import DIF\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize and fit the DIF model \n",
    "dif = DIF(contamination=0.03, n_estimators=100)\n",
    "dif.fit(tx)\n",
    "\n",
    "# Get outliers\n",
    "dif_scores = dif.decision_scores_\n",
    "dif_threshold = np.quantile(dif_scores, 0.97)  # Adjust percentile as needed\n",
    "dif_outliers = tx[dif_scores > dif_threshold]\n",
    "print(dif_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166690a-e613-4404-8309-b162928b3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(dif_outliers, tx, 'DIF')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "582feb74-aba6-4f2d-96df-6466f82fe4b5",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Deep Learning (PyOD)\n",
    "\n",
    "## What are Deep Learning-Based Methods?\n",
    "\n",
    "Deep learning brings **neural networks** to anomaly detection, offering powerful capabilities:\n",
    "\n",
    "**Core Idea**:\n",
    "- Use neural networks to **learn compressed representations** of normal data\n",
    "- Points that **reconstruct poorly** from these representations are anomalies\n",
    "- The network learns what \"normal\" looks like automatically\n",
    "\n",
    "**Advantages**:\n",
    "- ✅ **Automatic feature learning**: No manual feature engineering needed\n",
    "- ✅ **Handle complex patterns**: Can model highly non-linear relationships\n",
    "- ✅ **Scalable**: Works with high-dimensional data\n",
    "- ✅ **Flexible**: Can incorporate various architectures\n",
    "\n",
    "**Challenges**:\n",
    "- ❌ **Computationally expensive**: Requires more time and resources\n",
    "- ❌ **Hyperparameter tuning**: Learning rate, epochs, batch size, architecture\n",
    "- ❌ **Less interpretable**: \"Black box\" compared to simpler methods\n",
    "- ❌ **May overfit**: Especially with small datasets\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 1: AutoEncoder\n",
    "\n",
    "### What is an AutoEncoder?\n",
    "\n",
    "An **AutoEncoder** is a neural network trained to **reconstruct its input**:\n",
    "\n",
    "```\n",
    "Input → Encoder → Compressed Representation (Bottleneck) → Decoder → Reconstructed Output\n",
    "```\n",
    "\n",
    "### How AutoEncoders Detect Anomalies\n",
    "\n",
    "1. **Training Phase** (on normal data):\n",
    "   - **Encoder**: Compresses input to a low-dimensional representation (bottleneck)\n",
    "   - **Decoder**: Reconstructs input from the compressed representation\n",
    "   - **Objective**: Minimize reconstruction error for normal points\n",
    "   - The network learns to **encode patterns of normal data**\n",
    "\n",
    "2. **Detection Phase**:\n",
    "   - Pass new data through the trained AutoEncoder\n",
    "   - Calculate **reconstruction error** = |input - reconstructed output|\n",
    "   - **Normal points**: Small reconstruction error (network learned their patterns)\n",
    "   - **Anomalous points**: Large reconstruction error (network never learned these patterns)\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "```\n",
    "Input Layer (n features)\n",
    "    ↓\n",
    "Hidden Layer 1 (decreasing size)\n",
    "    ↓\n",
    "Hidden Layer 2 (even smaller)\n",
    "    ↓\n",
    "Bottleneck (smallest - the compressed representation)\n",
    "    ↓\n",
    "Hidden Layer 3 (increasing size)\n",
    "    ↓\n",
    "Hidden Layer 4 (same size as input)\n",
    "    ↓\n",
    "Output Layer (n features - reconstruction)\n",
    "```\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **`lr` (learning rate)**: How fast the network learns\n",
    "   - Too high: Network doesn't converge, unstable learning\n",
    "   - Too low: Training is very slow\n",
    "   - Typical range: 0.001 - 0.01\n",
    "\n",
    "2. **`epoch_num`**: Number of complete passes through the data\n",
    "   - Too few: Network doesn't learn enough (underfitting)\n",
    "   - Too many: Network memorizes even anomalies (overfitting)\n",
    "   - Start with 10-50, increase if needed\n",
    "   - Use validation to find optimal value\n",
    "\n",
    "3. **`batch_size`**: Number of samples per training update\n",
    "   - Smaller: More updates, noisier gradient, can escape local minima\n",
    "   - Larger: Faster training, more stable gradient, more memory\n",
    "   - Typical values: 16, 32, 64, 128\n",
    "\n",
    "4. **Architecture** (hidden layers, neurons):\n",
    "   - Deeper: Can learn more complex patterns\n",
    "   - Bottleneck size: Should be much smaller than input (compression)\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "**Underfitting Signs**:\n",
    "- High reconstruction error even on normal data\n",
    "- Solution: Train longer (more epochs), add capacity (more layers/neurons)\n",
    "\n",
    "**Overfitting Signs**:\n",
    "- Training error very low, but poor anomaly detection\n",
    "- Network reconstructs anomalies well (memorized them)\n",
    "- Solution: Early stopping, regularization, more training data\n",
    "\n",
    "### When to Use AutoEncoders\n",
    "\n",
    "- ✅ **High-dimensional data** (e.g., images, text, many features)\n",
    "- ✅ Data has **complex, non-linear patterns**\n",
    "- ✅ Have **sufficient normal training data**\n",
    "- ✅ Can afford **longer training time**\n",
    "- ❌ Small datasets (simpler methods might work better)\n",
    "- ❌ Need fast, real-time detection without GPU\n",
    "- ❌ Need interpretable results\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 2: Variational AutoEncoder (VAE)\n",
    "\n",
    "### What is a VAE?\n",
    "\n",
    "**VAE** is a probabilistic variant of AutoEncoder:\n",
    "\n",
    "**Key Differences**:\n",
    "1. **Probabilistic encoding**: Instead of a single compressed representation, VAE learns a **distribution** (mean and variance)\n",
    "2. **Regularization**: Forces the learned distribution to be close to a standard normal distribution\n",
    "3. **Generative**: Can generate new samples (not just reconstruct)\n",
    "\n",
    "### How VAE Detects Anomalies\n",
    "\n",
    "1. **Encoder**: Maps input to a probability distribution (μ, σ)\n",
    "2. **Sampling**: Sample from this distribution (with reparameterization trick)\n",
    "3. **Decoder**: Reconstructs from the sample\n",
    "4. **Anomaly Score**: Combination of:\n",
    "   - Reconstruction error (like AutoEncoder)\n",
    "   - KL divergence (how far the learned distribution is from standard normal)\n",
    "\n",
    "### VAE vs. AutoEncoder\n",
    "\n",
    "| Feature | AutoEncoder | VAE |\n",
    "|---------|-------------|-----|\n",
    "| **Output** | Deterministic | Probabilistic |\n",
    "| **Regularization** | Optional | Built-in (KL divergence) |\n",
    "| **Overfitting** | More prone | More robust |\n",
    "| **Anomaly Score** | Reconstruction error | Reconstruction + KL |\n",
    "| **Complexity** | Simpler | More complex |\n",
    "| **Performance** | Good | Often better, more robust |\n",
    "\n",
    "### When to Use VAE over AutoEncoder\n",
    "\n",
    "- ✅ Want more **robust** anomaly detection\n",
    "- ✅ Need **generative capabilities**\n",
    "- ✅ Have **diverse** anomaly types\n",
    "- ✅ Want built-in **regularization**\n",
    "\n",
    "Let's apply both AutoEncoder and VAE to see how deep learning performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c9013-7b29-46f7-84a8-5b7d97622a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d7607-8019-4be6-bceb-4ea1ffaac172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.auto_encoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43136f35-a6a0-452f-8fef-98a1eac7cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr: Learning rate for optimization\n",
    "# epoch_num: Number of training iterations\n",
    "# batch_size: Number of samples per training batch\n",
    "auto_encoder = AutoEncoder(contamination=0.03,\n",
    "                           lr=0.001,\n",
    "                           epoch_num=10,\n",
    "                           batch_size=32)\n",
    "auto_encoder.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da07a0-199b-4927-a83c-1c5589b51330",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_predicted = pd.Series(auto_encoder.predict(tx), \n",
    "                      index=tx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b118992-5386-4ea2-b8e1-e619fdd58232",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of AutoEncoder outliers = ', ae_predicted.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daee061-8e39-4ebb-81b1-91b83422cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from ae_predicted\n",
    "ae_outliers = ae_predicted[ae_predicted == 1]\n",
    "ae_outliers = tx.loc[ae_outliers.index] \n",
    "print(ae_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9dc2de-1621-43dc-8731-49ba437d4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_scores = pd.Series(auto_encoder.decision_scores_, \n",
    "                      index=tx.index)\n",
    "\n",
    "threshold = auto_encoder.threshold_\n",
    "# or you can do it using quantile \n",
    "threshold = ae_scores.quantile(0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac0bcf-ad55-4b1a-a252-132625476c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_outliers =tx[ae_scores > ae_scores.quantile(0.97)]\n",
    "print(ae_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b3a8f-2573-413f-b668-b30a2234b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ae_outliers, tx, 'AutoEncoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f44a3-4660-4169-8f8e-fa95622e9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "auto_encoder = AutoEncoder(contamination=0.03,\n",
    "                           lr=0.001,\n",
    "                           epoch_num=1000,\n",
    "                           batch_size=32)\n",
    "auto_encoder.fit(tx)\n",
    "ae_predicted = pd.Series(auto_encoder.predict(tx), \n",
    "                      index=tx.index)\n",
    "# extract the outlier values and dates from ae_predicted\n",
    "ae_outliers = ae_predicted[ae_predicted == 1]\n",
    "ae_outliers = tx.loc[ae_outliers.index] \n",
    "print(ae_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6590f6d-c3f4-4fb7-8055-75c40876c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ae_outliers, tx, 'AutoEncoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57bcc3-4493-41df-95f3-958fd2b77eaa",
   "metadata": {},
   "source": [
    "## Advanced Deep Learning: Variational AutoEncoder (VAE)\n",
    "\n",
    "Now let's explore **VAE**, a more sophisticated variant that often provides more robust anomaly detection through its probabilistic approach.\n",
    "\n",
    "**Watch for**:\n",
    "- Does VAE find different anomalies than the standard AutoEncoder?\n",
    "- How does the probabilistic approach affect the results?\n",
    "- Which deep learning method works better for our taxi data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11c65c-ad29-4da3-8cf5-f75217946e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.vae import VAE\n",
    "\n",
    "# lr: Learning rate for optimization\n",
    "# epoch_num: Number of training iterations\n",
    "# batch_size: Number of samples per training batch\n",
    "vae = VAE(contamination=0.03,\n",
    "                           lr=0.001,\n",
    "                           epoch_num=1000,\n",
    "                           batch_size=32)\n",
    "vae.fit(tx)\n",
    "\n",
    "vae_predicted = pd.Series(vae.predict(tx), \n",
    "                      index=tx.index)\n",
    "# extract the outlier values and dates from ae_predicted\n",
    "vae_outliers = vae_predicted[vae_predicted == 1]\n",
    "vae_outliers = tx.loc[vae_outliers.index] \n",
    "print(vae_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfff16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88aa45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Key Takeaways\n",
    "\n",
    "## Algorithms We Explored\n",
    "\n",
    "Throughout this lab, we explored **8+ different anomaly detection algorithms** across multiple families:\n",
    "\n",
    "### 1. **Distance-Based Algorithms**\n",
    "- **KNN (K-Nearest Neighbors)**: Uses average/median/max distance to k neighbors\n",
    "  - Simple, interpretable, but computationally expensive\n",
    "  - Works well when outliers are far from normal points\n",
    "\n",
    "- **LOF (Local Outlier Factor)**: Uses local density comparison\n",
    "  - Better than KNN for data with varying density\n",
    "  - Detects local anomalies within clusters\n",
    "\n",
    "### 2. **Clustering-Based Algorithms**\n",
    "- **CBLOF (Cluster-Based Local Outlier Factor)**: Clusters first, then scores\n",
    "  - Efficient for large datasets\n",
    "  - Works well when data has natural groupings\n",
    "  - Points in small clusters or far from clusters are anomalies\n",
    "\n",
    "### 3. **Probabilistic/Statistical Algorithms**\n",
    "- **COPOD (Copula-Based Outlier Detection)**: Uses copula theory\n",
    "  - Parameter-free (except contamination)\n",
    "  - Fast, handles feature dependencies\n",
    "  - Based on tail probabilities\n",
    "\n",
    "- **ECOD (Empirical Cumulative Outlier Detection)**: Simpler than COPOD\n",
    "  - Even faster, assumes feature independence\n",
    "  - Good baseline method\n",
    "\n",
    "### 4. **Kernel-Based Algorithms**\n",
    "- **OCSVM (One-Class SVM)**: Learns decision boundary using kernels\n",
    "  - Handles non-linear patterns through kernel trick\n",
    "  - Requires feature scaling\n",
    "  - Multiple kernel options (rbf, linear, poly, sigmoid)\n",
    "\n",
    "### 5. **Ensemble Methods**\n",
    "- **Isolation Forest**: Uses isolation trees to detect outliers\n",
    "  - Very fast and scalable\n",
    "  - Works well with high-dimensional data\n",
    "  - Based on \"ease of isolation\" principle\n",
    "\n",
    "- **DIF (Deep Isolation Forest)**: Combines isolation with deep learning\n",
    "  - More sophisticated than IForest\n",
    "  - Can learn better representations\n",
    "\n",
    "### 6. **Deep Learning Methods**\n",
    "- **AutoEncoder**: Neural network that learns to reconstruct normal data\n",
    "  - Detects anomalies through reconstruction error\n",
    "  - Good for complex, high-dimensional patterns\n",
    "  - Requires tuning (learning rate, epochs, batch size)\n",
    "\n",
    "- **VAE (Variational AutoEncoder)**: Probabilistic variant of AutoEncoder\n",
    "  - More robust than standard AutoEncoder\n",
    "  - Uses both reconstruction error and KL divergence\n",
    "  - Better regularization through probabilistic approach\n",
    "\n",
    "---\n",
    "\n",
    "## Techniques for Better Anomaly Detection\n",
    "\n",
    "### 1. **Time Series Decomposition**\n",
    "- Separate trend, seasonality, and residuals\n",
    "- Apply anomaly detection to residuals\n",
    "- Removes seasonal patterns that could hide true anomalies\n",
    "\n",
    "### 2. **Sliding Windows**\n",
    "- Create sequences of consecutive observations\n",
    "- Detect patterns in temporal context\n",
    "- Good for contextual anomalies\n",
    "\n",
    "### 3. **Feature Engineering**\n",
    "- Add temporal features (day of week, month, year)\n",
    "- Use cyclical encoding for periodic features\n",
    "- Incorporate domain knowledge (holidays, weekends)\n",
    "- Combine with sliding windows for rich representations\n",
    "\n",
    "### 4. **Feature Scaling**\n",
    "- Essential for distance-based and kernel methods\n",
    "- Use StandardScaler or PyOD's standardizer\n",
    "- Not needed for tree-based methods (IForest)\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing the Right Algorithm\n",
    "\n",
    "| Scenario | Recommended Algorithm |\n",
    "|----------|----------------------|\n",
    "| **Large dataset, need speed** | Isolation Forest, COPOD, ECOD |\n",
    "| **Small dataset** | KNN, LOF, OCSVM |\n",
    "| **High-dimensional data** | Isolation Forest, AutoEncoder, VAE |\n",
    "| **Complex non-linear patterns** | OCSVM (RBF), AutoEncoder, VAE |\n",
    "| **Varying density clusters** | LOF, CBLOF |\n",
    "| **Need interpretability** | KNN, Isolation Forest |\n",
    "| **Parameter-free method** | COPOD, ECOD |\n",
    "| **Time series with seasonality** | Use decomposition + any algorithm |\n",
    "| **Contextual anomalies** | Sliding windows + feature engineering |\n",
    "\n",
    "---\n",
    "\n",
    "## PyOD Workflow Best Practices\n",
    "\n",
    "1. **Initialize** the model with hyperparameters\n",
    "   ```python\n",
    "   model = KNN(contamination=0.03, n_neighbors=5)\n",
    "   ```\n",
    "\n",
    "2. **Fit** the model on data\n",
    "   ```python\n",
    "   model.fit(X)\n",
    "   ```\n",
    "\n",
    "3. **Get predictions** (binary labels: 0=normal, 1=anomaly)\n",
    "   ```python\n",
    "   predictions = model.predict(X)\n",
    "   ```\n",
    "\n",
    "4. **Get decision scores** (continuous scores)\n",
    "   ```python\n",
    "   scores = model.decision_scores_\n",
    "   # or\n",
    "   scores = model.decision_function(X)\n",
    "   ```\n",
    "\n",
    "5. **Get probabilities** (if needed)\n",
    "   ```python\n",
    "   proba = model.predict_proba(X, method='linear')\n",
    "   ```\n",
    "\n",
    "6. **Save/Load model**\n",
    "   ```python\n",
    "   from joblib import dump, load\n",
    "   dump(model, 'model.joblib')\n",
    "   model = load('model.joblib')\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Parameters\n",
    "\n",
    "- **`contamination`**: Expected proportion of outliers (e.g., 0.03 = 3%)\n",
    "  - Sets the threshold for classification\n",
    "  - Critical parameter for all algorithms\n",
    "  - Should be based on domain knowledge\n",
    "\n",
    "- **`n_neighbors` (KNN, LOF)**: Number of neighbors to consider\n",
    "  - Small k: More sensitive to local variations\n",
    "  - Large k: Smoother, more global view\n",
    "\n",
    "- **`n_clusters` (CBLOF)**: Number of clusters\n",
    "  - Domain-dependent\n",
    "  - Try different values and evaluate\n",
    "\n",
    "- **`kernel`, `gamma`, `nu` (OCSVM)**: Control decision boundary\n",
    "  - Start with 'rbf' kernel and 'auto' gamma\n",
    "  - Adjust based on results\n",
    "\n",
    "- **`n_estimators` (IForest)**: Number of trees\n",
    "  - More trees = more stable, but slower\n",
    "  - 100 is usually sufficient\n",
    "\n",
    "- **`lr`, `epoch_num`, `batch_size` (AutoEncoder, VAE)**: Training parameters\n",
    "  - Require experimentation\n",
    "  - Use validation to find optimal values\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Strategies\n",
    "\n",
    "1. **Use known anomalies** (if available) as ground truth\n",
    "2. **Visual inspection**: Plot detected anomalies\n",
    "3. **Multiple algorithms**: Compare results across different methods\n",
    "4. **Domain expertise**: Verify if detected anomalies make sense\n",
    "5. **PyOD's evaluation utilities**:\n",
    "   ```python\n",
    "   from pyod.utils import evaluate_print\n",
    "   evaluate_print('ModelName', y_true, decision_scores)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "**No single algorithm is best for all cases!**\n",
    "\n",
    "- Start with **simple, fast methods** (COPOD, IForest)\n",
    "- Try **multiple algorithms** and compare\n",
    "- Consider the **specific characteristics** of your data\n",
    "- Combine **domain knowledge** with algorithmic results\n",
    "- Use **feature engineering** to improve detection\n",
    "- **Visualize results** to build intuition\n",
    "\n",
    "**The best approach is often**:\n",
    "1. Try several algorithms\n",
    "2. Ensemble their results (e.g., voting, averaging scores)\n",
    "3. Incorporate domain knowledge\n",
    "4. Iterate based on feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
