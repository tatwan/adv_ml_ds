{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6e0e9c-88ae-49c4-a90f-b0cc90c1c21f",
   "metadata": {},
   "source": [
    "# Outlier Detection Using Unsupervised Machine Learning\n",
    "\n",
    "## Introduction to Anomaly Detection with PyOD\n",
    "\n",
    "Welcome to this hands-on lab on **anomaly detection** using the **Python Outlier Detection (PyOD)** library. This notebook will guide you through various machine learning-based approaches to detect anomalies in time series data.\n",
    "\n",
    "### What is Anomaly Detection?\n",
    "\n",
    "**Anomaly detection** (also called outlier detection) is the process of identifying data points, events, or observations that deviate significantly from the majority of the data. Unlike statistical methods that rely on simple thresholds and distributions, machine learning approaches can:\n",
    "\n",
    "- Handle **high-dimensional data** with multiple features\n",
    "- Detect **complex patterns** that simple statistics might miss\n",
    "- Learn **contextual relationships** between features\n",
    "- Adapt to **non-linear patterns** in the data\n",
    "\n",
    "### Types of Anomalies\n",
    "\n",
    "1. **Point Anomalies**: Individual data points that are anomalous (e.g., an unusually high taxi ridership on a random Tuesday)\n",
    "2. **Contextual Anomalies**: Data points that are anomalous in a specific context (e.g., low ridership on a holiday is normal, but low ridership on a regular workday is anomalous)\n",
    "3. **Collective Anomalies**: A collection of related data points that are anomalous together (e.g., a sequence of unusual events)\n",
    "\n",
    "### About PyOD\n",
    "\n",
    "PyOD (Python Outlier Detection) is a comprehensive Python toolkit for detecting outliers in multivariate data. It provides:\n",
    "- **30+ algorithms** from different families (statistical, proximity-based, clustering, neural networks)\n",
    "- A **unified API** similar to scikit-learn\n",
    "- **Scalable** implementations for large datasets\n",
    "- Tools for **model evaluation and comparison**\n",
    "\n",
    "In this lab, we'll explore algorithms from different families and learn when to use each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e9fd2-6a92-414a-8512-cd59d6cec153",
   "metadata": {},
   "source": [
    "# Technical Requirements\n",
    "\n",
    "Before we begin, let's verify that we have all the necessary libraries installed. These libraries form the foundation of our anomaly detection toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /Users/tarekatwan/Repos/MyWork/Teach/repos/adv_ml_ds/dev1\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m18 packages\u001b[0m \u001b[2min 318ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 33ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.45.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.62.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyod\u001b[0m\u001b[2m==2.0.5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install pyod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad9a87b-b531-4c7a-acaf-09886135c22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matplotlib -> 3.10.7\n",
      "pandas -> 2.3.3   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib \n",
    "import pandas as pd\n",
    "import pyod \n",
    "import statsmodels\n",
    "\n",
    "print(f'''\n",
    "Matplotlib -> {matplotlib.__version__}\n",
    "pandas -> {pd.__version__}   \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a4e443-e6cb-4a6c-a86d-540464831574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyod import version\n",
    "version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53f5a99-e47c-44b3-9668-21fabc57ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff46e8e-16e4-4556-95c6-85d1290558f7",
   "metadata": {},
   "source": [
    "## Dataset: NYC Taxi Ridership\n",
    "\n",
    "We'll use NYC taxi ridership data, which contains the number of passengers over time. This is an excellent dataset for learning anomaly detection because:\n",
    "\n",
    "- It has **clear seasonal patterns** (daily, weekly, yearly)\n",
    "- It contains **known anomalies** (holidays, special events)\n",
    "- It's a **real-world use case** (anomaly detection in transportation systems)\n",
    "\n",
    "The dataset has observations every 30 minutes, showing how many taxi passengers were recorded during each time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f85e2b-0941-4bd0-8f53-f95ee15dc731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Height",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Weight",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c608cf9b-d175-419e-b1d4-03fb37b74d1f",
       "rows": [
        [
         "0",
         "Male",
         "73.847017017515",
         "241.893563180437"
        ],
        [
         "1",
         "Male",
         "68.7819040458903",
         "162.3104725213"
        ],
        [
         "2",
         "Male",
         "74.1101053917849",
         "212.7408555565"
        ],
        [
         "3",
         "Male",
         "71.7309784033377",
         "220.042470303077"
        ],
        [
         "4",
         "Male",
         "69.8817958611153",
         "206.349800623871"
        ],
        [
         "5",
         "Male",
         "67.2530156878065",
         "152.212155757083"
        ],
        [
         "6",
         "Male",
         "68.7850812516616",
         "183.927888604031"
        ],
        [
         "7",
         "Male",
         "68.3485155115879",
         "167.971110489509"
        ],
        [
         "8",
         "Male",
         "67.018949662883",
         "175.92944039571"
        ],
        [
         "9",
         "Male",
         "63.4564939783664",
         "156.399676387112"
        ],
        [
         "10",
         "Male",
         "71.1953822829745",
         "186.604925560358"
        ],
        [
         "11",
         "Male",
         "71.6408051192206",
         "213.741169489411"
        ],
        [
         "12",
         "Male",
         "64.7663291334055",
         "167.127461073476"
        ],
        [
         "13",
         "Male",
         "69.2830700967204",
         "189.446181386738"
        ],
        [
         "14",
         "Male",
         "69.2437322298112",
         "186.434168021239"
        ],
        [
         "15",
         "Male",
         "67.6456197004212",
         "172.186930058117"
        ],
        [
         "16",
         "Male",
         "72.4183166259878",
         "196.028506330482"
        ],
        [
         "17",
         "Male",
         "63.974325721061",
         "172.88347020878"
        ],
        [
         "18",
         "Male",
         "69.6400598997523",
         "185.98395757313"
        ],
        [
         "19",
         "Male",
         "67.9360048540095",
         "182.426648013226"
        ],
        [
         "20",
         "Male",
         "67.9150501938206",
         "174.115929081393"
        ],
        [
         "21",
         "Male",
         "69.4394398680395",
         "197.73142161472"
        ],
        [
         "22",
         "Male",
         "66.1491319608781",
         "149.173566007975"
        ],
        [
         "23",
         "Male",
         "75.2059736142212",
         "228.761780615196"
        ],
        [
         "24",
         "Male",
         "67.8931963386043",
         "162.006651848287"
        ],
        [
         "25",
         "Male",
         "68.1440327982008",
         "192.343976579187"
        ],
        [
         "26",
         "Male",
         "69.0896314289256",
         "184.435174408406"
        ],
        [
         "27",
         "Male",
         "72.8008435165003",
         "206.828189420354"
        ],
        [
         "28",
         "Male",
         "67.4212422817167",
         "175.213922399227"
        ],
        [
         "29",
         "Male",
         "68.4964153568827",
         "154.342638925955"
        ],
        [
         "30",
         "Male",
         "68.6181105502058",
         "187.506843155807"
        ],
        [
         "31",
         "Male",
         "74.0338076216678",
         "212.910225325521"
        ],
        [
         "32",
         "Male",
         "71.5282160355709",
         "195.032243233835"
        ],
        [
         "33",
         "Male",
         "69.1801610995692",
         "205.183621341371"
        ],
        [
         "34",
         "Male",
         "69.577202365402",
         "204.164125484101"
        ],
        [
         "35",
         "Male",
         "70.4009288884762",
         "192.903515074649"
        ],
        [
         "36",
         "Male",
         "69.0761711675356",
         "197.488242598925"
        ],
        [
         "37",
         "Male",
         "67.1935232827228",
         "183.810973232751"
        ],
        [
         "38",
         "Male",
         "65.8073156549306",
         "163.851824878622"
        ],
        [
         "39",
         "Male",
         "64.3041878915595",
         "163.108017147583"
        ],
        [
         "40",
         "Male",
         "67.9743362271967",
         "172.135597406825"
        ],
        [
         "41",
         "Male",
         "72.1894259592134",
         "194.045404898059"
        ],
        [
         "42",
         "Male",
         "65.2703455240394",
         "168.617746204292"
        ],
        [
         "43",
         "Male",
         "66.0901773762725",
         "161.193432596622"
        ],
        [
         "44",
         "Male",
         "67.5103215157138",
         "164.660277264007"
        ],
        [
         "45",
         "Male",
         "70.1047862551571",
         "188.922303151274"
        ],
        [
         "46",
         "Male",
         "68.2518364408672",
         "187.060552163801"
        ],
        [
         "47",
         "Male",
         "72.1727091157973",
         "209.070863390252"
        ],
        [
         "48",
         "Male",
         "69.1798576188774",
         "192.014335412005"
        ],
        [
         "49",
         "Male",
         "72.870360147235",
         "211.34249681964"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10000
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>73.847017</td>\n",
       "      <td>241.893563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>68.781904</td>\n",
       "      <td>162.310473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>74.110105</td>\n",
       "      <td>212.740856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>71.730978</td>\n",
       "      <td>220.042470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>69.881796</td>\n",
       "      <td>206.349801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Female</td>\n",
       "      <td>66.172652</td>\n",
       "      <td>136.777454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Female</td>\n",
       "      <td>67.067155</td>\n",
       "      <td>170.867906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Female</td>\n",
       "      <td>63.867992</td>\n",
       "      <td>128.475319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Female</td>\n",
       "      <td>69.034243</td>\n",
       "      <td>163.852461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Female</td>\n",
       "      <td>61.944246</td>\n",
       "      <td>113.649103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Gender     Height      Weight\n",
       "0       Male  73.847017  241.893563\n",
       "1       Male  68.781904  162.310473\n",
       "2       Male  74.110105  212.740856\n",
       "3       Male  71.730978  220.042470\n",
       "4       Male  69.881796  206.349801\n",
       "...      ...        ...         ...\n",
       "9995  Female  66.172652  136.777454\n",
       "9996  Female  67.067155  170.867906\n",
       "9997  Female  63.867992  128.475319\n",
       "9998  Female  69.034243  163.852461\n",
       "9999  Female  61.944246  113.649103\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = Path(\"data/weight-height.csv\")\n",
    "wh = pd.read_csv(file)\n",
    "wh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eab690c7-2b8c-41a4-91dc-a8b31255f64c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Detecting Outliers Using Distance-Based Algorithms (PyOD)\n",
    "\n",
    "## What are Distance-Based Algorithms?\n",
    "\n",
    "Distance-based algorithms identify anomalies by measuring how **far** data points are from their neighbors. The intuition is simple:\n",
    "\n",
    "- **Normal points** are surrounded by many neighbors (they're in dense regions)\n",
    "- **Anomalous points** are isolated and far from other points (they're in sparse regions)\n",
    "\n",
    "These algorithms work well when anomalies are in low-density regions of the feature space.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 1: K-Nearest Neighbors (KNN)\n",
    "\n",
    "### How KNN Works for Anomaly Detection\n",
    "\n",
    "The **KNN algorithm** identifies outliers based on the distance to their k-nearest neighbors:\n",
    "\n",
    "1. **For each data point**, find its k closest neighbors\n",
    "2. **Calculate a distance metric** (e.g., mean, median, or largest distance to the k neighbors)\n",
    "3. **Assign an anomaly score** based on this distance\n",
    "4. **Points with large distances** to their neighbors are considered outliers\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `n_neighbors` (k): How many neighbors to consider (default: 5)\n",
    "- `method`: How to aggregate distances ('mean', 'median', 'largest')\n",
    "- `contamination`: Expected proportion of outliers in the data (e.g., 0.03 = 3%)\n",
    "\n",
    "**When to use KNN:**\n",
    "- ✅ Simple to understand and interpret\n",
    "- ✅ Works well when outliers are far from normal points\n",
    "- ❌ Computationally expensive for large datasets\n",
    "- ❌ Sensitive to the choice of k\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 2: Local Outlier Factor (LOF)\n",
    "\n",
    "### How LOF Works for Anomaly Detection\n",
    "\n",
    "**LOF** is an improvement over KNN that considers **local density** rather than just distance:\n",
    "\n",
    "1. **Calculate local density** for each point based on distances to its k neighbors\n",
    "2. **Compare each point's density** to the density of its neighbors\n",
    "3. **Compute LOF score**: Ratio of the point's density to its neighbors' average density\n",
    "4. **Points in low-density regions** relative to their neighbors are outliers\n",
    "\n",
    "**Key Differences from KNN:**\n",
    "- LOF considers **relative density** (compares local densities)\n",
    "- KNN uses **absolute distance** (compares distances directly)\n",
    "- LOF handles **varying density regions** better than KNN\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `n_neighbors`: Number of neighbors to use for density estimation (default: 20)\n",
    "- `contamination`: Expected proportion of outliers\n",
    "\n",
    "**When to use LOF:**\n",
    "- ✅ Data has **clusters with different densities**\n",
    "- ✅ Need to detect **local** anomalies (outliers within a cluster)\n",
    "- ✅ More robust than KNN for complex data distributions\n",
    "- ❌ Still computationally expensive for very large datasets\n",
    "\n",
    "---\n",
    "\n",
    "Let's apply both algorithms to detect point outliers in our taxi data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55dac0e0-fdd2-42e0-b7d7-297490788dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ebc4157-8de7-4bbc-905e-c39fa2ce5e6c",
   "metadata": {},
   "source": [
    "## Detecting Point Outliers\n",
    "\n",
    "In this section, we apply KNN and LOF directly to the raw time series values. This approach detects **point anomalies** - individual observations that have unusual values compared to other observations, without considering temporal context or trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71d0a6ae-c3b5-45a8-b1c5-34ee5c33e31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(algorithm='auto', contamination=0.005, leaf_size=30, method='mean',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0)\n"
     ]
    }
   ],
   "source": [
    "knn = KNN(contamination=0.005,\n",
    "          method='mean',\n",
    "          n_neighbors=5)\n",
    "print(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "513bf01d-eed4-4b81-8d91-c10cde676163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOF(algorithm='auto', contamination=0.005, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, novelty=True, p=2)\n"
     ]
    }
   ],
   "source": [
    "lof = LOF(contamination=0.005, \n",
    "          n_neighbors=20) \n",
    "print(lof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3e23679-1017-441f-aad2-b2842ba928b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOF(algorithm='auto', contamination=0.005, leaf_size=30, metric='minkowski',\n",
       "  metric_params=None, n_jobs=1, n_neighbors=20, novelty=True, p=2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(wh[['Height']])\n",
    "lof.fit(wh[['Height']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3470e8ac-7285-4795-9511-f45409de069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = pd.Series(knn.predict(wh[['Height']]))\n",
    "\n",
    "lof_pred = pd.Series(lof.predict(wh[['Height']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09d271fe-a287-47dd-b01f-e3d12de0c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of KNN outliers =  28\n",
      "Number of LOF outliers =  42\n"
     ]
    }
   ],
   "source": [
    "print('Number of KNN outliers = ', knn_pred.sum())\n",
    "print('Number of LOF outliers = ', lof_pred.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ab71b8c-dcc9-4a6c-b0e0-c7e7168064ff",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Clustering-Based Algorithms (PyOD)\n",
    "\n",
    "## What are Clustering-Based Algorithms?\n",
    "\n",
    "Clustering-based algorithms use a different intuition than distance-based methods:\n",
    "\n",
    "**Core Idea**: \n",
    "- First, **group similar data points** into clusters\n",
    "- Then, identify points that **don't fit well** into any cluster\n",
    "- Points far from cluster centers or in small/sparse clusters are anomalies\n",
    "\n",
    "**Advantage over distance-based methods**:\n",
    "- Can handle **complex data distributions** with multiple modes\n",
    "- More efficient for **large datasets** (cluster first, then evaluate)\n",
    "- Can detect anomalies as **points between clusters**\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm: Cluster-Based Local Outlier Factor (CBLOF)\n",
    "\n",
    "### How CBLOF Works\n",
    "\n",
    "CBLOF combines clustering with outlier scoring:\n",
    "\n",
    "1. **Clustering Phase**: \n",
    "   - Apply clustering (e.g., K-means) to partition the data into `n_clusters` groups\n",
    "   - Classify clusters as **large** (many points) or **small** (few points)\n",
    "\n",
    "2. **Outlier Scoring**:\n",
    "   - For points in **large clusters**: Score based on distance to cluster center\n",
    "   - For points in **small clusters**: Score based on distance to nearest large cluster\n",
    "   - Intuition: Small clusters are likely to contain outliers\n",
    "\n",
    "3. **Final Score Calculation**:\n",
    "   - Uses `alpha` and `beta` parameters to weight the contribution\n",
    "   - `alpha`: Threshold to classify clusters as large vs. small (default: 0.9)\n",
    "   - `beta`: Weight factor for small cluster penalty (default: 5)\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "- **`n_clusters`**: Number of clusters to create\n",
    "  - Too few: Might group anomalies with normal points\n",
    "  - Too many: Normal points might form small clusters\n",
    "  \n",
    "- **`contamination`**: Expected proportion of outliers (e.g., 0.03 = 3%)\n",
    "\n",
    "- **`alpha`**: Ratio to determine large vs. small clusters\n",
    "  - Default 0.9 means: clusters with > 90% of average cluster size are \"large\"\n",
    "  \n",
    "- **`beta`**: Penalty multiplier for small clusters\n",
    "  - Higher values = stronger penalty for points in small clusters\n",
    "\n",
    "### When to Use CBLOF\n",
    "\n",
    "- ✅ Data has **natural groupings** or clusters\n",
    "- ✅ Need better **scalability** than pure distance-based methods\n",
    "- ✅ Outliers are likely to be **isolated** or in sparse regions\n",
    "- ❌ Need to choose the number of clusters (requires domain knowledge)\n",
    "- ❌ Less effective if data doesn't have clear cluster structure\n",
    "\n",
    "Let's see how CBLOF performs on our taxi data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4aa7a7fa-a89e-4b7c-846e-70dca557db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.cblof import CBLOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "906d9070-0242-4541-b9d4-0777fcbc3e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBLOF(alpha=0.9, beta=5, check_estimator=False, clustering_estimator=None,\n",
       "   contamination=0.001, n_clusters=8, n_jobs=None, random_state=None,\n",
       "   use_weights=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cblof = CBLOF(n_clusters=8, \n",
    "              contamination=0.001,\n",
    "              alpha=0.9,\n",
    "              beta=5)\n",
    "cblof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa259588-f78c-4f3a-b433-ffa1892ddddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cblof.fit(wh[['Height']])\n",
    "\n",
    "cblof.predict(wh[['Height']]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ca629-3b59-4fdc-911c-943efdc8ee8d",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Probabilistic and Statistical Algorithms\n",
    "\n",
    "## What are Probabilistic Algorithms?\n",
    "\n",
    "Unlike distance or clustering methods, probabilistic algorithms model the **probability distribution** of the data:\n",
    "\n",
    "**Core Idea**:\n",
    "- Learn the **joint probability distribution** of all features\n",
    "- Calculate how **probable** each data point is under this distribution\n",
    "- Points with **low probability** are considered anomalies\n",
    "\n",
    "**Advantages**:\n",
    "- ✅ Theoretically grounded in **statistics and probability theory**\n",
    "- ✅ Can handle **dependencies between features** \n",
    "- ✅ Often **parameter-free** or require minimal tuning\n",
    "- ✅ Fast prediction after training\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 1: COPOD (Copula-Based Outlier Detection)\n",
    "\n",
    "### How COPOD Works\n",
    "\n",
    "COPOD uses **copula theory** from statistics to model dependencies:\n",
    "\n",
    "1. **Marginal Distributions**: \n",
    "   - Model each feature's distribution independently\n",
    "   - Estimate the **empirical cumulative distribution function (ECDF)** for each feature\n",
    "   \n",
    "2. **Tail Probabilities**:\n",
    "   - For each feature, compute left-tail and right-tail probabilities\n",
    "   - Left-tail: P(X ≤ x) - how unusual is this low value?\n",
    "   - Right-tail: P(X ≥ x) - how unusual is this high value?\n",
    "   \n",
    "3. **Copula-Based Combination**:\n",
    "   - Use **copula functions** to combine probabilities across features\n",
    "   - Captures dependencies between features without assuming specific distributions\n",
    "   \n",
    "4. **Outlier Score**:\n",
    "   - Compute final score based on combined tail probabilities\n",
    "   - Lower probability = higher anomaly score\n",
    "\n",
    "### Key Advantages of COPOD\n",
    "\n",
    "- **Parameter-free**: No hyperparameters to tune (except contamination)\n",
    "- **Fast**: Linear time complexity O(n)\n",
    "- **Interpretable**: Based on probability theory\n",
    "- **Distribution-free**: Doesn't assume Gaussian or any specific distribution\n",
    "\n",
    "### When to Use COPOD\n",
    "\n",
    "- ✅ Need a **simple, fast** algorithm with **no tuning**\n",
    "- ✅ Data has **multiple features** with potential dependencies\n",
    "- ✅ Want **probabilistic interpretation** of anomaly scores\n",
    "- ✅ Working with **large datasets** (very efficient)\n",
    "- ❌ Data is highly complex and non-linear (consider deep learning methods)\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 2: ECOD (Empirical Cumulative Distribution Outlier Detection)\n",
    "\n",
    "ECOD is a simpler, even faster variant:\n",
    "\n",
    "### How ECOD Works\n",
    "\n",
    "1. **Empirical CDFs**: Compute ECDF for each feature independently\n",
    "2. **Tail Probabilities**: Calculate left and right tail probabilities\n",
    "3. **Independence Assumption**: Combines probabilities assuming feature independence\n",
    "4. **Outlier Score**: Points in the tails of multiple features score highest\n",
    "\n",
    "### COPOD vs. ECOD\n",
    "\n",
    "| Feature | COPOD | ECOD |\n",
    "|---------|-------|------|\n",
    "| **Speed** | Fast | Even faster |\n",
    "| **Dependencies** | Models feature dependencies | Assumes independence |\n",
    "| **Accuracy** | Higher for correlated features | Good for independent features |\n",
    "| **Complexity** | Uses copula theory | Simple tail probability |\n",
    "\n",
    "Let's apply both algorithms to see how they perform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fad015-34f0-47a3-9d3d-8403400e3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.copod import COPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482212f-4983-4096-bcef-6df71410ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "copod = COPOD(contamination=0.03)\n",
    "copod.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea00716-d818-42af-9d59-302eeb18c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "copod_pred = pd.Series(copod.predict(tx), \n",
    "                      index=tx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c271854-39e0-42a1-a07b-1db5737b0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of COPOD outliers = ', copod_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cce99-a02d-493e-b67c-535736e323ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from copod_pred\n",
    "copod_outliers = copod_pred[copod_pred == 1]\n",
    "copod_outliers = tx.loc[copod_outliers.index]\n",
    "print(copod_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a50f12-63d2-4163-8687-8d6d7bb1e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211f440-e0fd-4952-963a-7679d7768d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(copod_outliers, tx, 'COPOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376dde97-24f5-40fc-88e5-d85b2ed0e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(known_outliers, tx, 'Known')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cfb34f-5133-4289-a54a-c5cd4d2d2e5f",
   "metadata": {},
   "source": [
    "## Additional Probabilistic Methods: ECOD\n",
    "\n",
    "In this section, we explore **ECOD** as another probabilistic approach. Compare its results with COPOD to see how the independence assumption affects anomaly detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc67374-53d0-43fe-9c87-625ddc214580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "# Initialize and fit the ECOD model\n",
    "ecod = ECOD(contamination=0.03)\n",
    "ecod.fit(tx)\n",
    "\n",
    "# Predict outliers\n",
    "ecod_pred = pd.Series(ecod.predict(tx), \n",
    "                     index=tx.index)\n",
    "print('Number of ECOD outliers = ', ecod_pred.sum())\n",
    "\n",
    "# extract the outlier values and dates from ecod_pred\n",
    "ecod_outliers = ecod_pred[ecod_pred == 1]\n",
    "ecod_outliers = tx.loc[ecod_outliers.index]\n",
    "print(ecod_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef7510-b756-4e8a-b353-6f0558ea6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ecod_outliers, tx, 'ECOD')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0fce3fc-8d37-4d11-899d-a79ce6b9fd3e",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Kernel-Based Algorithms (PyOD)\n",
    "\n",
    "## What are Kernel-Based Methods?\n",
    "\n",
    "Kernel-based algorithms use **kernel functions** to transform data into higher-dimensional spaces where separation between normal and anomalous points becomes easier.\n",
    "\n",
    "**Core Concept**:\n",
    "- Data that's **not linearly separable** in the original space...\n",
    "- ...might be **separable in a transformed (kernel) space**\n",
    "- Use the **\"kernel trick\"** to work in high dimensions without explicitly computing the transformation\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm: One-Class SVM (OCSVM)\n",
    "\n",
    "### How One-Class SVM Works\n",
    "\n",
    "Unlike traditional SVM (which learns a boundary between two classes), **One-Class SVM** learns a boundary around **one class** (normal data):\n",
    "\n",
    "1. **Map to Feature Space**:\n",
    "   - Use a kernel function (RBF, linear, polynomial, sigmoid) to implicitly transform data\n",
    "   - In this new space, find a **hyperplane** that separates normal data from the origin\n",
    "\n",
    "2. **Learn the Decision Boundary**:\n",
    "   - Fit a hyperplane that **encloses most normal points**\n",
    "   - Points are allowed to fall outside based on the `nu` parameter\n",
    "   - The hyperplane maximizes margin while allowing some flexibility\n",
    "\n",
    "3. **Classify New Points**:\n",
    "   - Points **inside the boundary** → normal\n",
    "   - Points **outside the boundary** → anomalies\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **`kernel`**: The kernel function to use\n",
    "   - **'rbf' (Radial Basis Function)**: Most popular, handles non-linear patterns\n",
    "     - Creates smooth, circular decision boundaries\n",
    "   - **'linear'**: Fastest, for linearly separable data\n",
    "   - **'poly'**: Polynomial boundaries, can overfit\n",
    "   - **'sigmoid'**: Neural network-like transformation\n",
    "\n",
    "2. **`gamma`**: Kernel coefficient (for 'rbf', 'poly', 'sigmoid')\n",
    "   - **'auto'**: Uses 1 / n_features\n",
    "   - **High gamma**: Model focuses on nearby points (complex, can overfit)\n",
    "   - **Low gamma**: Model considers distant points (simpler, smoother boundary)\n",
    "\n",
    "3. **`nu`**: Upper bound on fraction of outliers and lower bound on fraction of support vectors\n",
    "   - Similar to contamination, but interpreted differently\n",
    "   - Range: (0, 1), typically 0.5\n",
    "   - Higher nu → more flexible boundary, more points classified as outliers\n",
    "\n",
    "### The Importance of Scaling\n",
    "\n",
    "⚠️ **Critical**: OCSVM is **sensitive to feature scales**!\n",
    "\n",
    "- Features with large ranges dominate the distance calculations\n",
    "- **Always standardize/normalize** features before applying OCSVM\n",
    "- Use `StandardScaler` or PyOD's `standardizer` utility\n",
    "\n",
    "### When to Use OCSVM\n",
    "\n",
    "- ✅ Data has **complex, non-linear patterns**\n",
    "- ✅ Need a **robust** decision boundary\n",
    "- ✅ Have **only normal data** for training (one-class problem)\n",
    "- ✅ Can afford to **scale your features** properly\n",
    "- ❌ Have very large datasets (can be slow)\n",
    "- ❌ Need easy interpretability (kernel methods are \"black box\")\n",
    "\n",
    "### Kernel Selection Guide\n",
    "\n",
    "- **Start with 'rbf'**: Works well in most cases\n",
    "- **Try 'linear'**: If data seems linearly separable or for baseline\n",
    "- **Use 'poly'**: If you suspect polynomial relationships\n",
    "- **Avoid 'sigmoid'**: Unless you have specific reasons\n",
    "\n",
    "Let's see how OCSVM performs with different kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e069cc-a459-41eb-96e6-28356b5dcd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.ocsvm import OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ed201-4ceb-4998-afde-f57c457d7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocsvm = OCSVM(contamination=0.03, \n",
    "              kernel='rbf',\n",
    "              gamma='auto',\n",
    "              nu=0.5)\n",
    "ocsvm.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bd301-58ed-49be-988c-93cb6a121f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocsvm_pred = pd.Series(ocsvm.predict(tx), \n",
    "                      index=tx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb09ca-1300-4e7e-a024-3b7a0de07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of OCSVM outliers = ', ocsvm_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db406c-6aaf-46bd-a32d-e424e033be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from ocsvm_pred\n",
    "ocsvm_outliers = ocsvm_pred[ocsvm_pred == 1]\n",
    "ocsvm_outliers = tx.loc[ocsvm_outliers.index] \n",
    "print(ocsvm_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a3741-f497-427b-9f5f-0a6fe3bf63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(known_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb88f4a-e3dc-4975-a207-b3805b2e030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(known_outliers, tx, 'Known')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb03e3-c6d3-44e0-9b84-7e79dfac7701",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ocsvm_outliers, tx, 'OCSVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2585a-4c9f-441f-8a67-61e4894b6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.utils.utility import standardizer\n",
    "\n",
    "ocsvm = OCSVM(contamination=0.03, \n",
    "              kernel='rbf',\n",
    "              gamma='auto',\n",
    "              nu=0.5)\n",
    "\n",
    "scaled = standardizer(tx)\n",
    "ocsvm.fit(scaled)\n",
    "ocsvm_pred_sc = pd.Series(ocsvm.predict(scaled), \n",
    "                      index=tx.index)\n",
    "\n",
    "# extract the outlier values and dates from ocsvm_pred_sc\n",
    "ocsvm_outliers_sc = ocsvm_pred_sc[ocsvm_pred_sc == 1]\n",
    "ocsvm_outliers_sc = tx.loc[ocsvm_outliers_sc.index] \n",
    "print(ocsvm_outliers_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5d8de-6939-4ceb-9905-2da7a04d2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ocsvm_outliers_sc, tx, 'OCSVM after scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3acda-6a12-48a3-b497-e34edbcea536",
   "metadata": {},
   "source": [
    "## Exploring Different Kernels\n",
    "\n",
    "Different kernels can capture different types of patterns. Let's compare how 'linear', 'poly', 'rbf', and 'sigmoid' kernels detect anomalies in our taxi data.\n",
    "\n",
    "**Watch for**:\n",
    "- Which kernel finds the most known anomalies?\n",
    "- Which kernel produces the most stable results?\n",
    "- How do the decision boundaries differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082a9c9-fa24-4e46-9e56-b8f79464e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    ocsvm = OCSVM(contamination=0.03, \n",
    "                  kernel=kernel)\n",
    "    ocsvm.fit(scaled)\n",
    "    ocsvm_pred_sc = pd.Series(ocsvm.predict(scaled), \n",
    "                              index=tx.index, \n",
    "                              name=kernel)\n",
    "    ocsvm_outliers_sc = ocsvm_pred_sc[ocsvm_pred_sc == 1]\n",
    "    ocsvm_outliers_sc = tx.loc[ocsvm_outliers_sc.index]\n",
    "    print(f\"Outliers using {kernel} kerenl: \\n{ocsvm_outliers_sc}\")\n",
    "    #plot_outliers(ocsvm_outliers_sc, tx, f\"OCSVM using {kernel} kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aacaa9-f415-4631-a146-75f35bc415b3",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Ensemble Methods (PyOD)\n",
    "\n",
    "## What are Ensemble Methods?\n",
    "\n",
    "**Ensemble methods** combine multiple models or decision strategies to improve overall performance:\n",
    "\n",
    "**Core Principle**: \"Wisdom of the crowd\"\n",
    "- Individual models might make mistakes\n",
    "- But **combining multiple models** often produces better results\n",
    "- Different models capture different aspects of anomalies\n",
    "\n",
    "**Types of Ensembles**:\n",
    "1. **Bagging**: Train multiple models on random subsets of data\n",
    "2. **Boosting**: Train models sequentially, focusing on hard cases\n",
    "3. **Feature bagging**: Train models on random subsets of features\n",
    "4. **Model combination**: Combine different algorithm types\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 1: Isolation Forest (IForest)\n",
    "\n",
    "### How Isolation Forest Works\n",
    "\n",
    "IForest uses a unique approach based on **isolation** rather than distance or density:\n",
    "\n",
    "**Key Insight**: \n",
    "- **Anomalies are rare and different** → easier to isolate\n",
    "- **Normal points are common and similar** → harder to isolate\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "1. **Build Isolation Trees** (`n_estimators` trees):\n",
    "   - Randomly select a feature\n",
    "   - Randomly select a split value between min and max\n",
    "   - Split the data recursively\n",
    "   - Stop when each point is isolated or tree reaches max depth\n",
    "\n",
    "2. **Measure Path Length**:\n",
    "   - For each point, count how many splits needed to isolate it\n",
    "   - **Anomalies** → isolated quickly → **short paths**\n",
    "   - **Normal points** → many splits needed → **long paths**\n",
    "\n",
    "3. **Aggregate Across Trees**:\n",
    "   - Average path length across all trees\n",
    "   - Normalize to get anomaly score\n",
    "   - Shorter average path = higher anomaly score\n",
    "\n",
    "### Visual Intuition\n",
    "\n",
    "Imagine randomly drawing lines to separate points:\n",
    "- An outlier sitting far from others gets separated in 1-2 cuts ✂️\n",
    "- A normal point in a dense cluster needs many cuts ✂️✂️✂️✂️✂️\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **`n_estimators`**: Number of isolation trees (default: 100)\n",
    "   - More trees → more stable, but slower\n",
    "   - Typical range: 50-200\n",
    "\n",
    "2. **`contamination`**: Expected proportion of outliers\n",
    "\n",
    "3. **`bootstrap`**: Whether to use sampling with replacement\n",
    "   - `True`: Each tree trained on a bootstrapped sample (introduces more diversity)\n",
    "   - `False`: Each tree sees all data (faster, but less diverse)\n",
    "\n",
    "4. **`max_samples`**: Number of samples to draw for each tree\n",
    "   - Default: 256 or size of dataset if smaller\n",
    "   - Larger → more accurate but slower\n",
    "   - Smaller → faster but less stable\n",
    "\n",
    "### Why Isolation Forest is Popular\n",
    "\n",
    "- ✅ **Fast**: Linear time complexity O(n)\n",
    "- ✅ **Scalable**: Works well with large datasets\n",
    "- ✅ **Few parameters**: Easy to use\n",
    "- ✅ **No need for distance/density computation**: Different paradigm\n",
    "- ✅ **Handles high-dimensional data** well\n",
    "- ✅ **Interpretable**: Path length has intuitive meaning\n",
    "\n",
    "### When to Use IForest\n",
    "\n",
    "- ✅ **Large datasets** (very efficient)\n",
    "- ✅ **High-dimensional data** (doesn't suffer from curse of dimensionality)\n",
    "- ✅ Need **fast training and prediction**\n",
    "- ✅ Want a **parameter-free** method (works well with defaults)\n",
    "- ✅ Outliers are **globally anomalous** (not local anomalies)\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 2: Deep Isolation Forest (DIF)\n",
    "\n",
    "### What is DIF?\n",
    "\n",
    "**DIF** is an advanced variant that combines:\n",
    "- Isolation Forest's isolation principle\n",
    "- Deep learning's representation learning\n",
    "\n",
    "### How DIF Improves on IForest\n",
    "\n",
    "1. **Feature Representation**:\n",
    "   - Uses neural networks to learn better feature representations\n",
    "   - Can capture **non-linear relationships** automatically\n",
    "\n",
    "2. **Ensemble of Representations**:\n",
    "   - Each tree uses a different learned representation\n",
    "   - More diverse ensemble → better performance\n",
    "\n",
    "### When to Use DIF\n",
    "\n",
    "- ✅ Data has **complex, non-linear patterns**\n",
    "- ✅ Have **sufficient training data** for deep learning\n",
    "- ✅ Standard IForest doesn't perform well\n",
    "- ❌ Need fast training (DIF is slower than IForest)\n",
    "- ❌ Need interpretability (neural networks are less interpretable)\n",
    "\n",
    "Let's apply both IForest and DIF to our taxi data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5ccf0-fac5-440d-9309-4a46942fcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc6f47-5c1c-4574-bbac-ab07eae19e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest = IForest(contamination=0.03,\n",
    "                 n_estimators=100,\n",
    "                 bootstrap=False,\n",
    "                 random_state=45)\n",
    "iforest.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdb25c-de65-4fef-8258-d7d031c6d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_pred = pd.Series(iforest.predict(tx), \n",
    "                      index=tx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99a794-718f-4559-bc10-6b4d08c569e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of IForest outliers = ', iforest_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43abcdc-6aa0-4d04-9448-fdceb029221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from iforest_pred\n",
    "iforest_outliers = iforest_pred[iforest_pred == 1]\n",
    "iforest_outliers = tx.loc[iforest_outliers.index] \n",
    "print(iforest_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee733d11-43f4-4058-9f8b-61bfeb843f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(iforest_outliers, tx, 'IForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f794c41-fbb5-4de3-b64e-fe0f63c25681",
   "metadata": {},
   "source": [
    "## Advanced Ensemble: Deep Isolation Forest (DIF)\n",
    "\n",
    "Now let's explore DIF, which combines isolation with deep learning for potentially better anomaly detection on complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cc949-ad09-41c3-a891-f5369ff552ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyod.models.dif import DIF\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize and fit the DIF model \n",
    "dif = DIF(contamination=0.03, n_estimators=100)\n",
    "dif.fit(tx)\n",
    "\n",
    "# Get outliers\n",
    "dif_scores = dif.decision_scores_\n",
    "dif_threshold = np.quantile(dif_scores, 0.97)  # Adjust percentile as needed\n",
    "dif_outliers = tx[dif_scores > dif_threshold]\n",
    "print(dif_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166690a-e613-4404-8309-b162928b3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(dif_outliers, tx, 'DIF')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "582feb74-aba6-4f2d-96df-6466f82fe4b5",
   "metadata": {},
   "source": [
    "# Detecting Outliers Using Deep Learning (PyOD)\n",
    "\n",
    "## What are Deep Learning-Based Methods?\n",
    "\n",
    "Deep learning brings **neural networks** to anomaly detection, offering powerful capabilities:\n",
    "\n",
    "**Core Idea**:\n",
    "- Use neural networks to **learn compressed representations** of normal data\n",
    "- Points that **reconstruct poorly** from these representations are anomalies\n",
    "- The network learns what \"normal\" looks like automatically\n",
    "\n",
    "**Advantages**:\n",
    "- ✅ **Automatic feature learning**: No manual feature engineering needed\n",
    "- ✅ **Handle complex patterns**: Can model highly non-linear relationships\n",
    "- ✅ **Scalable**: Works with high-dimensional data\n",
    "- ✅ **Flexible**: Can incorporate various architectures\n",
    "\n",
    "**Challenges**:\n",
    "- ❌ **Computationally expensive**: Requires more time and resources\n",
    "- ❌ **Hyperparameter tuning**: Learning rate, epochs, batch size, architecture\n",
    "- ❌ **Less interpretable**: \"Black box\" compared to simpler methods\n",
    "- ❌ **May overfit**: Especially with small datasets\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 1: AutoEncoder\n",
    "\n",
    "### What is an AutoEncoder?\n",
    "\n",
    "An **AutoEncoder** is a neural network trained to **reconstruct its input**:\n",
    "\n",
    "```\n",
    "Input → Encoder → Compressed Representation (Bottleneck) → Decoder → Reconstructed Output\n",
    "```\n",
    "\n",
    "### How AutoEncoders Detect Anomalies\n",
    "\n",
    "1. **Training Phase** (on normal data):\n",
    "   - **Encoder**: Compresses input to a low-dimensional representation (bottleneck)\n",
    "   - **Decoder**: Reconstructs input from the compressed representation\n",
    "   - **Objective**: Minimize reconstruction error for normal points\n",
    "   - The network learns to **encode patterns of normal data**\n",
    "\n",
    "2. **Detection Phase**:\n",
    "   - Pass new data through the trained AutoEncoder\n",
    "   - Calculate **reconstruction error** = |input - reconstructed output|\n",
    "   - **Normal points**: Small reconstruction error (network learned their patterns)\n",
    "   - **Anomalous points**: Large reconstruction error (network never learned these patterns)\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "```\n",
    "Input Layer (n features)\n",
    "    ↓\n",
    "Hidden Layer 1 (decreasing size)\n",
    "    ↓\n",
    "Hidden Layer 2 (even smaller)\n",
    "    ↓\n",
    "Bottleneck (smallest - the compressed representation)\n",
    "    ↓\n",
    "Hidden Layer 3 (increasing size)\n",
    "    ↓\n",
    "Hidden Layer 4 (same size as input)\n",
    "    ↓\n",
    "Output Layer (n features - reconstruction)\n",
    "```\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **`lr` (learning rate)**: How fast the network learns\n",
    "   - Too high: Network doesn't converge, unstable learning\n",
    "   - Too low: Training is very slow\n",
    "   - Typical range: 0.001 - 0.01\n",
    "\n",
    "2. **`epoch_num`**: Number of complete passes through the data\n",
    "   - Too few: Network doesn't learn enough (underfitting)\n",
    "   - Too many: Network memorizes even anomalies (overfitting)\n",
    "   - Start with 10-50, increase if needed\n",
    "   - Use validation to find optimal value\n",
    "\n",
    "3. **`batch_size`**: Number of samples per training update\n",
    "   - Smaller: More updates, noisier gradient, can escape local minima\n",
    "   - Larger: Faster training, more stable gradient, more memory\n",
    "   - Typical values: 16, 32, 64, 128\n",
    "\n",
    "4. **Architecture** (hidden layers, neurons):\n",
    "   - Deeper: Can learn more complex patterns\n",
    "   - Bottleneck size: Should be much smaller than input (compression)\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "**Underfitting Signs**:\n",
    "- High reconstruction error even on normal data\n",
    "- Solution: Train longer (more epochs), add capacity (more layers/neurons)\n",
    "\n",
    "**Overfitting Signs**:\n",
    "- Training error very low, but poor anomaly detection\n",
    "- Network reconstructs anomalies well (memorized them)\n",
    "- Solution: Early stopping, regularization, more training data\n",
    "\n",
    "### When to Use AutoEncoders\n",
    "\n",
    "- ✅ **High-dimensional data** (e.g., images, text, many features)\n",
    "- ✅ Data has **complex, non-linear patterns**\n",
    "- ✅ Have **sufficient normal training data**\n",
    "- ✅ Can afford **longer training time**\n",
    "- ❌ Small datasets (simpler methods might work better)\n",
    "- ❌ Need fast, real-time detection without GPU\n",
    "- ❌ Need interpretable results\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm 2: Variational AutoEncoder (VAE)\n",
    "\n",
    "### What is a VAE?\n",
    "\n",
    "**VAE** is a probabilistic variant of AutoEncoder:\n",
    "\n",
    "**Key Differences**:\n",
    "1. **Probabilistic encoding**: Instead of a single compressed representation, VAE learns a **distribution** (mean and variance)\n",
    "2. **Regularization**: Forces the learned distribution to be close to a standard normal distribution\n",
    "3. **Generative**: Can generate new samples (not just reconstruct)\n",
    "\n",
    "### How VAE Detects Anomalies\n",
    "\n",
    "1. **Encoder**: Maps input to a probability distribution (μ, σ)\n",
    "2. **Sampling**: Sample from this distribution (with reparameterization trick)\n",
    "3. **Decoder**: Reconstructs from the sample\n",
    "4. **Anomaly Score**: Combination of:\n",
    "   - Reconstruction error (like AutoEncoder)\n",
    "   - KL divergence (how far the learned distribution is from standard normal)\n",
    "\n",
    "### VAE vs. AutoEncoder\n",
    "\n",
    "| Feature | AutoEncoder | VAE |\n",
    "|---------|-------------|-----|\n",
    "| **Output** | Deterministic | Probabilistic |\n",
    "| **Regularization** | Optional | Built-in (KL divergence) |\n",
    "| **Overfitting** | More prone | More robust |\n",
    "| **Anomaly Score** | Reconstruction error | Reconstruction + KL |\n",
    "| **Complexity** | Simpler | More complex |\n",
    "| **Performance** | Good | Often better, more robust |\n",
    "\n",
    "### When to Use VAE over AutoEncoder\n",
    "\n",
    "- ✅ Want more **robust** anomaly detection\n",
    "- ✅ Need **generative capabilities**\n",
    "- ✅ Have **diverse** anomaly types\n",
    "- ✅ Want built-in **regularization**\n",
    "\n",
    "Let's apply both AutoEncoder and VAE to see how deep learning performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c9013-7b29-46f7-84a8-5b7d97622a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d7607-8019-4be6-bceb-4ea1ffaac172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.auto_encoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43136f35-a6a0-452f-8fef-98a1eac7cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr: Learning rate for optimization\n",
    "# epoch_num: Number of training iterations\n",
    "# batch_size: Number of samples per training batch\n",
    "auto_encoder = AutoEncoder(contamination=0.03,\n",
    "                           lr=0.001,\n",
    "                           epoch_num=10,\n",
    "                           batch_size=32)\n",
    "auto_encoder.fit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da07a0-199b-4927-a83c-1c5589b51330",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_predicted = pd.Series(auto_encoder.predict(tx), \n",
    "                      index=tx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b118992-5386-4ea2-b8e1-e619fdd58232",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of AutoEncoder outliers = ', ae_predicted.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daee061-8e39-4ebb-81b1-91b83422cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the outlier values and dates from ae_predicted\n",
    "ae_outliers = ae_predicted[ae_predicted == 1]\n",
    "ae_outliers = tx.loc[ae_outliers.index] \n",
    "print(ae_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9dc2de-1621-43dc-8731-49ba437d4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_scores = pd.Series(auto_encoder.decision_scores_, \n",
    "                      index=tx.index)\n",
    "\n",
    "threshold = auto_encoder.threshold_\n",
    "# or you can do it using quantile \n",
    "threshold = ae_scores.quantile(0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac0bcf-ad55-4b1a-a252-132625476c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_outliers =tx[ae_scores > ae_scores.quantile(0.97)]\n",
    "print(ae_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b3a8f-2573-413f-b668-b30a2234b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ae_outliers, tx, 'AutoEncoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f44a3-4660-4169-8f8e-fa95622e9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "auto_encoder = AutoEncoder(contamination=0.03,\n",
    "                           lr=0.001,\n",
    "                           epoch_num=1000,\n",
    "                           batch_size=32)\n",
    "auto_encoder.fit(tx)\n",
    "ae_predicted = pd.Series(auto_encoder.predict(tx), \n",
    "                      index=tx.index)\n",
    "# extract the outlier values and dates from ae_predicted\n",
    "ae_outliers = ae_predicted[ae_predicted == 1]\n",
    "ae_outliers = tx.loc[ae_outliers.index] \n",
    "print(ae_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6590f6d-c3f4-4fb7-8055-75c40876c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(ae_outliers, tx, 'AutoEncoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57bcc3-4493-41df-95f3-958fd2b77eaa",
   "metadata": {},
   "source": [
    "## Advanced Deep Learning: Variational AutoEncoder (VAE)\n",
    "\n",
    "Now let's explore **VAE**, a more sophisticated variant that often provides more robust anomaly detection through its probabilistic approach.\n",
    "\n",
    "**Watch for**:\n",
    "- Does VAE find different anomalies than the standard AutoEncoder?\n",
    "- How does the probabilistic approach affect the results?\n",
    "- Which deep learning method works better for our taxi data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11c65c-ad29-4da3-8cf5-f75217946e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.vae import VAE\n",
    "\n",
    "# lr: Learning rate for optimization\n",
    "# epoch_num: Number of training iterations\n",
    "# batch_size: Number of samples per training batch\n",
    "vae = VAE(contamination=0.03,\n",
    "                           lr=0.001,\n",
    "                           epoch_num=1000,\n",
    "                           batch_size=32)\n",
    "vae.fit(tx)\n",
    "\n",
    "vae_predicted = pd.Series(vae.predict(tx), \n",
    "                      index=tx.index)\n",
    "# extract the outlier values and dates from ae_predicted\n",
    "vae_outliers = vae_predicted[vae_predicted == 1]\n",
    "vae_outliers = tx.loc[vae_outliers.index] \n",
    "print(vae_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfff16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88aa45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Key Takeaways\n",
    "\n",
    "## Algorithms We Explored\n",
    "\n",
    "Throughout this lab, we explored **8+ different anomaly detection algorithms** across multiple families:\n",
    "\n",
    "### 1. **Distance-Based Algorithms**\n",
    "- **KNN (K-Nearest Neighbors)**: Uses average/median/max distance to k neighbors\n",
    "  - Simple, interpretable, but computationally expensive\n",
    "  - Works well when outliers are far from normal points\n",
    "\n",
    "- **LOF (Local Outlier Factor)**: Uses local density comparison\n",
    "  - Better than KNN for data with varying density\n",
    "  - Detects local anomalies within clusters\n",
    "\n",
    "### 2. **Clustering-Based Algorithms**\n",
    "- **CBLOF (Cluster-Based Local Outlier Factor)**: Clusters first, then scores\n",
    "  - Efficient for large datasets\n",
    "  - Works well when data has natural groupings\n",
    "  - Points in small clusters or far from clusters are anomalies\n",
    "\n",
    "### 3. **Probabilistic/Statistical Algorithms**\n",
    "- **COPOD (Copula-Based Outlier Detection)**: Uses copula theory\n",
    "  - Parameter-free (except contamination)\n",
    "  - Fast, handles feature dependencies\n",
    "  - Based on tail probabilities\n",
    "\n",
    "- **ECOD (Empirical Cumulative Outlier Detection)**: Simpler than COPOD\n",
    "  - Even faster, assumes feature independence\n",
    "  - Good baseline method\n",
    "\n",
    "### 4. **Kernel-Based Algorithms**\n",
    "- **OCSVM (One-Class SVM)**: Learns decision boundary using kernels\n",
    "  - Handles non-linear patterns through kernel trick\n",
    "  - Requires feature scaling\n",
    "  - Multiple kernel options (rbf, linear, poly, sigmoid)\n",
    "\n",
    "### 5. **Ensemble Methods**\n",
    "- **Isolation Forest**: Uses isolation trees to detect outliers\n",
    "  - Very fast and scalable\n",
    "  - Works well with high-dimensional data\n",
    "  - Based on \"ease of isolation\" principle\n",
    "\n",
    "- **DIF (Deep Isolation Forest)**: Combines isolation with deep learning\n",
    "  - More sophisticated than IForest\n",
    "  - Can learn better representations\n",
    "\n",
    "### 6. **Deep Learning Methods**\n",
    "- **AutoEncoder**: Neural network that learns to reconstruct normal data\n",
    "  - Detects anomalies through reconstruction error\n",
    "  - Good for complex, high-dimensional patterns\n",
    "  - Requires tuning (learning rate, epochs, batch size)\n",
    "\n",
    "- **VAE (Variational AutoEncoder)**: Probabilistic variant of AutoEncoder\n",
    "  - More robust than standard AutoEncoder\n",
    "  - Uses both reconstruction error and KL divergence\n",
    "  - Better regularization through probabilistic approach\n",
    "\n",
    "---\n",
    "\n",
    "## Techniques for Better Anomaly Detection\n",
    "\n",
    "### 1. **Time Series Decomposition**\n",
    "- Separate trend, seasonality, and residuals\n",
    "- Apply anomaly detection to residuals\n",
    "- Removes seasonal patterns that could hide true anomalies\n",
    "\n",
    "### 2. **Sliding Windows**\n",
    "- Create sequences of consecutive observations\n",
    "- Detect patterns in temporal context\n",
    "- Good for contextual anomalies\n",
    "\n",
    "### 3. **Feature Engineering**\n",
    "- Add temporal features (day of week, month, year)\n",
    "- Use cyclical encoding for periodic features\n",
    "- Incorporate domain knowledge (holidays, weekends)\n",
    "- Combine with sliding windows for rich representations\n",
    "\n",
    "### 4. **Feature Scaling**\n",
    "- Essential for distance-based and kernel methods\n",
    "- Use StandardScaler or PyOD's standardizer\n",
    "- Not needed for tree-based methods (IForest)\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing the Right Algorithm\n",
    "\n",
    "| Scenario | Recommended Algorithm |\n",
    "|----------|----------------------|\n",
    "| **Large dataset, need speed** | Isolation Forest, COPOD, ECOD |\n",
    "| **Small dataset** | KNN, LOF, OCSVM |\n",
    "| **High-dimensional data** | Isolation Forest, AutoEncoder, VAE |\n",
    "| **Complex non-linear patterns** | OCSVM (RBF), AutoEncoder, VAE |\n",
    "| **Varying density clusters** | LOF, CBLOF |\n",
    "| **Need interpretability** | KNN, Isolation Forest |\n",
    "| **Parameter-free method** | COPOD, ECOD |\n",
    "| **Time series with seasonality** | Use decomposition + any algorithm |\n",
    "| **Contextual anomalies** | Sliding windows + feature engineering |\n",
    "\n",
    "---\n",
    "\n",
    "## PyOD Workflow Best Practices\n",
    "\n",
    "1. **Initialize** the model with hyperparameters\n",
    "   ```python\n",
    "   model = KNN(contamination=0.03, n_neighbors=5)\n",
    "   ```\n",
    "\n",
    "2. **Fit** the model on data\n",
    "   ```python\n",
    "   model.fit(X)\n",
    "   ```\n",
    "\n",
    "3. **Get predictions** (binary labels: 0=normal, 1=anomaly)\n",
    "   ```python\n",
    "   predictions = model.predict(X)\n",
    "   ```\n",
    "\n",
    "4. **Get decision scores** (continuous scores)\n",
    "   ```python\n",
    "   scores = model.decision_scores_\n",
    "   # or\n",
    "   scores = model.decision_function(X)\n",
    "   ```\n",
    "\n",
    "5. **Get probabilities** (if needed)\n",
    "   ```python\n",
    "   proba = model.predict_proba(X, method='linear')\n",
    "   ```\n",
    "\n",
    "6. **Save/Load model**\n",
    "   ```python\n",
    "   from joblib import dump, load\n",
    "   dump(model, 'model.joblib')\n",
    "   model = load('model.joblib')\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Parameters\n",
    "\n",
    "- **`contamination`**: Expected proportion of outliers (e.g., 0.03 = 3%)\n",
    "  - Sets the threshold for classification\n",
    "  - Critical parameter for all algorithms\n",
    "  - Should be based on domain knowledge\n",
    "\n",
    "- **`n_neighbors` (KNN, LOF)**: Number of neighbors to consider\n",
    "  - Small k: More sensitive to local variations\n",
    "  - Large k: Smoother, more global view\n",
    "\n",
    "- **`n_clusters` (CBLOF)**: Number of clusters\n",
    "  - Domain-dependent\n",
    "  - Try different values and evaluate\n",
    "\n",
    "- **`kernel`, `gamma`, `nu` (OCSVM)**: Control decision boundary\n",
    "  - Start with 'rbf' kernel and 'auto' gamma\n",
    "  - Adjust based on results\n",
    "\n",
    "- **`n_estimators` (IForest)**: Number of trees\n",
    "  - More trees = more stable, but slower\n",
    "  - 100 is usually sufficient\n",
    "\n",
    "- **`lr`, `epoch_num`, `batch_size` (AutoEncoder, VAE)**: Training parameters\n",
    "  - Require experimentation\n",
    "  - Use validation to find optimal values\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Strategies\n",
    "\n",
    "1. **Use known anomalies** (if available) as ground truth\n",
    "2. **Visual inspection**: Plot detected anomalies\n",
    "3. **Multiple algorithms**: Compare results across different methods\n",
    "4. **Domain expertise**: Verify if detected anomalies make sense\n",
    "5. **PyOD's evaluation utilities**:\n",
    "   ```python\n",
    "   from pyod.utils import evaluate_print\n",
    "   evaluate_print('ModelName', y_true, decision_scores)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "**No single algorithm is best for all cases!**\n",
    "\n",
    "- Start with **simple, fast methods** (COPOD, IForest)\n",
    "- Try **multiple algorithms** and compare\n",
    "- Consider the **specific characteristics** of your data\n",
    "- Combine **domain knowledge** with algorithmic results\n",
    "- Use **feature engineering** to improve detection\n",
    "- **Visualize results** to build intuition\n",
    "\n",
    "**The best approach is often**:\n",
    "1. Try several algorithms\n",
    "2. Ensemble their results (e.g., voting, averaging scores)\n",
    "3. Incorporate domain knowledge\n",
    "4. Iterate based on feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
