{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding and Detecting Model Drift\n",
        "\n",
        "\n",
        "This notebook provides a hands-on demonstration of **model drift**, a critical concept in machine learning operations (MLOps). We will explore what model drift is, why it occurs, and how to detect it using Python libraries.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "1. Understand what model drift is and why it matters\n",
        "2. Distinguish between different types of drift\n",
        "3. Simulate drift in a data stream\n",
        "4. Implement drift detection algorithms\n",
        "5. Visualize and interpret drift detection results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is Model Drift?\n",
        "\n",
        "**Model drift**, also known as **model decay**, refers to the degradation of a machine learning model's performance over time. This phenomenon occurs when the statistical properties of the data the model encounters in production diverge from the data it was trained on.\n",
        "\n",
        "### Why Does Model Drift Matter?\n",
        "\n",
        "When model drift occurs, the model's predictions become less accurate, leading to:\n",
        "- Poor decision-making\n",
        "- Decreased business value\n",
        "- Potential financial losses\n",
        "- Reduced user trust\n",
        "\n",
        "### Real-World Example\n",
        "\n",
        "Consider a credit card fraud detection model trained on data from 2020. As fraudsters develop new techniques in 2021 and beyond, the patterns of fraudulent transactions change. The model, still operating on 2020 patterns, begins to miss new types of fraud—this is model drift in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Types of Model Drift\n",
        "\n",
        "There are three main types of drift that can affect machine learning models:\n",
        "\n",
        "### 2.1 Concept Drift\n",
        "\n",
        "**Concept drift** occurs when the relationship between the input variables (features) and the target variable changes over time. In mathematical terms, the conditional probability P(Y|X) changes, where Y is the target and X represents the features.\n",
        "\n",
        "**Example:** In a spam email classifier, what constitutes \"spam\" evolves as spammers adapt their tactics. An email pattern that was spam yesterday might be legitimate today, or vice versa.\n",
        "\n",
        "**Types of Concept Drift:**\n",
        "- **Sudden (Abrupt):** A rapid change in the relationship (e.g., COVID-19 pandemic impact on consumer behavior)\n",
        "- **Gradual:** Slow evolution over time (e.g., changing fashion trends)\n",
        "- **Incremental:** Step-by-step changes\n",
        "- **Recurring (Seasonal):** Patterns that repeat periodically (e.g., holiday shopping patterns)\n",
        "\n",
        "### 2.2 Data Drift (Covariate Shift)\n",
        "\n",
        "**Data drift** happens when the statistical distribution of the input features changes, but the relationship between features and target remains constant. In mathematical terms, P(X) changes, but P(Y|X) stays the same.\n",
        "\n",
        "**Example:** A house price prediction model trained on urban data is deployed in a rural area. The distribution of features (house size, lot size, etc.) is different, even though the relationship between these features and price remains similar.\n",
        "\n",
        "### 2.3 Upstream Data Changes\n",
        "\n",
        "**Upstream data changes** occur when modifications in the data pipeline affect the data quality or format.\n",
        "\n",
        "**Examples:**\n",
        "- Change in measurement units (miles to kilometers)\n",
        "- Change in data collection methods\n",
        "- Software updates that alter feature engineering\n",
        "- Changes in data sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setting Up Our Environment\n",
        "\n",
        "Let's import the necessary libraries for our drift detection demonstration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Why Start with Synthetic Data?\n",
        "\n",
        "Before we dive into the code, you might wonder: \"Why use synthetic (artificial) data instead of real data?\"\n",
        "\n",
        "**Educational Benefits:**\n",
        "- **Controlled Environment**: We know exactly when and how drift occurs, making it easier to validate our detection methods\n",
        "- **Clear Patterns**: Without noise from real-world complexity, we can focus on understanding the drift detection algorithms\n",
        "- **Reproducibility**: Everyone gets the same results, making it easier to learn together\n",
        "\n",
        "**Real-World Application**: After mastering these concepts with synthetic data, you'll apply the same techniques to real datasets where drift is less obvious but just as important to detect.\n",
        "\n",
        "> **Coming Up**: Later in this notebook, we'll also work with a realistic machine learning model to see how drift affects actual predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install river"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from river import drift\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure matplotlib for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Simulating Data Drift\n",
        "\n",
        "To understand drift detection, we'll first create a synthetic data stream that contains a drift point. This will help us visualize what drift looks like and test our detection algorithms.\n",
        "\n",
        "### Scenario\n",
        "\n",
        "Imagine we're monitoring a sensor that measures temperature. Initially, the sensor reads values around 20°C (68°F). After some time, due to environmental changes or sensor calibration issues, the readings shift to around 25°C (77°F)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data stream with drift\n",
        "\n",
        "# Stream 1: Initial distribution (mean=20, std=2)\n",
        "stream_1 = np.random.normal(20, 2, 500)\n",
        "\n",
        "# Stream 2: Drifted distribution (mean=25, std=2.5)\n",
        "stream_2 = np.random.normal(25, 2.5, 500)\n",
        "\n",
        "# Concatenate to create a single stream with drift at index 500\n",
        "data_stream = np.concatenate((stream_1, stream_2))\n",
        "\n",
        "print(f\"Total data points: {len(data_stream)}\")\n",
        "print(f\"Stream 1 - Mean: {stream_1.mean():.2f}, Std: {stream_1.std():.2f}\")\n",
        "print(f\"Stream 2 - Mean: {stream_2.mean():.2f}, Std: {stream_2.std():.2f}\")\n",
        "print(f\"\\nDrift introduced at index: 500\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Data Stream\n",
        "\n",
        "Let's plot our data stream to see the drift visually. The red vertical line indicates where we introduced the drift."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the data stream\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data_stream, alpha=0.7, linewidth=0.8)\n",
        "plt.axvline(x=500, color='red', linestyle='--', linewidth=2, label='True Drift Point')\n",
        "plt.title('Synthetic Data Stream with Drift at Index 500', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Data Point Index', fontsize=12)\n",
        "plt.ylabel('Value', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice how the data values shift upward after index 500.\")\n",
        "print(\"This represents a sudden drift in the data distribution.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Drift Detection with ADWIN\n",
        "\n",
        "Now we'll use a drift detection algorithm to automatically identify the drift point. We'll use **ADWIN (ADaptive WINdowing)**, a popular algorithm for detecting changes in data streams.\n",
        "\n",
        "### How ADWIN Works\n",
        "\n",
        "ADWIN maintains a sliding window of recent data points and continuously checks if the distribution in the window has changed significantly. When it detects a change, it signals a drift and adjusts the window size.\n",
        "\n",
        "**Key Features:**\n",
        "- No need to set a fixed window size\n",
        "- Adapts to the rate of change\n",
        "- Provides rigorous guarantees on false positive rates\n",
        "- Effective for both sudden and gradual drifts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Understanding ADWIN Parameters\n",
        "\n",
        "Before we use ADWIN, let's understand how to configure it:\n",
        "\n",
        "**Key Parameter: `delta`**\n",
        "- **Purpose**: Controls the sensitivity of drift detection (confidence level)\n",
        "- **Range**: 0 to 1 (typically 0.002 to 0.1)\n",
        "- **Trade-off**:\n",
        "  - **Lower delta** (e.g., 0.002): More sensitive → Detects drift quickly BUT more false alarms\n",
        "  - **Higher delta** (e.g., 0.1): Less sensitive → Fewer false alarms BUT slower detection\n",
        "- **Default**: 0.002 (99.8% confidence level)\n",
        "\n",
        "**How ADWIN Works - Simplified:**\n",
        "\n",
        "1. **Maintains a Window**: Keeps a sliding window of recent data points\n",
        "2. **Splits and Compares**: Continuously divides the window into two parts and compares their statistics\n",
        "3. **Detects Change**: If the two parts are significantly different, drift is detected\n",
        "4. **Adapts Window Size**: Automatically grows or shrinks the window based on detected changes\n",
        "\n",
        "**When to Use ADWIN:**\n",
        "- ✅ Monitoring continuous numerical data (sensor readings, model predictions, feature values)\n",
        "- ✅ When you don't know the expected drift pattern\n",
        "- ✅ When both sudden and gradual drift might occur\n",
        "- ✅ Real-time streaming applications\n",
        "\n",
        "**Advantages:**\n",
        "- No need to manually set window size\n",
        "- Rigorous statistical guarantees\n",
        "- Memory efficient\n",
        "- Works for various drift types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the ADWIN drift detector\n",
        "adwin = drift.ADWIN()\n",
        "\n",
        "# Track detected drift points\n",
        "drift_points = []\n",
        "\n",
        "# Process the stream and detect drift\n",
        "print(\"Processing data stream...\\n\")\n",
        "\n",
        "for i, value in enumerate(data_stream):\n",
        "    # Update the detector with the new value\n",
        "    adwin.update(value)\n",
        "    \n",
        "    # Check if drift was detected\n",
        "    if adwin.drift_detected:\n",
        "        print(f\"Drift detected at index: {i}\")\n",
        "        drift_points.append(i)\n",
        "\n",
        "print(f\"\\nTotal drift points detected: {len(drift_points)}\")\n",
        "print(f\"Detected drift points: {drift_points}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Interpreting ADWIN Results\n",
        "\n",
        "**What to Expect:**\n",
        "\n",
        "When running ADWIN on a data stream, you might see:\n",
        "- **Single Detection**: One drift point near the actual change (ideal scenario)\n",
        "- **Multiple Detections**: Several drift points in a short period (common with noisy data or gradual drift)\n",
        "- **No Detection**: Either no drift exists, or delta is too high\n",
        "\n",
        "**Why Multiple Drift Points?**\n",
        "\n",
        "You might detect drift at indices like 502, 503, 505, etc., even though the true drift is at 500. This happens because:\n",
        "1. **Statistical Fluctuation**: Random variation in data can trigger additional alerts\n",
        "2. **Window Adjustment**: After detecting drift, ADWIN resets its window and may detect the \"tail\" of the change\n",
        "3. **Sensitivity**: With default delta=0.002, ADWIN is very sensitive\n",
        "\n",
        "**What This Means in Practice:**\n",
        "- The first detection point is usually the most important\n",
        "- Multiple close detections typically indicate the same drift event\n",
        "- In production, you'd implement a \"cooldown period\" to avoid repeated alerts for the same drift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Detected Drift\n",
        "\n",
        "Let's visualize the data stream along with the detected drift points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the stream with detected drift points\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data_stream, alpha=0.7, linewidth=0.8, label='Data Stream')\n",
        "plt.axvline(x=500, color='red', linestyle='--', linewidth=2, label='True Drift Point (500)', alpha=0.7)\n",
        "\n",
        "# Mark detected drift points\n",
        "for idx, drift_point in enumerate(drift_points):\n",
        "    if idx == 0:\n",
        "        plt.axvline(x=drift_point, color='green', linestyle=':', linewidth=2, \n",
        "                   label=f'Detected Drift ({drift_point})')\n",
        "    else:\n",
        "        plt.axvline(x=drift_point, color='green', linestyle=':', linewidth=2)\n",
        "\n",
        "plt.title('Data Stream with True and Detected Drift Points', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Data Point Index', fontsize=12)\n",
        "plt.ylabel('Value', fontsize=12)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate detection accuracy\n",
        "if drift_points:\n",
        "    detection_delay = drift_points[0] - 500\n",
        "    print(f\"\\nDetection Performance:\")\n",
        "    print(f\"  - True drift point: 500\")\n",
        "    print(f\"  - First detected drift: {drift_points[0]}\")\n",
        "    print(f\"  - Detection delay: {detection_delay} data points\")\n",
        "else:\n",
        "    print(\"\\nNo drift was detected by the algorithm.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Analyzing Detection Performance\n",
        "\n",
        "**Key Metrics to Evaluate Drift Detection:**\n",
        "\n",
        "1. **Detection Delay**: How many data points after the true drift point was it detected?\n",
        "   - **Calculation**: `detected_index - true_drift_index`\n",
        "   - **Goal**: Minimize delay while avoiding false positives\n",
        "\n",
        "2. **False Positive Rate**: How often does the detector signal drift when there isn't any?\n",
        "   - Important in stable periods before/after drift\n",
        "\n",
        "3. **Detection Accuracy**: Did we catch all real drift events?\n",
        "\n",
        "**Performance Trade-offs:**\n",
        "\n",
        "| Metric | Lower Delta | Higher Delta |\n",
        "|--------|-------------|--------------|\n",
        "| Detection Speed | ⚡ Faster | 🐌 Slower |\n",
        "| False Positives | ⚠️ More | ✅ Fewer |\n",
        "| Sensitivity | 📈 High | 📉 Low |\n",
        "\n",
        "**What Makes Good Detection?**\n",
        "- Detecting within 10-50 data points is generally excellent for sudden drift\n",
        "- For gradual drift, detection anywhere in the transition zone is acceptable\n",
        "- No false positives in stable regions is ideal but rare with very low delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Experimenting with Gradual Drift\n",
        "\n",
        "Let's create a more challenging scenario: **gradual drift**. Instead of an abrupt change, the data distribution shifts slowly over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Why Gradual Drift is Harder to Detect\n",
        "\n",
        "**The Challenge:**\n",
        "\n",
        "Gradual drift is more common in real-world scenarios but harder to detect because:\n",
        "\n",
        "1. **Subtle Changes**: Each individual data point looks similar to previous ones\n",
        "2. **Moving Target**: The distribution is constantly shifting, not just switching once\n",
        "3. **Detection Ambiguity**: It's unclear when exactly the drift \"starts\" and \"ends\"\n",
        "\n",
        "**Real-World Examples:**\n",
        "- **Customer Behavior**: Shopping preferences slowly evolve with trends\n",
        "- **Market Dynamics**: Stock prices gradually shift with economic changes\n",
        "- **Sensor Degradation**: Equipment slowly wears down, producing different readings\n",
        "- **Seasonal Patterns**: Temperature, sales, traffic gradually change with seasons\n",
        "\n",
        "**Expected Detector Behavior:**\n",
        "\n",
        "Unlike sudden drift (1 clean detection), gradual drift often triggers:\n",
        "- ✅ **Multiple detections** throughout the transition period\n",
        "- ✅ **Scattered alerts** as the algorithm notices ongoing changes\n",
        "- ✅ **No detection** in early transition (change too subtle)\n",
        "\n",
        "This is **normal and expected** - not a failure of the algorithm!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a gradual drift scenario\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initial stable period\n",
        "stable_data = np.random.normal(20, 2, 300)\n",
        "\n",
        "# Gradual transition period (300 points)\n",
        "transition_length = 300\n",
        "transition_data = []\n",
        "for i in range(transition_length):\n",
        "    # Gradually shift mean from 20 to 25\n",
        "    current_mean = 20 + (5 * i / transition_length)\n",
        "    # Gradually increase std from 2 to 2.5\n",
        "    current_std = 2 + (0.5 * i / transition_length)\n",
        "    transition_data.append(np.random.normal(current_mean, current_std))\n",
        "\n",
        "transition_data = np.array(transition_data)\n",
        "\n",
        "# New stable period\n",
        "new_stable_data = np.random.normal(25, 2.5, 400)\n",
        "\n",
        "# Combine all periods\n",
        "gradual_stream = np.concatenate([stable_data, transition_data, new_stable_data])\n",
        "\n",
        "print(f\"Gradual drift stream created:\")\n",
        "print(f\"  - Stable period 1: 0-299 (mean ≈ 20)\")\n",
        "print(f\"  - Transition period: 300-599 (mean shifts from 20 to 25)\")\n",
        "print(f\"  - Stable period 2: 600-999 (mean ≈ 25)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize gradual drift\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(gradual_stream, alpha=0.7, linewidth=0.8)\n",
        "plt.axvline(x=300, color='orange', linestyle='--', linewidth=2, label='Drift Start')\n",
        "plt.axvline(x=600, color='purple', linestyle='--', linewidth=2, label='Drift End')\n",
        "plt.title('Data Stream with Gradual Drift', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Data Point Index', fontsize=12)\n",
        "plt.ylabel('Value', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detecting Gradual Drift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect drift in gradual stream\n",
        "adwin_gradual = drift.ADWIN()\n",
        "gradual_drift_points = []\n",
        "\n",
        "print(\"Detecting gradual drift...\\n\")\n",
        "\n",
        "for i, value in enumerate(gradual_stream):\n",
        "    adwin_gradual.update(value)\n",
        "    if adwin_gradual.drift_detected:\n",
        "        print(f\"Drift detected at index: {i}\")\n",
        "        gradual_drift_points.append(i)\n",
        "\n",
        "print(f\"\\nTotal drift points detected: {len(gradual_drift_points)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize gradual drift detection\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(gradual_stream, alpha=0.7, linewidth=0.8, label='Data Stream')\n",
        "plt.axvline(x=300, color='orange', linestyle='--', linewidth=2, label='Drift Start (300)', alpha=0.7)\n",
        "plt.axvline(x=600, color='purple', linestyle='--', linewidth=2, label='Drift End (600)', alpha=0.7)\n",
        "\n",
        "for idx, drift_point in enumerate(gradual_drift_points):\n",
        "    if idx == 0:\n",
        "        plt.axvline(x=drift_point, color='green', linestyle=':', linewidth=2, \n",
        "                   label=f'Detected Drift')\n",
        "    else:\n",
        "        plt.axvline(x=drift_point, color='green', linestyle=':', linewidth=2)\n",
        "\n",
        "plt.title('Gradual Drift Detection Results', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Data Point Index', fontsize=12)\n",
        "plt.ylabel('Value', fontsize=12)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservation: Gradual drift is harder to detect and may trigger multiple alerts.\")\n",
        "print(\"The detector identifies changes throughout the transition period.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Practical Strategies for Gradual Drift\n",
        "\n",
        "**How to Handle Multiple Detections:**\n",
        "\n",
        "1. **Aggregation Window**: Group detections within a time window (e.g., 50 data points)\n",
        "   - If 3+ detections occur within 50 points, consider it one drift event\n",
        "\n",
        "2. **Cooldown Period**: After detecting drift, pause alerts for N data points\n",
        "   - Prevents alert fatigue from repeated notifications\n",
        "\n",
        "3. **Threshold-Based**: Trigger action only after detecting drift X times in Y points\n",
        "   - Example: 3 detections in 100 points = likely real drift\n",
        "\n",
        "4. **Combine with Performance Monitoring**: Correlate drift detection with model accuracy\n",
        "   - If drift detected AND accuracy drops → high confidence to retrain\n",
        "\n",
        "**Code Example for Production:**\n",
        "```python\n",
        "# Pseudocode for production drift handling\n",
        "cooldown_remaining = 0\n",
        "drift_window = []\n",
        "\n",
        "for data_point in stream:\n",
        "    detector.update(data_point)\n",
        "    \n",
        "    if detector.drift_detected and cooldown_remaining == 0:\n",
        "        drift_window.append(current_index)\n",
        "        \n",
        "        # Check if multiple drifts in window\n",
        "        if len(drift_window) >= 3:\n",
        "            trigger_retraining()\n",
        "            cooldown_remaining = 100  # Pause for 100 points\n",
        "            drift_window.clear()\n",
        "    \n",
        "    cooldown_remaining = max(0, cooldown_remaining - 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comparing Different Drift Detectors\n",
        "\n",
        "The `river` library provides several drift detection algorithms. Let's compare ADWIN with another popular detector: **DDM (Drift Detection Method)**.\n",
        "\n",
        "### DDM (Drift Detection Method)\n",
        "\n",
        "DDM monitors the error rate of an online learner. It's based on the assumption that the error rate will increase when drift occurs. DDM is particularly effective for classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Understanding DDM (Drift Detection Method)\n",
        "\n",
        "**What Makes DDM Different from ADWIN?**\n",
        "\n",
        "| Feature | ADWIN | DDM |\n",
        "|---------|-------|-----|\n",
        "| **Input Data** | Any numerical values | Binary (0/1) - correct/incorrect predictions |\n",
        "| **Use Case** | General data drift | Classification model performance |\n",
        "| **What It Monitors** | Distribution of values | Error rate pattern |\n",
        "| **Best For** | Feature drift, data changes | Concept drift, model degradation |\n",
        "\n",
        "**How DDM Works:**\n",
        "\n",
        "DDM monitors the **error rate** of predictions and uses statistical process control:\n",
        "\n",
        "1. **Baseline**: Establishes normal error rate during initial stable period\n",
        "2. **Warning Level**: Error rate increases beyond expected variance\n",
        "   - Signal: \"Something might be changing, keep watching\"\n",
        "3. **Drift Level**: Error rate significantly exceeds normal bounds\n",
        "   - Signal: \"Drift confirmed, take action!\"\n",
        "\n",
        "**Key Insight**: DDM assumes that when the relationship between features and target changes (concept drift), the model's error rate will increase.\n",
        "\n",
        "**When to Use DDM:**\n",
        "- ✅ Monitoring classification model performance\n",
        "- ✅ When you have ground truth labels (even with delay)\n",
        "- ✅ Detecting concept drift specifically\n",
        "- ✅ Binary or multi-class classification problems\n",
        "\n",
        "**Limitations:**\n",
        "- ❌ Requires labeled data (true outcomes)\n",
        "- ❌ Not suitable for regression problems\n",
        "- ❌ Won't detect data drift if model performance doesn't change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate a classification scenario with drift\n",
        "np.random.seed(42)\n",
        "\n",
        "# Before drift: model has 10% error rate\n",
        "errors_before = np.random.choice([0, 1], size=500, p=[0.9, 0.1])\n",
        "\n",
        "# After drift: model has 30% error rate (performance degraded)\n",
        "errors_after = np.random.choice([0, 1], size=500, p=[0.7, 0.3])\n",
        "\n",
        "# Combine error streams\n",
        "error_stream = np.concatenate([errors_before, errors_after])\n",
        "\n",
        "print(f\"Classification error stream created:\")\n",
        "print(f\"  - Before drift (0-499): ~10% error rate\")\n",
        "print(f\"  - After drift (500-999): ~30% error rate\")\n",
        "print(f\"  - Actual error rate before: {errors_before.mean()*100:.1f}%\")\n",
        "print(f\"  - Actual error rate after: {errors_after.mean()*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Creating a Realistic Classification Scenario\n",
        "\n",
        "**The Scenario:**\n",
        "\n",
        "Imagine a spam email classifier deployed in production:\n",
        "- **Initially**: The model correctly identifies 90% of emails (10% error rate)\n",
        "- **After Drift**: Spammers adapt their tactics, and the model's accuracy drops to 70% (30% error rate)\n",
        "\n",
        "**Why This Happens (Concept Drift):**\n",
        "- Spammers learn to avoid common spam words the model looks for\n",
        "- New types of phishing attacks emerge\n",
        "- The relationship between email features and \"spam/not spam\" has changed\n",
        "\n",
        "**Data Representation:**\n",
        "- `0` = Model made correct prediction\n",
        "- `1` = Model made incorrect prediction (error)\n",
        "\n",
        "We'll simulate 1000 email predictions, with drift occurring at position 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from river.drift.binary import DDM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use DDM to detect drift\n",
        "ddm = DDM()\n",
        "ddm_drift_points = []\n",
        "ddm_warning_points = []\n",
        "\n",
        "print(\"\\nDetecting drift with DDM...\\n\")\n",
        "\n",
        "for i, error in enumerate(error_stream):\n",
        "    # DDM expects binary input (0 = correct, 1 = error)\n",
        "    ddm.update(error)\n",
        "    \n",
        "    if ddm.drift_detected:\n",
        "        print(f\"Drift detected at index: {i}\")\n",
        "        ddm_drift_points.append(i)\n",
        "\n",
        "print(f\"\\nTotal drift points detected by DDM: {len(ddm_drift_points)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize error rates and detected drift\n",
        "# Calculate moving average of errors for visualization\n",
        "window_size = 50\n",
        "moving_avg = np.convolve(error_stream, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(range(window_size-1, len(error_stream)), moving_avg, \n",
        "         linewidth=2, label='Error Rate (Moving Average)')\n",
        "plt.axvline(x=500, color='red', linestyle='--', linewidth=2, \n",
        "           label='True Drift Point (500)', alpha=0.7)\n",
        "\n",
        "for idx, drift_point in enumerate(ddm_drift_points):\n",
        "    if idx == 0:\n",
        "        plt.axvline(x=drift_point, color='green', linestyle=':', linewidth=2, \n",
        "                   label=f'DDM Detected Drift')\n",
        "    else:\n",
        "        plt.axvline(x=drift_point, color='green', linestyle=':', linewidth=2)\n",
        "\n",
        "plt.title('Drift Detection in Classification Error Stream (DDM)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Data Point Index', fontsize=12)\n",
        "plt.ylabel('Error Rate', fontsize=12)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDDM is particularly effective for detecting drift in classification performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Interpreting DDM Results\n",
        "\n",
        "**What the Visualization Shows:**\n",
        "\n",
        "The moving average smooths out individual prediction errors to show the overall trend:\n",
        "- **Flat line** around 0.1 (10%) before index 500 = Good performance\n",
        "- **Sharp increase** to 0.3 (30%) after index 500 = Performance degradation\n",
        "- **Green line(s)** = When DDM detected the drift\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "1. **Detection Timing**: DDM typically detects drift shortly after the error rate increases\n",
        "   - It needs to accumulate evidence (multiple errors) before signaling drift\n",
        "   - This is why detection might lag by 20-100 data points\n",
        "\n",
        "2. **Statistical Confidence**: DDM uses hypothesis testing\n",
        "   - It waits until it's statistically confident the increase is real, not just random variance\n",
        "   - This prevents false alarms from occasional bad predictions\n",
        "\n",
        "3. **Comparison with ADWIN**:\n",
        "   - **ADWIN**: Detects *any* distribution change in the data\n",
        "   - **DDM**: Detects *performance degradation* specifically\n",
        "   - For production ML systems, use both: ADWIN for features, DDM for predictions\n",
        "\n",
        "**Practical Implications:**\n",
        "\n",
        "In a real system, this drift detection would trigger:\n",
        "1. ✉️ **Alert**: Notify ML engineers of performance drop\n",
        "2. 📊 **Investigation**: Analyze recent data for changes\n",
        "3. 🔄 **Retraining**: Collect new labeled data and retrain model\n",
        "4. 🚀 **Deployment**: Deploy updated model to restore performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Key Takeaways and Best Practices\n",
        "\n",
        "### Summary\n",
        "\n",
        "In this notebook, we've explored:\n",
        "\n",
        "1. **What model drift is**: The degradation of model performance over time due to changes in data\n",
        "2. **Types of drift**: Concept drift, data drift, and upstream data changes\n",
        "3. **Drift patterns**: Sudden, gradual, incremental, and recurring drift\n",
        "4. **Detection methods**: ADWIN for general drift detection and DDM for classification scenarios\n",
        "5. **Practical implementation**: How to use the `river` library to detect drift in Python\n",
        "6. **Real-world application**: Complete ML pipeline with drift monitoring\n",
        "7. **Production strategies**: Combining multiple signals for robust monitoring\n",
        "\n",
        "### Best Practices for Production Systems\n",
        "\n",
        "1. **Monitor Continuously**: Set up automated drift detection in your ML pipelines\n",
        "2. **Choose Appropriate Detectors**: Select algorithms based on your data type and drift patterns\n",
        "3. **Set Thresholds Carefully**: Balance between false positives and detection delay\n",
        "4. **Combine Multiple Signals**: Use both performance metrics and drift detectors\n",
        "5. **Establish Retraining Protocols**: Have a plan for when drift is detected\n",
        "6. **Log and Analyze**: Keep records of detected drifts for pattern analysis\n",
        "7. **Test Drift Scenarios**: Simulate different types of drift during model development\n",
        "8. **Don't Over-React**: Not every drift detection means immediate retraining\n",
        "9. **Business Context Matters**: Align drift sensitivity with business costs of errors\n",
        "10. **Automate Responsibly**: Have human review for high-stakes decisions\n",
        "\n",
        "### When to Retrain Your Model\n",
        "\n",
        "Consider retraining when:\n",
        "- ✅ Drift is detected consistently over multiple periods\n",
        "- ✅ Model performance metrics drop below acceptable thresholds\n",
        "- ✅ Business metrics (e.g., conversion rates, revenue) are affected\n",
        "- ✅ Multiple drift signals converge (features + performance)\n",
        "- ✅ Major events occur that likely affect your domain (e.g., regulatory changes, market shifts)\n",
        "\n",
        "Consider NOT retraining when:\n",
        "- ❌ Single isolated drift detection (might be noise)\n",
        "- ❌ Drift detected but performance remains acceptable\n",
        "- ❌ Temporary data quality issues (fix data pipeline instead)\n",
        "- ❌ Cost of retraining exceeds cost of slightly degraded performance\n",
        "\n",
        "### The Drift Detection Lifecycle\n",
        "\n",
        "```\n",
        "1. Design Phase\n",
        "   └── Choose drift detectors based on model type\n",
        "   └── Set sensitivity parameters\n",
        "   └── Define alerting thresholds\n",
        "\n",
        "2. Deployment Phase\n",
        "   └── Implement monitoring in production\n",
        "   └── Log all relevant metrics\n",
        "   └── Set up dashboards\n",
        "\n",
        "3. Monitoring Phase\n",
        "   └── Continuously track drift signals\n",
        "   └── Correlate with business KPIs\n",
        "   └── Investigate alerts\n",
        "\n",
        "4. Response Phase\n",
        "   └── Validate drift is real\n",
        "   └── Collect new training data\n",
        "   └── Retrain and evaluate\n",
        "   └── Deploy with A/B testing\n",
        "\n",
        "5. Learning Phase\n",
        "   └── Analyze drift patterns\n",
        "   └── Adjust detector parameters\n",
        "   └── Update retraining protocols\n",
        "   └── Document lessons learned\n",
        "```\n",
        "\n",
        "### Final Thoughts\n",
        "\n",
        "**Model drift is inevitable.** Real-world data distributions change, user behaviors evolve, and business environments shift. The question is not *if* your model will experience drift, but *when* and *how well you'll detect it*.\n",
        "\n",
        "By implementing robust drift detection and monitoring, you ensure that your ML systems remain reliable, trustworthy, and valuable to your organization over time.\n",
        "\n",
        "**Remember**: A model without monitoring is like a ship without a compass - you might stay on course for a while, but eventually, you'll drift off target without even knowing it! 🚢\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "- **River Documentation**: https://riverml.xyz/\n",
        "- **Evidently AI**: Another excellent library for ML monitoring\n",
        "- **Research Papers**: Look up \"concept drift detection\" on Google Scholar\n",
        "- **MLOps Practices**: Study continuous training and monitoring patterns\n",
        "- **Coursera/edX**: Search for MLOps courses that cover model monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Choosing the Right Drift Detection Strategy\n",
        "\n",
        "### Decision Framework\n",
        "\n",
        "Use this guide to select the appropriate drift detection approach for your use case:\n",
        "\n",
        "#### **1. If You Have a Classification Model with Ground Truth Labels:**\n",
        "\n",
        "**Use DDM (or similar performance-based detectors)**\n",
        "- ✅ Directly monitors what matters: model performance\n",
        "- ✅ Detects concept drift (when relationships change)\n",
        "- ✅ Simple to interpret: \"error rate increased\"\n",
        "- Example: Fraud detection, spam filtering, churn prediction\n",
        "\n",
        "#### **2. If You Need to Monitor Feature Distributions:**\n",
        "\n",
        "**Use ADWIN (or statistical tests)**\n",
        "- ✅ Detects data drift before it affects performance\n",
        "- ✅ No need for labels\n",
        "- ✅ Can monitor individual features or predictions\n",
        "- Example: Sensor monitoring, input data validation, feature engineering pipelines\n",
        "\n",
        "#### **3. If You Have a Regression Model:**\n",
        "\n",
        "**Use ADWIN on Prediction Errors**\n",
        "- Monitor: `|predicted_value - actual_value|`\n",
        "- Can also use: ADWIN on feature distributions\n",
        "- Alternative: PageHinkley test for regression errors\n",
        "\n",
        "#### **4. For Comprehensive Monitoring (Recommended for Production):**\n",
        "\n",
        "**Use Multiple Detectors:**\n",
        "```\n",
        "├── Feature-Level: ADWIN on each important feature\n",
        "├── Prediction-Level: ADWIN on model outputs\n",
        "├── Performance-Level: DDM on prediction errors (if labels available)\n",
        "└── Business-Level: Monitor business KPIs\n",
        "```\n",
        "\n",
        "### Comparison Table\n",
        "\n",
        "| Detector | Input Type | Best For | Requires Labels | Detection Speed |\n",
        "|----------|-----------|----------|-----------------|-----------------|\n",
        "| **ADWIN** | Continuous values | Data/Feature drift | ❌ No | Fast |\n",
        "| **DDM** | Binary (0/1) | Performance drift | ✅ Yes | Medium |\n",
        "| **KSWIN** | Continuous values | Distribution changes | ❌ No | Medium |\n",
        "| **PageHinkley** | Continuous values | Mean shifts | ❌ No | Very Fast |\n",
        "| **EDDM** | Binary (0/1) | Gradual concept drift | ✅ Yes | Slow |\n",
        "\n",
        "### Practical Tips\n",
        "\n",
        "1. **Start Simple**: Begin with one detector, add more as needed\n",
        "2. **Tune Carefully**: Test different sensitivity parameters on historical data\n",
        "3. **Set Up Baselines**: Run detectors on stable data to establish false positive rates\n",
        "4. **Combine Signals**: Don't rely on a single detector - use multiple indicators\n",
        "5. **Monitor Business Metrics**: Drift detection should align with business impact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Real-World Application: Complete ML Pipeline with Drift\n",
        "\n",
        "Now let's see how drift affects an actual machine learning model. We'll build a complete example showing:\n",
        "1. Training a model on initial data\n",
        "2. Simulating production data with drift\n",
        "3. Detecting both data drift and performance degradation\n",
        "4. Understanding when to retrain\n",
        "\n",
        "### The Scenario: Customer Churn Prediction\n",
        "\n",
        "We'll create a binary classification model that predicts whether customers will churn (leave the service).\n",
        "- **Initial Period**: Customer behavior patterns are stable\n",
        "- **Drift Period**: Market conditions change, affecting customer behavior\n",
        "- **Goal**: Detect when our model's predictions become unreliable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import additional libraries for ML model\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create initial training data\n",
        "print(\"Step 1: Creating Initial Training Dataset\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "X_train, y_train = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=8,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    random_state=42,\n",
        "    flip_y=0.1  # 10% label noise (realistic scenario)\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(X_train)} samples\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "print(f\"Class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Class balance: {y_train.mean():.2%} positive class\")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nStep 2: Training Logistic Regression Model\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on training data\n",
        "train_predictions = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy:.2%}\")\n",
        "print(\"\\nModel trained successfully! ✓\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate production data stream with concept drift\n",
        "print(\"Step 3: Simulating Production Data Stream\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Period 1: Stable - Similar to training data (500 samples)\n",
        "X_stable, y_stable = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=10,\n",
        "    n_informative=8,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    random_state=100,\n",
        "    flip_y=0.1\n",
        ")\n",
        "\n",
        "# Period 2: Drifted - Changed relationship (500 samples)\n",
        "# Simulate concept drift by:\n",
        "# 1. Shifting feature means\n",
        "# 2. Increasing class imbalance\n",
        "# 3. Adding noise\n",
        "X_drifted, y_drifted = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=10,\n",
        "    n_informative=8,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    random_state=200,\n",
        "    flip_y=0.25,  # More noise (concept became harder)\n",
        "    weights=[0.7, 0.3],  # Class imbalance changed\n",
        "    shift=2.0  # Feature distribution shifted\n",
        ")\n",
        "\n",
        "# Combine into production stream\n",
        "X_production = np.vstack([X_stable, X_drifted])\n",
        "y_production = np.concatenate([y_stable, y_drifted])\n",
        "\n",
        "print(f\"Production stream created:\")\n",
        "print(f\"  - Stable period (0-499): Similar to training data\")\n",
        "print(f\"  - Drifted period (500-999): Concept drift introduced\")\n",
        "print(f\"\\nStable period - Class balance: {y_stable.mean():.2%}\")\n",
        "print(f\"Drifted period - Class balance: {y_drifted.mean():.2%}\")\n",
        "print(f\"\\nFeature 0 mean shift: {X_stable[:, 0].mean():.2f} → {X_drifted[:, 0].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor model performance and detect drift\n",
        "print(\"Step 4: Monitoring Model Performance in Production\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Initialize drift detectors\n",
        "adwin_feature = drift.ADWIN()  # Monitor feature drift\n",
        "ddm_performance = DDM()  # Monitor performance drift\n",
        "\n",
        "# Track metrics\n",
        "predictions = []\n",
        "actual_labels = []\n",
        "prediction_errors = []\n",
        "accuracies = []\n",
        "feature_drift_points = []\n",
        "performance_drift_points = []\n",
        "\n",
        "# Simulate online learning scenario\n",
        "window_size = 50  # Calculate accuracy every 50 predictions\n",
        "\n",
        "print(\"\\nProcessing production stream...\\n\")\n",
        "\n",
        "for i in range(len(X_production)):\n",
        "    # Get current sample\n",
        "    X_current = X_production[i:i+1]\n",
        "    y_current = y_production[i]\n",
        "    \n",
        "    # Make prediction\n",
        "    y_pred = model.predict(X_current)[0]\n",
        "    predictions.append(y_pred)\n",
        "    actual_labels.append(y_current)\n",
        "    \n",
        "    # Calculate error (0 = correct, 1 = incorrect)\n",
        "    error = int(y_pred != y_current)\n",
        "    prediction_errors.append(error)\n",
        "    \n",
        "    # Monitor feature drift (using first feature as example)\n",
        "    adwin_feature.update(X_current[0, 0])\n",
        "    if adwin_feature.drift_detected:\n",
        "        feature_drift_points.append(i)\n",
        "        print(f\"⚠️  Feature drift detected at index: {i}\")\n",
        "    \n",
        "    # Monitor performance drift\n",
        "    ddm_performance.update(error)\n",
        "    if ddm_performance.drift_detected:\n",
        "        performance_drift_points.append(i)\n",
        "        print(f\"🔴 Performance drift detected at index: {i}\")\n",
        "    \n",
        "    # Calculate rolling accuracy\n",
        "    if (i + 1) % window_size == 0:\n",
        "        recent_errors = prediction_errors[-window_size:]\n",
        "        accuracy = 1 - np.mean(recent_errors)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "print(f\"\\nMonitoring complete!\")\n",
        "print(f\"Feature drift detections: {len(feature_drift_points)}\")\n",
        "print(f\"Performance drift detections: {len(performance_drift_points)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comprehensive drift monitoring results\n",
        "print(\"Step 5: Visualizing Model Performance Degradation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
        "\n",
        "# Plot 1: Rolling Accuracy\n",
        "ax1 = axes[0]\n",
        "accuracy_indices = np.arange(window_size, len(X_production) + 1, window_size)\n",
        "ax1.plot(accuracy_indices, accuracies, linewidth=2, color='blue', label='Rolling Accuracy')\n",
        "ax1.axvline(x=500, color='red', linestyle='--', linewidth=2, label='True Drift Point', alpha=0.7)\n",
        "ax1.axhline(y=train_accuracy, color='green', linestyle=':', linewidth=2, label='Training Accuracy', alpha=0.7)\n",
        "ax1.set_title('Model Performance Over Time', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Sample Index')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Feature Distribution (Feature 0)\n",
        "ax2 = axes[1]\n",
        "feature_values = X_production[:, 0]\n",
        "ax2.plot(feature_values, alpha=0.6, linewidth=0.8, label='Feature 0 Values')\n",
        "ax2.axvline(x=500, color='red', linestyle='--', linewidth=2, label='True Drift Point', alpha=0.7)\n",
        "for drift_point in feature_drift_points[:3]:  # Show first 3 to avoid clutter\n",
        "    ax2.axvline(x=drift_point, color='orange', linestyle=':', linewidth=1.5, alpha=0.6)\n",
        "ax2.set_title('Feature Distribution Drift (Feature 0)', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Sample Index')\n",
        "ax2.set_ylabel('Feature Value')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Cumulative Error Rate\n",
        "ax3 = axes[2]\n",
        "cumulative_errors = np.cumsum(prediction_errors) / np.arange(1, len(prediction_errors) + 1)\n",
        "ax3.plot(cumulative_errors, linewidth=2, color='red', label='Cumulative Error Rate')\n",
        "ax3.axvline(x=500, color='red', linestyle='--', linewidth=2, label='True Drift Point', alpha=0.7)\n",
        "for drift_point in performance_drift_points[:3]:\n",
        "    ax3.axvline(x=drift_point, color='purple', linestyle=':', linewidth=2, alpha=0.6)\n",
        "ax3.set_title('Cumulative Error Rate with Performance Drift Detection', fontsize=14, fontweight='bold')\n",
        "ax3.set_xlabel('Sample Index')\n",
        "ax3.set_ylabel('Error Rate')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Key Observations:\")\n",
        "print(f\"  • Accuracy before drift: {np.mean(accuracies[:10]):.2%}\")\n",
        "print(f\"  • Accuracy after drift: {np.mean(accuracies[10:]):.2%}\")\n",
        "print(f\"  • Performance drop: {(np.mean(accuracies[:10]) - np.mean(accuracies[10:])):.2%}\")\n",
        "print(f\"  • First performance drift detected at: {performance_drift_points[0] if performance_drift_points else 'N/A'}\")\n",
        "print(f\"  • Detection delay: {performance_drift_points[0] - 500 if performance_drift_points else 'N/A'} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1 Understanding the Complete Picture\n",
        "\n",
        "**What We Just Demonstrated:**\n",
        "\n",
        "This example shows the **complete lifecycle** of model drift in production:\n",
        "\n",
        "1. **Training Phase**: Model achieves good accuracy on historical data\n",
        "2. **Stable Production**: Model performs well initially (samples 0-499)\n",
        "3. **Drift Occurs**: Real-world conditions change at sample 500\n",
        "4. **Detection**: Our monitoring system catches both:\n",
        "   - **Feature Drift**: Input data distribution changes (ADWIN)\n",
        "   - **Performance Drift**: Prediction accuracy drops (DDM)\n",
        "5. **Decision Point**: Time to retrain or update the model\n",
        "\n",
        "**Three Key Metrics to Monitor:**\n",
        "\n",
        "| Metric | What It Shows | When to Alert |\n",
        "|--------|---------------|---------------|\n",
        "| **Rolling Accuracy** | Current model performance | Drops below threshold |\n",
        "| **Feature Distribution** | Input data changes | Statistical shift detected |\n",
        "| **Error Rate Trend** | Long-term degradation | Sustained increase |\n",
        "\n",
        "**Why Monitor Multiple Signals?**\n",
        "\n",
        "- **Feature Drift Alone**: May not impact performance (false alarm)\n",
        "- **Performance Drift Alone**: May be temporary (data quality issue)\n",
        "- **Both Together**: Strong signal that retraining is needed ✅\n",
        "\n",
        "**Real-World Decision Logic:**\n",
        "\n",
        "```python\n",
        "if feature_drift_detected AND performance_drift_detected:\n",
        "    if accuracy < performance_threshold:\n",
        "        # High confidence: Trigger retraining\n",
        "        initiate_retraining_pipeline()\n",
        "    else:\n",
        "        # Medium confidence: Investigate\n",
        "        send_alert_to_ml_team()\n",
        "elif performance_drift_detected:\n",
        "    # Performance drop without feature drift\n",
        "    # Could be: data quality issue, labeling problem\n",
        "    investigate_data_quality()\n",
        "elif feature_drift_detected:\n",
        "    # Feature drift without performance drop\n",
        "    # Monitor closely, but don't retrain yet\n",
        "    increase_monitoring_frequency()\n",
        "```\n",
        "\n",
        "**Key Takeaway**: Always combine multiple monitoring signals before taking action. One detector alone can produce false positives, but converging evidence from multiple detectors gives you confidence to act."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Practice Exercises\n",
        "\n",
        "Now it's your turn to apply what you've learned! Try these exercises to deepen your understanding.\n",
        "\n",
        "### Exercise 1: Experiment with ADWIN Sensitivity ⭐\n",
        "\n",
        "**Task**: Modify the ADWIN detector's `delta` parameter and observe how it affects drift detection.\n",
        "\n",
        "**Steps**:\n",
        "1. Go back to the sudden drift example (Section 5)\n",
        "2. Try `delta=0.1` (less sensitive) and `delta=0.0001` (more sensitive)\n",
        "3. Compare detection timing and number of drift points\n",
        "\n",
        "**Questions to Answer**:\n",
        "- Which delta value detects drift fastest?\n",
        "- Which delta value has more false positives?\n",
        "- What delta would you choose for a production system where false alarms are costly?\n",
        "\n",
        "### Exercise 2: Create Your Own Drift Scenario ⭐⭐\n",
        "\n",
        "**Task**: Create a data stream with **recurring drift** (seasonal pattern).\n",
        "\n",
        "**Hint**:\n",
        "```python\n",
        "# Create a stream that oscillates between two distributions\n",
        "stream = []\n",
        "for i in range(1000):\n",
        "    if (i // 100) % 2 == 0:  # Every 100 points, switch\n",
        "        stream.append(np.random.normal(20, 2))\n",
        "    else:\n",
        "        stream.append(np.random.normal(25, 2))\n",
        "```\n",
        "\n",
        "**Questions to Answer**:\n",
        "- How does ADWIN handle recurring drift?\n",
        "- What happens to the number of detected drift points?\n",
        "- How would you design a detector specifically for seasonal patterns?\n",
        "\n",
        "### Exercise 3: Multi-Feature Drift Monitoring ⭐⭐⭐\n",
        "\n",
        "**Task**: Extend the ML pipeline example (Section 10) to monitor **multiple features** simultaneously.\n",
        "\n",
        "**Steps**:\n",
        "1. Create one ADWIN detector for each feature\n",
        "2. Track which features drift and when\n",
        "3. Visualize which features are most affected by drift\n",
        "\n",
        "**Challenge**: Can you identify which feature drift most strongly correlates with performance drop?\n",
        "\n",
        "### Exercise 4: Implement a Retraining Decision System ⭐⭐⭐\n",
        "\n",
        "**Task**: Build a simple decision system that determines when to retrain based on multiple signals.\n",
        "\n",
        "**Requirements**:\n",
        "- Monitor at least 3 signals (feature drift, performance drift, accuracy)\n",
        "- Implement a scoring system (e.g., each signal = 1 point)\n",
        "- Trigger \"retrain\" when score exceeds threshold\n",
        "- Count how many false alarms vs. true retraining triggers\n",
        "\n",
        "**Bonus**: Add a cooldown period to prevent retraining too frequently.\n",
        "\n",
        "---\n",
        "\n",
        "**💡 Tips for Success**:\n",
        "- Start with Exercise 1 to build intuition\n",
        "- Use print statements to track detector states\n",
        "- Visualize everything - plots make patterns obvious\n",
        "- Compare your results with classmates - different approaches are valuable!\n",
        "\n",
        "**📝 Reflection Questions**:\n",
        "1. In what situations might drift detection fail?\n",
        "2. How would you handle drift in a system with delayed labels (e.g., loan defaults)?\n",
        "3. What business considerations affect your choice of drift detector sensitivity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Additional Tools and Resources\n",
        "\n",
        "### Beyond River: Other Drift Detection Libraries\n",
        "\n",
        "While we used the `river` library in this notebook, here are other excellent tools for production ML monitoring:\n",
        "\n",
        "#### **1. Evidently AI** 🔍\n",
        "```python\n",
        "# pip install evidently\n",
        "from evidently.report import Report\n",
        "from evidently.metric_preset import DataDriftPreset\n",
        "```\n",
        "- **Best For**: Comprehensive monitoring dashboards and reports\n",
        "- **Features**: \n",
        "  - Visual reports (HTML)\n",
        "  - Multiple statistical tests\n",
        "  - Feature-level drift analysis\n",
        "  - Model performance monitoring\n",
        "- **Use Case**: Batch monitoring, generating reports for stakeholders\n",
        "\n",
        "#### **2. Alibi Detect** 🎯\n",
        "```python\n",
        "# pip install alibi-detect\n",
        "from alibi_detect.cd import MMDDrift, KSDrift\n",
        "```\n",
        "- **Best For**: Advanced statistical tests and deep learning drift detection\n",
        "- **Features**:\n",
        "  - Maximum Mean Discrepancy (MMD)\n",
        "  - Kolmogorov-Smirnov test\n",
        "  - Chi-square test\n",
        "  - Deep learning-based detectors\n",
        "- **Use Case**: Research, complex high-dimensional data\n",
        "\n",
        "#### **3. NannyML** 👶\n",
        "```python\n",
        "# pip install nannyml\n",
        "import nannyml as nml\n",
        "```\n",
        "- **Best For**: Post-deployment monitoring without ground truth labels\n",
        "- **Features**:\n",
        "  - Estimated performance (without labels!)\n",
        "  - Multivariate drift detection\n",
        "  - Business value tracking\n",
        "- **Use Case**: When labels are delayed or expensive\n",
        "\n",
        "#### **4. Great Expectations** ✅\n",
        "```python\n",
        "# pip install great_expectations\n",
        "import great_expectations as gx\n",
        "```\n",
        "- **Best For**: Data quality and validation\n",
        "- **Features**:\n",
        "  - Data profiling\n",
        "  - Expectation-based testing\n",
        "  - Integration with data pipelines\n",
        "- **Use Case**: Data quality monitoring, upstream drift detection\n",
        "\n",
        "### Statistical Tests for Drift Detection\n",
        "\n",
        "Beyond ADWIN and DDM, here are common statistical approaches:\n",
        "\n",
        "| Test | Data Type | What It Detects | Python Implementation |\n",
        "|------|-----------|-----------------|----------------------|\n",
        "| **Kolmogorov-Smirnov** | Continuous | Distribution changes | `scipy.stats.ks_2samp()` |\n",
        "| **Chi-Square** | Categorical | Category distribution | `scipy.stats.chisquare()` |\n",
        "| **Population Stability Index (PSI)** | Both | Distribution shift | Custom or `evidently` |\n",
        "| **Jensen-Shannon Divergence** | Both | Distribution similarity | `scipy.spatial.distance.jensenshannon()` |\n",
        "| **Wasserstein Distance** | Continuous | Distribution difference | `scipy.stats.wasserstein_distance()` |\n",
        "\n",
        "### Quick Example: Using Kolmogorov-Smirnov Test\n",
        "\n",
        "```python\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# Compare two distributions\n",
        "stat, p_value = ks_2samp(stream_1, stream_2)\n",
        "\n",
        "if p_value < 0.05:  # 95% confidence\n",
        "    print(f\"Drift detected! p-value: {p_value:.4f}\")\n",
        "else:\n",
        "    print(f\"No significant drift. p-value: {p_value:.4f}\")\n",
        "```\n",
        "\n",
        "### Production Monitoring Architecture\n",
        "\n",
        "**Recommended Setup for Production ML Systems:**\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────┐\n",
        "│         Production ML System                     │\n",
        "├─────────────────────────────────────────────────┤\n",
        "│                                                  │\n",
        "│  📊 Data Collection Layer                       │\n",
        "│     ├── Log predictions                         │\n",
        "│     ├── Log features                            │\n",
        "│     └── Log ground truth (when available)       │\n",
        "│                                                  │\n",
        "│  🔍 Monitoring Layer                            │\n",
        "│     ├── Feature drift detectors (ADWIN)         │\n",
        "│     ├── Prediction drift monitors               │\n",
        "│     ├── Performance trackers (DDM)              │\n",
        "│     └── Business metric monitors                │\n",
        "│                                                  │\n",
        "│  📈 Alerting Layer                              │\n",
        "│     ├── Dashboard (Grafana/Evidently)           │\n",
        "│     ├── Alerts (PagerDuty/Slack)               │\n",
        "│     └── Reports (Weekly summaries)              │\n",
        "│                                                  │\n",
        "│  🔄 Response Layer                              │\n",
        "│     ├── Automated retraining triggers           │\n",
        "│     ├── Model rollback capabilities             │\n",
        "│     └── A/B testing framework                   │\n",
        "│                                                  │\n",
        "└─────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "**Academic Papers:**\n",
        "- \"Learning under Concept Drift: A Review\" - Gama et al. (2014)\n",
        "- \"A Survey on Concept Drift Adaptation\" - Lu et al. (2018)\n",
        "- \"ADWIN: Adaptive Windowing for Mining Data Streams\" - Bifet & Gavaldà (2007)\n",
        "\n",
        "**Online Resources:**\n",
        "- River Documentation: https://riverml.xyz/\n",
        "- Evidently AI Blog: https://evidentlyai.com/blog\n",
        "- Made With ML - MLOps: https://madewithml.com/\n",
        "- Google's ML Crash Course: https://developers.google.com/machine-learning/crash-course\n",
        "\n",
        "**Tools to Explore:**\n",
        "- MLflow for experiment tracking\n",
        "- Weights & Biases for model monitoring\n",
        "- WhyLabs for data logging and monitoring\n",
        "- Arize AI for ML observability\n",
        "\n",
        "### Next Steps for Your Learning Journey\n",
        "\n",
        "1. ✅ **Complete the practice exercises** in Section 11\n",
        "2. 📚 **Read about different drift types** in academic papers\n",
        "3. 🛠️ **Implement monitoring** in your own ML projects\n",
        "4. 🔬 **Experiment with different detectors** on real datasets\n",
        "5. 🏗️ **Build a complete monitoring pipeline** using Evidently or NannyML\n",
        "6. 🚀 **Deploy a model with drift detection** in production\n",
        "\n",
        "Remember: **Drift detection is not optional for production ML systems** - it's a critical component of responsible AI deployment!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_drift_demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
