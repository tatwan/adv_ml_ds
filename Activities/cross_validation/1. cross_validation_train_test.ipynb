{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ebb2f",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- Why we need to split data into training and testing sets\n",
    "- The difference between training error and testing error\n",
    "- What overfitting means and why it's problematic\n",
    "- How cross-validation provides more robust model evaluation\n",
    "- How to interpret cross-validation results and their variability\n",
    "\n",
    "## What is Cross-Validation?\n",
    "\n",
    "Cross-validation is a fundamental technique in machine learning that helps us:\n",
    "1. **Evaluate model performance more reliably** than a single train-test split\n",
    "2. **Detect overfitting** by comparing training and testing errors\n",
    "3. **Estimate how well our model will generalize** to new, unseen data\n",
    "4. **Choose between different models** or hyperparameters\n",
    "\n",
    "Think of it as getting multiple \"second opinions\" about your model's performance rather than relying on just one test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "data, target = housing.data, housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaaec33",
   "metadata": {},
   "source": [
    "## Dataset Introduction\n",
    "\n",
    "We'll use the California Housing dataset to demonstrate cross-validation concepts. This dataset is ideal for learning because:\n",
    "- It's a **regression problem** (predicting continuous house prices)\n",
    "- It has real-world complexity with multiple features\n",
    "- It's large enough to show meaningful cross-validation results\n",
    "- The target variable (house prices) is something we can intuitively understand\n",
    "\n",
    "Let's load the data and explore its basic structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the aim is to predict the median value of houses in an area\n",
    "in California. The features collected are based on general real-estate and\n",
    "geographical information.\n",
    "\n",
    "Therefore, the task to solve is different from the one shown in the previous\n",
    "notebook. The target to be predicted is a continuous variable and not anymore\n",
    "discrete. This task is called regression.\n",
    "\n",
    "Thus, we will use a predictive model specific to regression and not to\n",
    "classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify future visualization, let's transform the prices from the 100\n",
    "(k\\\\$) range to the thousand dollars (k\\\\$) range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target *= 100\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c2289",
   "metadata": {},
   "source": [
    "## The Central Problem in Machine Learning\n",
    "\n",
    "Before we dive into cross-validation, let's understand the fundamental challenge we're trying to solve:\n",
    "\n",
    "**The Goal**: Build a model that performs well on *new, unseen data* (not just the data we trained on)\n",
    "\n",
    "**The Challenge**: We only have a limited dataset to work with\n",
    "\n",
    "**The Risk**: Our model might just memorize the training data instead of learning generalizable patterns\n",
    "\n",
    "This is where the concepts of **training error** vs **testing error** become crucial. Let's see this in action:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training error vs testing error\n",
    "\n",
    "To solve this regression task, we will use a decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "regressor.fit(data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the regressor, we would like to know its potential\n",
    "generalization performance once deployed in production. For this purpose, we\n",
    "use the mean absolute error, which gives us an error in the native unit, i.e.\n",
    "k\\\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "target_predicted = regressor.predict(data)\n",
    "score = mean_absolute_error(target, target_predicted)\n",
    "print(f\"On average, our regressor makes an error of {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70134fb9",
   "metadata": {},
   "source": [
    "### Red Flag Alert! \n",
    "\n",
    "**Question for you**: Does this result seem realistic? Should we celebrate this \"perfect\" performance?\n",
    "\n",
    "**The Answer**: No! This is actually a warning sign. Here's why:\n",
    "\n",
    "1. **Real-world data is noisy** - Perfect predictions are extremely rare\n",
    "2. **Our model likely memorized** rather than learned patterns\n",
    "3. **This won't generalize** to new houses\n",
    "\n",
    "This is a classic example of **overfitting**. Let's investigate further..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We get perfect prediction with no error. It is too optimistic and almost\n",
    "always revealing a methodological problem when doing machine learning.\n",
    "\n",
    "Indeed, we trained and predicted on the same dataset. Since our decision tree\n",
    "was fully grown, every sample in the dataset is stored in a leaf node.\n",
    "Therefore, our decision tree fully memorized the dataset given during `fit`\n",
    "and therefore made no error when predicting.\n",
    "\n",
    "This error computed above is called the **empirical error** or **training\n",
    "error**.\n",
    "\n",
    "\n",
    "We trained a predictive model to minimize the training error but our aim is to\n",
    "minimize the error on data that has not been seen during training.\n",
    "\n",
    "This error is also called the **generalization error** or the \"true\" **testing\n",
    "error**.\n",
    "\n",
    "\n",
    "Thus, the most basic evaluation involves:\n",
    "\n",
    "* splitting our dataset into two subsets: a training set and a testing set;\n",
    "* fitting the model on the training set;\n",
    "* estimating the training error on the training set;\n",
    "* estimating the testing error on the testing set.\n",
    "\n",
    "So let's split our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe810d",
   "metadata": {},
   "source": [
    "## The Solution: Train-Test Split\n",
    "\n",
    "To get a realistic assessment of our model's performance, we need to:\n",
    "\n",
    "1. **Hide some data** from the model during training (test set)\n",
    "2. **Train only on the remaining data** (training set) \n",
    "3. **Evaluate on the hidden data** to simulate real-world performance\n",
    "\n",
    "### Key Principles:\n",
    "- **Never** let your model see the test data during training\n",
    "- The test set represents \"future\" data your model hasn't seen\n",
    "- This gives us an honest estimate of generalization performance\n",
    "\n",
    "### Typical Split Ratios:\n",
    "- **75% training, 25% testing** (what we'll use)\n",
    "- Sometimes 80/20 or 70/30 depending on dataset size\n",
    "\n",
    "Let's implement this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we estimate the different types of errors. Let's start by computing\n",
    "the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = regressor.predict(data_train)\n",
    "score = mean_absolute_error(target_train, target_predicted)\n",
    "print(f\"The training error of our model is {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the same phenomena as in the previous experiment: our model\n",
    "memorized the training set. However, we now compute the testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = regressor.predict(data_test)\n",
    "score = mean_absolute_error(target_test, target_predicted)\n",
    "print(f\"The testing error of our model is {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This testing error is actually about what we would expect from our model if it\n",
    "was used in a production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67ffb5",
   "metadata": {},
   "source": [
    "### But Wait... Is One Split Enough?\n",
    "\n",
    "We got a testing error of ~45k$, but **how confident should we be in this number?**\n",
    "\n",
    "Consider these concerns:\n",
    "1. **What if we got \"lucky\" with our split?** Maybe the test set happened to contain easy-to-predict houses\n",
    "2. **What if we got \"unlucky\"?** Maybe the test set contained unusually difficult cases\n",
    "3. **Would a different random split give similar results?**\n",
    "\n",
    "**The Answer**: We need multiple splits to get a more reliable estimate. This is where **cross-validation** comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability of the cross-validation estimates\n",
    "\n",
    "When doing a single train-test split we don't give any indication regarding\n",
    "the robustness of the evaluation of our predictive model: in particular, if\n",
    "the test set is small, this estimate of the testing error will be unstable and\n",
    "wouldn't reflect the \"true error rate\" we would have observed with the same\n",
    "model on an unlimited amount of test data.\n",
    "\n",
    "For instance, we could have been lucky when we did our random split of our\n",
    "limited dataset and isolated some of the easiest cases to predict in the\n",
    "testing set just by chance: the estimation of the testing error would be\n",
    "overly optimistic, in this case.\n",
    "\n",
    "**Cross-validation** allows estimating the robustness of a predictive model by\n",
    "repeating the splitting procedure. It will give several training and testing\n",
    "errors and thus some **estimate of the variability of the model generalization\n",
    "performance**.\n",
    "\n",
    "There are [different cross-validation\n",
    "strategies](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators),\n",
    "for now we are going to focus on one called \"shuffle-split\". At each iteration\n",
    "of this strategy we:\n",
    "\n",
    "- randomly shuffle the order of the samples of a copy of the full dataset;\n",
    "- split the shuffled dataset into a train and a test set;\n",
    "- train a new model on the train set;\n",
    "- evaluate the testing error on the test set.\n",
    "\n",
    "We repeat this procedure `n_splits` times. Keep in mind that the computational\n",
    "cost increases with `n_splits`.\n",
    "\n",
    "\n",
    "\n",
    "In this case we will set `n_splits=40`, meaning that we will train 40 models\n",
    "in total and all of them will be discarded: we just record their\n",
    "generalization performance on each variant of the test set.\n",
    "\n",
    "To evaluate the generalization performance of our regressor, we can use\n",
    "[`sklearn.model_selection.cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)\n",
    "with a\n",
    "[`sklearn.model_selection.ShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html)\n",
    "object:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df154b42",
   "metadata": {},
   "source": [
    "### Understanding the Cross-Validation Process\n",
    "\n",
    "Imagine we repeat our train-test split process multiple times:\n",
    "\n",
    "```\n",
    "Split 1: [Train: 70%] [Test: 30%] → Model 1 → Test Error 1\n",
    "Split 2: [Train: 70%] [Test: 30%] → Model 2 → Test Error 2  \n",
    "Split 3: [Train: 70%] [Test: 30%] → Model 3 → Test Error 3\n",
    "...\n",
    "Split 40: [Train: 70%] [Test: 30%] → Model 40 → Test Error 40\n",
    "```\n",
    "\n",
    "**What we get**:\n",
    "- 40 different test error estimates\n",
    "- A **distribution** of performance rather than a single number\n",
    "- **Mean**: Our best estimate of model performance  \n",
    "- **Standard deviation**: How much the performance varies\n",
    "\n",
    "**Why this is better**:\n",
    "- More robust estimate (not dependent on one \"lucky\" or \"unlucky\" split)\n",
    "- Confidence intervals around our performance estimate\n",
    "- Better understanding of model stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=40, test_size=0.3, random_state=0)\n",
    "cv_results = cross_validate(\n",
    "    regressor, data, target, cv=cv, scoring=\"neg_mean_absolute_error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results `cv_results` are stored into a Python dictionary. We will convert\n",
    "it into a pandas dataframe to ease visualization and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Tip</p>\n",
    "<p>A score is a metric for which higher values mean better results. On the\n",
    "contrary, an error is a metric for which lower values mean better results.\n",
    "The parameter <tt class=\"docutils literal\">scoring</tt> in <tt class=\"docutils literal\">cross_validate</tt> always expect a function that is\n",
    "a score.</p>\n",
    "<p class=\"last\">To make it easy, all error metrics in scikit-learn, like\n",
    "<tt class=\"docutils literal\">mean_absolute_error</tt>, can be transformed into a score to be used in\n",
    "<tt class=\"docutils literal\">cross_validate</tt>. To do so, you need to pass a string of the error metric\n",
    "with an additional <tt class=\"docutils literal\">neg_</tt> string at the front to the parameter <tt class=\"docutils literal\">scoring</tt>;\n",
    "for instance <tt class=\"docutils literal\"><span class=\"pre\">scoring=\"neg_mean_absolute_error\"</span></tt>. In this case, the negative\n",
    "of the mean absolute error will be computed which would be equivalent to a\n",
    "score.</p>\n",
    "</div>\n",
    "\n",
    "Let us revert the negation to get the actual error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"test_error\"] = -cv_results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results reported by the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get timing information to fit and predict at each cross-validation\n",
    "iteration. Also, we get the test score, which corresponds to the testing error\n",
    "on each of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 40 entries in our resulting dataframe because we performed 40 splits.\n",
    "Therefore, we can show the testing error distribution and thus, have an\n",
    "estimate of its variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv_results[\"test_error\"].plot.hist(bins=10, edgecolor=\"black\")\n",
    "plt.xlabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Test error distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the testing error is clustered around 47 k\\\\$ and ranges from\n",
    "43 k\\\\$ to 50 k\\\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The mean cross-validated testing error is: \"\n",
    "    f\"{cv_results['test_error'].mean():.2f} k$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The standard deviation of the testing error is: \"\n",
    "    f\"{cv_results['test_error'].std():.2f} k$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb492d",
   "metadata": {},
   "source": [
    "### How to Interpret These Results\n",
    "\n",
    "**Our cross-validation results: 46.36 ± 1.14 k$**\n",
    "\n",
    "**What this means**:\n",
    "- **46.36 k$**: Our best estimate of the model's typical error\n",
    "- **± 1.14 k$**: The uncertainty in this estimate (95% of results fall within ~2 standard deviations)\n",
    "- **Range**: We expect most test errors to fall between ~44-48 k$\n",
    "\n",
    "**Why the small standard deviation is good**:\n",
    "- It means our model's performance is **consistent** across different data splits\n",
    "- We can be **confident** in our performance estimate\n",
    "- The model is **stable** (not highly dependent on which specific data it sees)\n",
    "\n",
    "**Key Question**: Is this performance good enough for our use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the standard deviation is much smaller than the mean: we could\n",
    "summarize that our cross-validation estimate of the testing error is 46.36 ±\n",
    "1.17 k\\\\$.\n",
    "\n",
    "If we were to train a single model on the full dataset (without\n",
    "cross-validation) and then later had access to an unlimited amount of test\n",
    "data, we would expect its true testing error to fall close to that region.\n",
    "\n",
    "While this information is interesting in itself, it should be contrasted to\n",
    "the scale of the natural variability of the vector `target` in our dataset.\n",
    "\n",
    "Let us plot the distribution of the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.plot.hist(bins=20, edgecolor=\"black\")\n",
    "plt.xlabel(\"Median House Value (k$)\")\n",
    "_ = plt.title(\"Target distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The standard deviation of the target is: {target.std():.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable ranges from close to 0 k\\\\$ up to 500 k\\\\$ and, with a\n",
    "standard deviation around 115 k\\\\$.\n",
    "\n",
    "We notice that the mean estimate of the testing error obtained by\n",
    "cross-validation is a bit smaller than the natural scale of variation of the\n",
    "target variable. Furthermore, the standard deviation of the cross validation\n",
    "estimate of the testing error is even smaller.\n",
    "\n",
    "This is a good start, but not necessarily enough to decide whether the\n",
    "generalization performance is good enough to make our prediction useful in\n",
    "practice.\n",
    "\n",
    "We recall that our model makes, on average, an error around 47 k\\\\$. With this\n",
    "information and looking at the target distribution, such an error might be\n",
    "acceptable when predicting houses with a 500 k\\\\$. However, it would be an\n",
    "issue with a house with a value of 50 k\\\\$. Thus, this indicates that our\n",
    "metric (Mean Absolute Error) is not ideal.\n",
    "\n",
    "We might instead choose a metric relative to the target value to predict: the\n",
    "mean absolute percentage error would have been a much better choice.\n",
    "\n",
    "But in all cases, an error of 47 k\\\\$ might be too large to automatically use\n",
    "our model to tag house values without expert supervision.\n",
    "\n",
    "## More detail regarding `cross_validate`\n",
    "\n",
    "During cross-validation, many models are trained and evaluated. Indeed, the\n",
    "number of elements in each array of the output of `cross_validate` is a result\n",
    "from one of these `fit`/`score` procedures. To make it explicit, it is\n",
    "possible to retrieve these fitted models for each of the splits/folds by\n",
    "passing the option `return_estimator=True` in `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(regressor, data, target, return_estimator=True)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"estimator\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five decision tree regressors corresponds to the five fitted decision\n",
    "trees on the different folds. Having access to these regressors is handy\n",
    "because it allows to inspect the internal fitted parameters of these\n",
    "regressors.\n",
    "\n",
    "In the case where you only are interested in the test score, scikit-learn\n",
    "provide a `cross_val_score` function. It is identical to calling the\n",
    "`cross_validate` function and to select the `test_score` only (as we\n",
    "extensively did in the previous notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(regressor, data, target)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772c209",
   "metadata": {},
   "source": [
    "## Quick Check: Test Your Understanding\n",
    "\n",
    "Before we wrap up, see if you can answer these questions:\n",
    "\n",
    "1. **Why was our first model's \"perfect\" performance suspicious?**\n",
    "2. **What's the difference between training error and testing error?**\n",
    "3. **Why is cross-validation better than a single train-test split?**\n",
    "4. **What does the standard deviation in our cross-validation results tell us?**\n",
    "5. **How would you decide if our 46k$ error is \"good enough\"?**\n",
    "\n",
    "Think about your answers, then continue to the summary below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**1. The Overfitting Problem**\n",
    "- Models can memorize training data instead of learning patterns\n",
    "- Perfect training performance is usually a red flag\n",
    "- Always evaluate on unseen data\n",
    "\n",
    "**2. Train-Test Split Fundamentals**  \n",
    "- Essential for honest performance evaluation\n",
    "- Training error ≠ testing error (and testing error is what matters)\n",
    "- Never let your model see test data during training\n",
    "\n",
    "**3. Cross-Validation Benefits**\n",
    "- More robust than single train-test split\n",
    "- Provides performance distribution, not just single number\n",
    "- Helps detect unstable models\n",
    "- Better confidence in model selection\n",
    "\n",
    "**4. Practical Insights**\n",
    "- Our model: 46.36 ± 1.17 k$ error (quite stable!)\n",
    "- Context matters: compare error to target scale (~115 k$ std)\n",
    "- Consider business impact: Is 47k$ error acceptable for your use case?\n",
    "\n",
    "### Next Steps\n",
    "- Try different cross-validation strategies (K-fold, stratified, etc.)\n",
    "- Compare multiple models using cross-validation\n",
    "- Explore hyperparameter tuning with nested cross-validation\n",
    "- Learn about bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ffe03",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
