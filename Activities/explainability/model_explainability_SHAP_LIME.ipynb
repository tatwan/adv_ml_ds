{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explainability and Regulation using SHAP and LIME\n",
    "\n",
    "**Author:** Machine Learning Lab  \n",
    "**Topic:** Explainability and Interpretability in Machine Learning  \n",
    "**Focus:** SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Model Interpretability\n",
    "\n",
    "In the realm of machine learning, the predictive power of complex models often comes at the cost of transparency. As models become more sophisticated—transitioning from simple linear regression to deep neural networks and ensemble methods—their decision-making processes can become opaque, creating what is commonly referred to as a **\"black box\" model**. This lack of transparency poses significant challenges, particularly in high-stakes domains such as healthcare, finance, criminal justice, and autonomous systems.\n",
    "\n",
    "### Why Interpretability Matters\n",
    "\n",
    "Model interpretability, or explainability, refers to the ability to understand and explain the predictions made by a machine learning model in a way that is comprehensible to humans. This capability is crucial for several reasons:\n",
    "\n",
    "**Building Trust:** When stakeholders can understand why a model makes a particular decision, they are more likely to trust its predictions. In medical diagnosis, for example, a doctor needs to understand why an AI system recommends a specific treatment before acting on that recommendation.\n",
    "\n",
    "**Ensuring Fairness:** Interpretability methods enable us to identify and mitigate biases in our models. Without the ability to examine how features influence predictions, we risk perpetuating or amplifying existing societal biases. For instance, a lending model might unfairly discriminate against certain demographic groups if we cannot inspect its decision-making process.\n",
    "\n",
    "**Improving Robustness:** By understanding how a model works internally, we can identify its weaknesses and make it more robust to adversarial attacks and unexpected inputs. This is particularly important in security-critical applications.\n",
    "\n",
    "**Complying with Regulations:** Legal frameworks increasingly mandate explainability. The European Union's General Data Protection Regulation (GDPR) includes a \"right to explanation,\" which requires that individuals be able to obtain meaningful information about the logic involved in automated decisions that significantly affect them. Similarly, regulations in finance (such as fair lending laws) and healthcare (FDA requirements for medical devices) emphasize the importance of model transparency.\n",
    "\n",
    "### Types of Interpretability\n",
    "\n",
    "Interpretability can be categorized along two dimensions:\n",
    "\n",
    "**Global vs. Local Interpretability:**\n",
    "- **Global interpretability** provides an understanding of the model's behavior across the entire feature space. It answers questions like \"Which features are most important overall?\"\n",
    "- **Local interpretability** explains individual predictions. It answers \"Why did the model make this specific prediction for this particular instance?\"\n",
    "\n",
    "**Model-specific vs. Model-agnostic Methods:**\n",
    "- **Model-specific** methods work only with particular types of models (e.g., examining coefficients in linear regression)\n",
    "- **Model-agnostic** methods can be applied to any model, treating it as a black box\n",
    "\n",
    "In this lab, we will focus on two powerful **model-agnostic** techniques that provide both local and global interpretability: **SHAP** and **LIME**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Installation\n",
    "\n",
    "Before we begin, we need to install the necessary Python libraries. Run the following command in your terminal or in a code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install pandas scikit-learn matplotlib numpy shap lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import all the libraries we will be using throughout this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.datasets import fetch_california_housing, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "\n",
    "# SHAP and LIME\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Initialize SHAP for notebook visualization\n",
    "shap.initjs()\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Selection and Preparation\n",
    "\n",
    "For this lab, we will work with two real-world datasets from `sklearn.datasets`. Using real datasets instead of synthetic data provides more meaningful insights and better prepares you for practical applications.\n",
    "\n",
    "### Dataset 1: California Housing (Regression)\n",
    "\n",
    "The **California Housing dataset** contains information from the 1990 California census. The goal is to predict the median house value for California districts based on features such as median income, house age, average rooms, and location. This is a regression problem where we predict a continuous value.\n",
    "\n",
    "**Features:**\n",
    "- MedInc: Median income in block group\n",
    "- HouseAge: Median house age in block group\n",
    "- AveRooms: Average number of rooms per household\n",
    "- AveBedrms: Average number of bedrooms per household\n",
    "- Population: Block group population\n",
    "- AveOccup: Average number of household members\n",
    "- Latitude: Block group latitude\n",
    "- Longitude: Block group longitude\n",
    "\n",
    "### Dataset 2: Breast Cancer Wisconsin (Classification)\n",
    "\n",
    "The **Breast Cancer Wisconsin dataset** contains features computed from digitized images of fine needle aspirates of breast masses. The goal is to predict whether a tumor is malignant or benign. This is a binary classification problem with significant real-world implications for medical diagnosis.\n",
    "\n",
    "**Features:** 30 numerical features describing characteristics of cell nuclei (mean radius, mean texture, mean perimeter, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset for regression\n",
    "housing = fetch_california_housing()\n",
    "X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y_housing = housing.target\n",
    "\n",
    "print(\"California Housing Dataset:\")\n",
    "print(f\"  Shape: {X_housing.shape}\")\n",
    "print(f\"  Features: {list(X_housing.columns)}\")\n",
    "print(f\"  Target range: [{y_housing.min():.2f}, {y_housing.max():.2f}]\")\n",
    "print()\n",
    "\n",
    "# Load the Breast Cancer dataset for classification\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y_cancer = cancer.target\n",
    "\n",
    "print(\"Breast Cancer Wisconsin Dataset:\")\n",
    "print(f\"  Shape: {X_cancer.shape}\")\n",
    "print(f\"  Classes: {cancer.target_names}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_cancer)}\")\n",
    "print()\n",
    "\n",
    "# Split both datasets into training and testing sets\n",
    "X_housing_train, X_housing_test, y_housing_train, y_housing_test = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_cancer_train, X_cancer_test, y_cancer_train, y_cancer_test = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/Test Split:\")\n",
    "print(f\"  Housing: {X_housing_train.shape[0]} train, {X_housing_test.shape[0]} test\")\n",
    "print(f\"  Cancer: {X_cancer_train.shape[0]} train, {X_cancer_test.shape[0]} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "We will train **Random Forest** models for both tasks. Random Forests are ensemble methods that combine multiple decision trees to create powerful, non-linear models. While individual decision trees are interpretable, Random Forests with hundreds of trees become complex \"black boxes,\" making them ideal candidates for explainability techniques.\n",
    "\n",
    "### Why Random Forests?\n",
    "\n",
    "Random Forests are excellent for this lab because they:\n",
    "- Achieve high predictive performance\n",
    "- Capture complex, non-linear relationships\n",
    "- Are widely used in industry\n",
    "- Represent the type of \"black box\" model that requires explainability tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Regressor on the California Housing dataset\n",
    "print(\"Training Random Forest Regressor...\")\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_regressor.fit(X_housing_train, y_housing_train)\n",
    "\n",
    "# Evaluate the regressor\n",
    "y_housing_pred = rf_regressor.predict(X_housing_test)\n",
    "rmse_housing = np.sqrt(mean_squared_error(y_housing_test, y_housing_pred))\n",
    "print(f\"  RMSE: {rmse_housing:.4f}\")\n",
    "print()\n",
    "\n",
    "# Train a Random Forest Classifier on the Breast Cancer dataset\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_classifier.fit(X_cancer_train, y_cancer_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_cancer_pred = rf_classifier.predict(X_cancer_test)\n",
    "acc_cancer = accuracy_score(y_cancer_test, y_cancer_pred)\n",
    "print(f\"  Accuracy: {acc_cancer:.4f}\")\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_cancer_test, y_cancer_pred, target_names=cancer.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "### 5.1 Introduction to SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a unified framework for interpreting predictions based on **Shapley values** from cooperative game theory. Developed by Lloyd Shapley (who won the Nobel Prize in Economics for this work), Shapley values provide a principled way to distribute a \"payout\" among players based on their contributions to a coalition.\n",
    "\n",
    "### The Game Theory Connection\n",
    "\n",
    "In the context of machine learning:\n",
    "- The **\"game\"** is the prediction task\n",
    "- The **\"players\"** are the input features\n",
    "- The **\"payout\"** is the model's prediction\n",
    "- The **\"contribution\"** of each player is how much each feature contributes to the prediction\n",
    "\n",
    "### Key Properties of SHAP Values\n",
    "\n",
    "SHAP values have several desirable mathematical properties:\n",
    "\n",
    "1. **Local Accuracy:** The sum of SHAP values equals the difference between the model's prediction and the expected value (baseline)\n",
    "2. **Consistency:** If a model changes so that a feature's contribution increases, its SHAP value should not decrease\n",
    "3. **Missingness:** Features that don't affect the prediction have zero SHAP value\n",
    "\n",
    "### Interpreting SHAP Values\n",
    "\n",
    "A SHAP value for a feature represents:\n",
    "- **Magnitude:** How much the feature contributes to the prediction\n",
    "- **Direction:** Whether the feature pushes the prediction higher (positive) or lower (negative)\n",
    "- **Baseline:** All contributions are measured relative to the expected value of the model output\n",
    "\n",
    "For example, if the expected house price is \\$200,000 and a specific house is predicted at \\$250,000, SHAP values might show that high median income (+\\$60,000) and good location (+\\$20,000) increase the price, while old age (-\\$30,000) decreases it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SHAP for Regression (California Housing)\n",
    "\n",
    "Let's start by explaining the predictions of our Random Forest Regressor. We'll use SHAP's `TreeExplainer`, which is optimized for tree-based models and provides exact Shapley values efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer for the Random Forest Regressor\n",
    "# Note: We use a subset of test data for faster computation\n",
    "print(\"Creating SHAP explainer for regression model...\")\n",
    "explainer_reg = shap.TreeExplainer(rf_regressor)\n",
    "\n",
    "# Compute SHAP values for a subset of test data (for speed)\n",
    "X_housing_test_sample = X_housing_test.iloc[:100]\n",
    "shap_values_reg = explainer_reg.shap_values(X_housing_test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"✓ SHAP values computed for {X_housing_test_sample.shape[0]} instances\")\n",
    "print(f\"  Expected value (baseline): {explainer_reg.expected_value}\")\n",
    "print(f\"  SHAP values shape: {shap_values_reg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Global Feature Importance\n",
    "\n",
    "The **summary plot** shows the distribution of SHAP values for each feature across all instances. Features are ranked by importance (mean absolute SHAP value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot (bar) - shows average impact of each feature\n",
    "shap.summary_plot(shap_values_reg, X_housing_test_sample, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📊 What to Expect:**\n",
    "\n",
    "This **bar chart** will display the average absolute SHAP value for each feature, showing you which features have the **strongest overall impact** on house price predictions across all samples.\n",
    "\n",
    "**How to Interpret:**\n",
    "- **Vertical axis:** Lists features ranked by importance (most important at the top)\n",
    "- **Horizontal axis:** Shows the mean absolute SHAP value (magnitude of impact)\n",
    "- **Longer bars** = more influential features in determining house prices\n",
    "- This gives you a **global view** of feature importance across the entire dataset\n",
    "\n",
    "**Key Insight:** Look for which features consistently drive predictions—these are the model's primary decision-makers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Feature Impact Distribution\n",
    "\n",
    "The **beeswarm plot** (or dot plot) shows not just importance, but also:\n",
    "- The distribution of impacts (spread of dots)\n",
    "- The direction of effects (positive/negative)\n",
    "- Feature values (color: red = high, blue = low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot (dot) - shows distribution of SHAP values\n",
    "shap.summary_plot(shap_values_reg, X_housing_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📊 What to Expect:**\n",
    "\n",
    "This **beeswarm plot** (also called a SHAP summary plot) provides a richer view than the bar chart by showing not just importance, but also **direction and magnitude** of feature effects.\n",
    "\n",
    "**How to Interpret:**\n",
    "- **Vertical axis:** Features ranked by importance (same as bar chart)\n",
    "- **Horizontal axis:** SHAP value (negative = decreases price, positive = increases price)\n",
    "- **Each dot:** Represents one instance from the dataset\n",
    "- **Color:** Indicates the feature value\n",
    "  - 🔴 **Red (pink)** = high feature value\n",
    "  - 🔵 **Blue** = low feature value\n",
    "- **Dot position:** Shows the SHAP value (impact on prediction)\n",
    "\n",
    "**Key Insights to Look For:**\n",
    "- **Spread of dots:** Wide spread = feature has varying impact across different instances\n",
    "- **Color patterns:** If red dots are on the right (positive SHAP), high values of that feature increase predictions\n",
    "- **Example:** For MedInc (median income), expect red dots on the right—higher income increases house prices\n",
    "\n",
    "**Reading Example:** If you see red dots on the right for a feature, it means \"high values of this feature push predictions UP.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 Individual Prediction Explanation (Waterfall Plot)\n",
    "\n",
    "The **waterfall plot** explains a single prediction by showing how each feature pushes the prediction from the baseline (expected value) to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_housing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single prediction with waterfall plot\n",
    "instance_idx = 0\n",
    "print(f\"Explaining instance {instance_idx}:\")\n",
    "print(f\"  Actual value: {y_housing_test[instance_idx]}\")\n",
    "print(f\"  Predicted value: {rf_regressor.predict(X_housing_test_sample.iloc[[instance_idx]])[0]}\")\n",
    "print()\n",
    "\n",
    "shap.plots.waterfall(shap.Explanation(\n",
    "    values=shap_values_reg[instance_idx],\n",
    "    base_values=explainer_reg.expected_value,\n",
    "    data=X_housing_test_sample.iloc[instance_idx],\n",
    "    feature_names=X_housing_test_sample.columns.tolist()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📊 What to Expect:**\n",
    "\n",
    "This **waterfall plot** will show you a **step-by-step explanation** of how the model arrived at its prediction for one specific house, starting from the baseline (average) prediction.\n",
    "\n",
    "**How to Interpret:**\n",
    "- **Bottom value (E[f(X)]):** The baseline/expected value—the average prediction across all houses\n",
    "- **Each bar:** Shows how one feature pushes the prediction UP (red/positive) or DOWN (blue/negative)\n",
    "- **Feature values:** Listed next to each feature name (e.g., \"MedInc = 3.5\")\n",
    "- **Top value (f(x)):** The final prediction for this specific house\n",
    "- **Arrows:** Show the cumulative effect as features are added\n",
    "\n",
    "**Reading the Story:**\n",
    "Start at the baseline and follow the bars upward. Each bar tells you:\n",
    "- **\"This feature increases the price by X\"** (red bars going right)\n",
    "- **\"This feature decreases the price by Y\"** (blue bars going left)\n",
    "\n",
    "**Key Insight:** This answers \"Why did the model predict THIS price for THIS house?\"—you can see exactly which features drove the prediction up or down from the average.\n",
    "\n",
    "**Example interpretation:** If you see MedInc with a large red bar, it means \"High median income in this area significantly increased the predicted price for this specific house.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 Dependence Plots\n",
    "\n",
    "**Dependence plots** show how a single feature affects predictions across its range of values. They can also reveal interactions between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plot for MedInc (median income)\n",
    "shap.dependence_plot(\"MedInc\", shap_values_reg, X_housing_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📊 What to Expect:**\n",
    "\n",
    "This **dependence plot** will show you the **relationship between a feature's value and its impact** on predictions, helping you understand how the model uses that feature across its entire range.\n",
    "\n",
    "**How to Interpret:**\n",
    "- **Horizontal axis:** The actual values of the feature (e.g., MedInc values from low to high)\n",
    "- **Vertical axis:** SHAP value (impact on prediction)\n",
    "- **Each dot:** Represents one house in the dataset\n",
    "- **Dot color:** Represents another feature that may interact with the main feature\n",
    "- **Trend line:** Shows the overall relationship\n",
    "\n",
    "**Key Patterns to Look For:**\n",
    "\n",
    "1. **Linear relationship:** Straight line = feature has a consistent, proportional effect\n",
    "2. **Non-linear relationship:** Curved line = feature's effect varies across its range\n",
    "3. **Flat line:** Feature has minimal impact\n",
    "4. **Color patterns:** If dots change color along the vertical axis, it indicates **feature interaction**\n",
    "   - Example: If high values of another feature (shown in red) amplify the effect\n",
    "\n",
    "**Example Interpretation for MedInc:**\n",
    "- **Upward trend:** Higher median income → higher SHAP values → higher predicted prices\n",
    "- **Steepness:** How strongly income affects price\n",
    "- **Color variation:** If colored by location, you might see that income matters more in certain geographic areas\n",
    "\n",
    "**Key Insight:** This plot reveals **\"HOW does this feature affect predictions?\"** beyond just knowing it's important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 SHAP for Classification (Breast Cancer)\n",
    "\n",
    "For classification problems, SHAP values explain the log-odds (or probability) of each class. Let's apply SHAP to our breast cancer classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer for the Random Forest Classifier\n",
    "print(\"Creating SHAP explainer for classification model...\")\n",
    "explainer_clf = shap.TreeExplainer(rf_classifier)\n",
    "\n",
    "# Compute SHAP values for a subset of test data\n",
    "X_cancer_test_sample = X_cancer_test.iloc[:50]\n",
    "shap_values_clf = explainer_clf.shap_values(X_cancer_test_sample)\n",
    "\n",
    "print(f\"✓ SHAP values computed for {X_cancer_test_sample.shape[0]} instances\")\n",
    "print(f\"  Shape: {shap_values_clf.shape}\")\n",
    "print(f\"  Expected values: {explainer_clf.expected_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Global Feature Importance for Malignant Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot for class 1 (malignant)\n",
    "# Note: shap_values_clf is a 3D array (n_samples, n_features, n_classes)\n",
    "shap.summary_plot(shap_values_clf[:, :, 1], X_cancer_test_sample, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📊 What to Expect:**\n",
    "\n",
    "This **bar chart** shows the **global feature importance** for predicting the **malignant class** (cancer) across all test samples.\n",
    "\n",
    "**How to Interpret:**\n",
    "- **Vertical axis:** Features ranked by importance (most important at the top)\n",
    "- **Horizontal axis:** Mean absolute SHAP value for the malignant class\n",
    "- **Longer bars** = features that more strongly influence whether the model predicts malignant\n",
    "- **Focus:** This specifically explains what drives \"malignant\" predictions (not benign)\n",
    "\n",
    "**Key Insights to Look For:**\n",
    "- **Top features:** These are the measurements/characteristics the model relies on most when classifying tumors as malignant\n",
    "- **Medical relevance:** Compare the top features with known medical indicators of malignancy\n",
    "- **Domain validation:** Do the important features align with clinical knowledge?\n",
    "\n",
    "**Example:** If \"worst radius\" or \"worst concave points\" are at the top, it suggests the model learned that larger, more irregularly-shaped tumors are more likely to be malignant—which aligns with medical understanding.\n",
    "\n",
    "**Note:** For binary classification, SHAP computes values for each class. Here we focus on class 1 (malignant) since that's typically the class of greater clinical interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Feature Impact Distribution for Malignant Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm plot for class 1 (malignant)\n",
    "shap.summary_plot(shap_values_clf[:, :, 1], X_cancer_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📊 What to Expect:**\n",
    "\n",
    "This **beeswarm plot** provides a detailed view of how each feature contributes to **malignant predictions** across all samples, showing both **magnitude and direction** of effects.\n",
    "\n",
    "**How to Interpret:**\n",
    "- **Vertical axis:** Features ranked by overall importance (same as previous bar chart)\n",
    "- **Horizontal axis:** SHAP value for malignant class\n",
    "  - **Positive (right)** = pushes prediction toward malignant\n",
    "  - **Negative (left)** = pushes prediction toward benign\n",
    "- **Each dot:** Represents one patient/sample\n",
    "- **Color coding:**\n",
    "  - 🔴 **Red/Pink** = high feature value\n",
    "  - 🔵 **Blue** = low feature value\n",
    "\n",
    "**Key Patterns to Look For:**\n",
    "\n",
    "1. **Clear color separation:** If red dots are mostly on one side and blue on the other, there's a strong monotonic relationship\n",
    "   - Example: Red dots on the right = \"high values predict malignant\"\n",
    "   \n",
    "2. **Mixed colors:** Suggests complex or non-linear relationships\n",
    "\n",
    "3. **Spread:** Wide horizontal spread = feature has highly variable impact across patients\n",
    "\n",
    "**Clinical Interpretation Example:**\n",
    "If you see the feature \"worst concave points\":\n",
    "- **Red dots on the RIGHT** → High concave points strongly indicate malignancy\n",
    "- **Blue dots on the LEFT** → Low concave points strongly indicate benign\n",
    "This would confirm the medical understanding that irregular, concave features suggest cancer.\n",
    "\n",
    "**Key Insight:** This visualization helps you understand **\"What feature values are associated with cancer predictions?\"**—critical for validating that the model learned clinically meaningful patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Individual Prediction Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single prediction\n",
    "instance_idx = 0\n",
    "print(f\"Explaining instance {instance_idx}:\")\n",
    "print(f\"  Actual class: {cancer.target_names[y_cancer_test[instance_idx]]}\")\n",
    "print(f\"  Predicted class: {cancer.target_names[rf_classifier.predict(X_cancer_test_sample.iloc[[instance_idx]])[0]]}\")\n",
    "print(f\"  Predicted probabilities: {rf_classifier.predict_proba(X_cancer_test_sample.iloc[[instance_idx]])[0]}\")\n",
    "print()\n",
    "\n",
    "shap.plots.waterfall(shap.Explanation(\n",
    "    values=shap_values_clf[instance_idx, :, 1],\n",
    "    base_values=explainer_clf.expected_value[1],\n",
    "    data=X_cancer_test_sample.iloc[instance_idx],\n",
    "    feature_names=X_cancer_test_sample.columns.tolist()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📊 What to Expect:**\n",
    "\n",
    "This **waterfall plot** will explain a **single patient's cancer diagnosis prediction** by showing exactly which tumor characteristics (features) pushed the model toward or away from a malignant classification.\n",
    "\n",
    "**How to Interpret:**\n",
    "- **Bottom value (E[f(X)]):** The baseline log-odds for malignant class—what the model would predict on average without knowing anything about this patient\n",
    "- **Each bar:** Shows how one tumor measurement pushes the prediction toward malignant (red/right) or benign (blue/left)\n",
    "- **Feature values:** Displayed next to each feature (e.g., \"worst radius = 25.4\")\n",
    "- **Top value (f(x)):** The final log-odds prediction for this specific patient\n",
    "- **Cumulative effect:** Follow the bars to see the running total\n",
    "\n",
    "**Reading the Clinical Story:**\n",
    "\n",
    "1. Start at the baseline (average prediction)\n",
    "2. Each bar adds information: \"This measurement increases/decreases cancer likelihood\"\n",
    "3. The final prediction is the cumulative effect of all measurements\n",
    "\n",
    "**What to Look For:**\n",
    "- **Large red bars:** Tumor characteristics strongly suggesting malignancy\n",
    "- **Large blue bars:** Characteristics suggesting benign\n",
    "- **Alignment with diagnosis:** Does the final prediction match the actual diagnosis?\n",
    "- **Clinical sensibility:** Do the important features make medical sense?\n",
    "\n",
    "**Example Interpretation:**\n",
    "If \"worst concave points = 0.15\" has a large red bar, it means: \"This patient's high concave points measurement significantly increased the model's confidence that the tumor is malignant.\"\n",
    "\n",
    "**Key Insight:** This answers the critical clinical question: **\"WHY did the model classify THIS patient as malignant/benign?\"**—essential for doctor trust and patient explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "### 6.1 Introduction to LIME\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) takes a different approach to explainability compared to SHAP. Instead of using game theory, LIME explains predictions by approximating the complex model with a simple, interpretable model **locally** around the prediction of interest.\n",
    "\n",
    "### How LIME Works\n",
    "\n",
    "The LIME algorithm follows these steps:\n",
    "\n",
    "1. **Select an instance** to explain\n",
    "2. **Generate perturbed samples** around that instance\n",
    "3. **Get predictions** from the black-box model for these perturbed samples\n",
    "4. **Weight the samples** by their proximity to the original instance\n",
    "5. **Train a simple model** (e.g., linear regression) on the weighted samples\n",
    "6. **Extract feature weights** from the simple model as the explanation\n",
    "\n",
    "### Key Characteristics of LIME\n",
    "\n",
    "**Local Fidelity:** LIME prioritizes being accurate locally (around the instance being explained) rather than globally. The simple model may not represent the black-box model well everywhere, but it should be faithful in the neighborhood of the explained instance.\n",
    "\n",
    "**Model-Agnostic:** LIME treats the model as a black box, requiring only the ability to query it for predictions. This makes it applicable to any model type.\n",
    "\n",
    "**Interpretable Representations:** For tabular data, LIME can discretize continuous features into bins (e.g., \"age > 30\") to make explanations more human-friendly.\n",
    "\n",
    "### LIME vs SHAP\n",
    "\n",
    "While both provide local explanations, they differ in:\n",
    "- **Theoretical foundation:** SHAP uses game theory, LIME uses local approximation\n",
    "- **Consistency:** SHAP guarantees certain mathematical properties, LIME does not\n",
    "- **Speed:** LIME can be faster for individual explanations, especially for non-tree models\n",
    "- **Stability:** SHAP tends to be more stable across multiple runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 LIME for Classification (Breast Cancer)\n",
    "\n",
    "Let's use LIME to explain predictions from our breast cancer classifier. LIME requires training data to understand feature distributions and to generate realistic perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LIME explainer for the Random Forest Classifier\n",
    "print(\"Creating LIME explainer for classification model...\")\n",
    "explainer_lime_clf = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_cancer_train.values,\n",
    "    feature_names=X_cancer_train.columns.tolist(),\n",
    "    class_names=cancer.target_names.tolist(),\n",
    "    mode='classification',\n",
    "    discretize_continuous=True,  # Discretize features for more interpretable explanations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"✓ LIME explainer created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Explaining Individual Predictions\n",
    "\n",
    "Let's explain a specific prediction and compare it with the SHAP explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single instance\n",
    "instance_idx = 0\n",
    "instance = X_cancer_test.iloc[instance_idx].values\n",
    "\n",
    "print(f\"Explaining instance {instance_idx}:\")\n",
    "print(f\"  Actual class: {cancer.target_names[y_cancer_test[instance_idx]]}\")\n",
    "print(f\"  Predicted class: {cancer.target_names[rf_classifier.predict([instance])[0]]}\")\n",
    "print(f\"  Predicted probabilities: {rf_classifier.predict_proba([instance])[0]}\")\n",
    "print()\n",
    "\n",
    "# Generate LIME explanation\n",
    "exp_lime_clf = explainer_lime_clf.explain_instance(\n",
    "    data_row=instance,\n",
    "    predict_fn=rf_classifier.predict_proba,\n",
    "    num_features=10,  # Show top 10 features\n",
    "    top_labels=2  # Explain both classes\n",
    ")\n",
    "\n",
    "# Show the explanation as text (alternative to show_in_notebook)\n",
    "print(\"LIME Explanation for Malignant Class:\")\n",
    "for feature, weight in exp_lime_clf.as_list(label=1):\n",
    "    print(f\"  {feature}: {weight:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"LIME Explanation for Benign Class:\")\n",
    "for feature, weight in exp_lime_clf.as_list(label=0):\n",
    "    print(f\"  {feature}: {weight:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Visualizing LIME Explanation as a Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative visualization using matplotlib\n",
    "fig = exp_lime_clf.as_pyplot_figure(label=1)  # label=1 for malignant class\n",
    "plt.title(f\"LIME Explanation for Instance {instance_idx} (Malignant Class)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📊 What to Expect:**\n",
    "\n",
    "This **horizontal bar chart** will show the **LIME explanation** for a single patient's malignant classification, displaying which features the local linear model identified as most important.\n",
    "\n",
    "**How to Interpret:**\n",
    "- **Vertical axis:** The top features identified by LIME (usually 5-10 features)\n",
    "- **Horizontal axis:** The weight/contribution of each feature\n",
    "  - **Positive (orange/right)** = contributes to malignant prediction\n",
    "  - **Negative (blue/left)** = contributes to benign prediction\n",
    "- **Feature format:** LIME often shows discretized conditions (e.g., \"worst radius > 16.5\")\n",
    "- **Bar length:** Indicates strength of contribution\n",
    "\n",
    "**Key Differences from SHAP:**\n",
    "- **LIME approximates locally:** It builds a simple linear model around this one instance\n",
    "- **Discretized features:** Makes interpretations more intuitive (\"if radius > 16.5, then...\")\n",
    "- **May differ from SHAP:** LIME focuses on local fidelity, not global consistency\n",
    "\n",
    "**Reading the Explanation:**\n",
    "\n",
    "Each bar tells you: \"This feature condition contributes X amount to the malignant prediction\"\n",
    "\n",
    "**Example:**\n",
    "- \"worst concave points > 0.10\" with a large positive (orange) bar means:\n",
    "  - This patient has high concave points\n",
    "  - According to the local linear approximation, this pushes strongly toward malignant\n",
    "  \n",
    "**Key Insight:** LIME provides an **intuitive, rule-like explanation**: \"The model predicted malignant because the tumor has [these characteristics].\"\n",
    "\n",
    "**For Medical Context:** These explanations can be easier to communicate to patients: \"Your tumor has high concave points and large radius, which are indicators the model associates with malignancy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 LIME for Regression (California Housing)\n",
    "\n",
    "LIME can also be used for regression problems. Let's explain a house price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LIME explainer for the Random Forest Regressor\n",
    "print(\"Creating LIME explainer for regression model...\")\n",
    "explainer_lime_reg = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_housing_train.values,\n",
    "    feature_names=X_housing_train.columns.tolist(),\n",
    "    mode='regression',\n",
    "    discretize_continuous=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"✓ LIME explainer created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single instance\n",
    "instance_idx = 0\n",
    "instance = X_housing_test.iloc[instance_idx].values\n",
    "\n",
    "print(f\"Explaining instance {instance_idx}:\")\n",
    "print(f\"  Actual value: {y_housing_test[instance_idx]:.4f}\")\n",
    "print(f\"  Predicted value: {rf_regressor.predict([instance])[0]:.4f}\")\n",
    "print()\n",
    "\n",
    "# Generate LIME explanation\n",
    "exp_lime_reg = explainer_lime_reg.explain_instance(\n",
    "    data_row=instance,\n",
    "    predict_fn=rf_regressor.predict,\n",
    "    num_features=8  # Show all 8 features\n",
    ")\n",
    "\n",
    "# Show the explanation as text\n",
    "print(\"LIME Explanation for House Price Prediction:\")\n",
    "for feature, weight in exp_lime_reg.as_list():\n",
    "    print(f\"  {feature}: {weight:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAP vs. LIME: A Comprehensive Comparison\n",
    "\n",
    "Now that we've explored both SHAP and LIME, let's compare them systematically:\n",
    "\n",
    "| Aspect | SHAP | LIME |\n",
    "|--------|------|------|\n",
    "| **Theoretical Foundation** | Game Theory (Shapley Values) | Local Linear Approximation |\n",
    "| **Consistency Guarantee** | Yes (satisfies desirable axioms) | No (can be inconsistent) |\n",
    "| **Global Interpretability** | Yes (can aggregate local explanations) | Limited (primarily local) |\n",
    "| **Local Interpretability** | Yes | Yes |\n",
    "| **Model-Agnostic** | Yes | Yes |\n",
    "| **Computational Speed** | Can be slow for complex models | Generally faster for individual predictions |\n",
    "| **Stability** | More stable across runs | Can vary between runs |\n",
    "| **Output Format** | SHAP values (additive contributions) | Feature weights in local linear model |\n",
    "| **Feature Interactions** | Captures interactions implicitly | Limited interaction modeling |\n",
    "| **Best Use Cases** | When consistency and theoretical guarantees matter | When speed and simplicity are priorities |\n",
    "\n",
    "### When to Use SHAP\n",
    "\n",
    "Choose SHAP when you need:\n",
    "- **Theoretical guarantees** and consistency\n",
    "- **Global feature importance** in addition to local explanations\n",
    "- **Reliable, stable explanations** across multiple runs\n",
    "- **Tree-based models** (TreeExplainer is very efficient)\n",
    "- **Regulatory compliance** where mathematical rigor is important\n",
    "\n",
    "### When to Use LIME\n",
    "\n",
    "Choose LIME when you need:\n",
    "- **Fast explanations** for individual predictions\n",
    "- **Simple, intuitive explanations** for non-technical stakeholders\n",
    "- **Flexibility** in defining custom distance metrics or perturbation strategies\n",
    "- **Explanations for any model type** where SHAP might be slow\n",
    "- **Discretized feature representations** for easier interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Side-by-Side Comparison on the Same Instance\n",
    "\n",
    "Let's compare SHAP and LIME explanations for the same prediction to see how they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SHAP and LIME for the same instance\n",
    "instance_idx = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Comparing SHAP and LIME for Cancer Classification Instance {instance_idx}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Actual class: {cancer.target_names[y_cancer_test[instance_idx]]}\")\n",
    "print(f\"Predicted class: {cancer.target_names[rf_classifier.predict(X_cancer_test.iloc[[instance_idx]])[0]]}\")\n",
    "print(f\"Predicted probabilities: {rf_classifier.predict_proba(X_cancer_test.iloc[[instance_idx]])[0]}\")\n",
    "print()\n",
    "\n",
    "# SHAP explanation\n",
    "print(\"SHAP Top 5 Features (for malignant class):\")\n",
    "shap_values_instance = shap_values_clf[instance_idx, :, 1]\n",
    "feature_names = X_cancer_test_sample.columns.tolist()\n",
    "shap_importance = sorted(zip(feature_names, shap_values_instance), key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "for feat, val in shap_importance:\n",
    "    print(f\"  {feat:30s}: {val:+.4f}\")\n",
    "print()\n",
    "\n",
    "# LIME explanation\n",
    "print(\"LIME Top 5 Features (for malignant class):\")\n",
    "lime_exp = exp_lime_clf.as_list(label=1)[:5]\n",
    "for feat, val in lime_exp:\n",
    "    print(f\"  {feat:30s}: {val:+.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Both methods identify similar important features\")\n",
    "print(\"- SHAP values are additive and sum to the difference from baseline\")\n",
    "print(\"- LIME provides discretized feature conditions for easier interpretation\")\n",
    "print(\"- The magnitude of values differs due to different scales/interpretations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Regulatory and Ethical Considerations\n",
    "\n",
    "Model explainability is not merely a technical challenge; it is a fundamental requirement for responsible AI deployment. As machine learning systems increasingly influence critical decisions affecting human lives, the ability to explain these decisions becomes both an ethical imperative and a legal necessity.\n",
    "\n",
    "### Regulatory Landscape\n",
    "\n",
    "**General Data Protection Regulation (GDPR):** The European Union's GDPR includes provisions for a \"right to explanation\" in Article 22, which addresses automated decision-making. While the exact interpretation is debated, organizations must be able to provide meaningful information about the logic involved in automated decisions that significantly affect individuals.\n",
    "\n",
    "**Fair Lending Regulations:** In the United States, financial institutions must comply with fair lending laws such as the Equal Credit Opportunity Act (ECOA) and the Fair Credit Reporting Act (FCRA). These regulations require lenders to provide adverse action notices explaining why credit was denied, making model explainability essential for compliance.\n",
    "\n",
    "**Healthcare and FDA Requirements:** Medical devices and clinical decision support systems that use AI must demonstrate not only efficacy but also interpretability. The FDA increasingly requires explainability for AI-based medical devices to ensure patient safety and enable clinical validation.\n",
    "\n",
    "**Algorithmic Accountability:** Various jurisdictions are considering or have enacted algorithmic accountability laws that require organizations to assess and explain automated decision systems, particularly in high-stakes domains like employment, housing, and criminal justice.\n",
    "\n",
    "### Ethical Imperatives\n",
    "\n",
    "Beyond legal compliance, explainability serves several ethical purposes:\n",
    "\n",
    "**Fairness and Non-Discrimination:** Explainability tools like SHAP and LIME enable practitioners to identify when models rely on protected attributes (directly or through proxies) or exhibit disparate impact across demographic groups. By examining feature contributions, we can detect and mitigate bias before deployment.\n",
    "\n",
    "**Transparency and Trust:** Stakeholders—whether patients, loan applicants, or criminal defendants—deserve to understand how decisions that affect them are made. Transparency builds trust and enables informed consent.\n",
    "\n",
    "**Accountability:** When models make errors or cause harm, explainability enables us to understand what went wrong and assign responsibility appropriately. Without explainability, accountability becomes impossible.\n",
    "\n",
    "**Human Agency:** Explainable AI preserves human agency by enabling people to contest, appeal, or understand automated decisions. This is particularly important in domains where automated systems augment rather than replace human judgment.\n",
    "\n",
    "### Best Practices for Model Documentation\n",
    "\n",
    "Organizations deploying machine learning systems should:\n",
    "\n",
    "1. **Document model development:** Maintain records of data sources, feature engineering, model selection, and validation procedures\n",
    "2. **Conduct fairness audits:** Regularly assess models for disparate impact across protected groups\n",
    "3. **Implement explanation systems:** Integrate tools like SHAP and LIME into production systems to provide explanations on demand\n",
    "4. **Establish governance processes:** Create review boards and approval processes for high-stakes AI applications\n",
    "5. **Train stakeholders:** Ensure that both technical teams and end-users understand how to interpret explanations\n",
    "6. **Monitor in production:** Continuously monitor model behavior and explanations in production to detect drift or emerging issues\n",
    "\n",
    "### Limitations and Challenges\n",
    "\n",
    "While explainability techniques like SHAP and LIME are powerful, they have limitations:\n",
    "\n",
    "- **Explanation fidelity:** Local explanations may not fully capture complex model behavior\n",
    "- **Cognitive load:** Detailed explanations can overwhelm non-expert users\n",
    "- **Gaming:** Explanations could potentially be manipulated or gamed\n",
    "- **Trade-offs:** There may be tensions between accuracy, explainability, and other objectives\n",
    "\n",
    "Practitioners must balance these considerations thoughtfully, recognizing that explainability is necessary but not sufficient for responsible AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hands-on Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete the following exercises to deepen your understanding of SHAP and LIME.\n",
    "\n",
    "### Exercise 1: Explore Different Instances with SHAP\n",
    "\n",
    "Choose three different instances from the breast cancer test set:\n",
    "- One correctly classified as benign\n",
    "- One correctly classified as malignant\n",
    "- One misclassified instance\n",
    "\n",
    "For each instance:\n",
    "1. Generate a SHAP waterfall plot\n",
    "2. Identify the top 3 features contributing to the prediction\n",
    "3. Explain in your own words why the model made that prediction\n",
    "\n",
    "**Hint:** Use `rf_classifier.predict()` and compare with `y_cancer_test` to find misclassified instances.\n",
    "\n",
    "### Exercise 2: Compare SHAP and LIME Explanations\n",
    "\n",
    "For the same instance from Exercise 1 (the misclassified one):\n",
    "1. Generate both SHAP and LIME explanations\n",
    "2. Compare the top 5 features identified by each method\n",
    "3. Discuss: Do they agree? If not, why might they differ?\n",
    "4. Which explanation do you find more useful, and why?\n",
    "\n",
    "### Exercise 3: Feature Interactions with SHAP\n",
    "\n",
    "Using the California Housing dataset:\n",
    "1. Create a SHAP dependence plot for the \"Latitude\" feature\n",
    "2. Observe if there are any interaction effects (indicated by color patterns)\n",
    "3. Create a dependence plot for \"Longitude\" as well\n",
    "4. Discuss: How do location features (Latitude and Longitude) interact to affect house prices?\n",
    "\n",
    "### Exercise 4: Train a Different Model and Explain It\n",
    "\n",
    "Train a Gradient Boosting Classifier on the breast cancer dataset:\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier.fit(X_cancer_train, y_cancer_train)\n",
    "```\n",
    "\n",
    "Then:\n",
    "1. Use SHAP to explain predictions from this new model\n",
    "2. Compare the feature importance rankings with the Random Forest model\n",
    "3. Discuss: Are the explanations consistent? What does this tell you about the two models?\n",
    "\n",
    "### Exercise 5: Bias Detection\n",
    "\n",
    "Imagine the California Housing dataset includes a sensitive attribute (e.g., a proxy for race or ethnicity). \n",
    "1. Use SHAP to examine if the model relies heavily on location features (Latitude/Longitude)\n",
    "2. Discuss: Could this lead to discriminatory outcomes? How?\n",
    "3. Propose: What steps could you take to mitigate this issue?\n",
    "\n",
    "**Note:** This is a thought exercise. The actual dataset doesn't include demographic information, but location can serve as a proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercises goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "In this lab, we have explored two powerful techniques for model explainability: SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations). Through hands-on examples with real-world datasets, we have learned how to use these tools to understand the predictions of complex, black-box machine learning models.\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "**Model Interpretability:** We examined why explainability matters for trust, fairness, robustness, and regulatory compliance. The distinction between global and local interpretability, as well as model-specific versus model-agnostic approaches, provides a framework for thinking about explanation methods.\n",
    "\n",
    "**SHAP:** Grounded in game theory, SHAP provides consistent, theoretically sound explanations with desirable mathematical properties. SHAP values are additive, meaning they sum to the difference between the prediction and the baseline. SHAP excels at both local explanations (waterfall plots, force plots) and global understanding (summary plots, dependence plots).\n",
    "\n",
    "**LIME:** Based on local linear approximation, LIME explains predictions by fitting simple models around specific instances. While LIME lacks the theoretical guarantees of SHAP, it offers speed and flexibility, making it practical for quick explanations and non-technical audiences.\n",
    "\n",
    "**Regulatory and Ethical Considerations:** Explainability is not just a technical tool but a requirement for responsible AI. Regulations like GDPR, fair lending laws, and healthcare requirements mandate transparency. Beyond compliance, explainability serves ethical imperatives around fairness, accountability, and human agency.\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "When deploying machine learning systems in practice:\n",
    "\n",
    "- **Use SHAP for tree-based models** where TreeExplainer provides fast, exact computations\n",
    "- **Use LIME for quick, intuitive explanations** when interacting with non-technical stakeholders\n",
    "- **Combine multiple explanation methods** to gain different perspectives on model behavior\n",
    "- **Integrate explainability into your workflow** from development through production monitoring\n",
    "- **Document your models thoroughly** including data sources, features, and explanation methodologies\n",
    "- **Conduct regular fairness audits** using explainability tools to detect and mitigate bias\n",
    "\n",
    "### Further Learning\n",
    "\n",
    "To deepen your understanding of model explainability:\n",
    "\n",
    "- **Read the original papers:** Lundberg & Lee (2017) for SHAP, Ribeiro et al. (2016) for LIME\n",
    "- **Explore the SHAP documentation:** https://shap.readthedocs.io/\n",
    "- **Study Christoph Molnar's book:** \"Interpretable Machine Learning\" (available free online)\n",
    "- **Experiment with other techniques:** Integrated Gradients, Anchors, Counterfactual Explanations\n",
    "- **Stay current:** The field of explainable AI is rapidly evolving with new methods and best practices\n",
    "\n",
    "By mastering explainability techniques like SHAP and LIME, you are equipped to build more transparent, trustworthy, and accountable AI systems that benefit society while respecting individual rights and dignity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
