{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods Lab\n",
    "\n",
    "This hands-on lab demonstrates key ensemble learning concepts using real-world data. We'll explore:\n",
    "\n",
    "1. **Using Multiple Models Together** - Combining different algorithms\n",
    "2. **Random Forests and GBTs** - Tree-based ensemble methods\n",
    "3. **Understanding Bootstrap Aggregation** - The bagging technique\n",
    "4. **Combining Heterogeneous Models** - Stacking and blending\n",
    "5. **Evaluating Ensembles of Methods** - Comprehensive performance analysis\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We'll use the **Wine Quality Dataset** from UCI Machine Learning Repository. This dataset contains physicochemical properties of Portuguese \"Vinho Verde\" wine samples, along with sensory quality ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "The next code cell installs optional libraries used later in this notebook: **XGBoost** and **LightGBM**. These are advanced implementations of gradient boosting algorithms that are widely used in industry and competitive machine learning.\n",
    "\n",
    "**Why these libraries?**\n",
    "- **scikit-learn**: Great for learning and understanding core concepts, excellent documentation\n",
    "- **XGBoost**: Highly optimized for speed and performance, includes advanced regularization\n",
    "- **LightGBM**: Developed by Microsoft, extremely fast on large datasets, uses a unique leaf-wise tree growth\n",
    "\n",
    "If these packages are already installed in your environment, the installation command will simply skip them. Don't worry if the installation takes a minute - these are substantial libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install xgboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# The imports below provide the datasets, modeling algorithms,\n",
    "# preprocessing utilities, and evaluation metrics we will use.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Individual models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Ensemble methods\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier\n",
    ")\n",
    "# Optional high-performance gradient boosting libraries\n",
    "# These packages may not be available in every environment (e.g., classroom machines).\n",
    "# We import them defensively so the notebook continues to run if they're missing.\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "    print('xgboost not available; XGBoost comparisons will be skipped.')\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    lgb = None\n",
    "    print('lightgbm not available; LightGBM comparisons will be skipped.')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Key Concepts\n",
    "\n",
    "Before proceeding, let's establish some foundational knowledge that will help you understand the ensemble methods we'll explore:\n",
    "\n",
    "### Important Concepts to Remember:\n",
    "\n",
    "**1. Bagging vs Boosting:**\n",
    "- **Bagging** (Bootstrap Aggregating): Trains multiple models independently in parallel, then averages their predictions. Think of it as \"wisdom of the crowd\" where each expert gives an independent opinion.\n",
    "- **Boosting**: Trains models sequentially, where each new model focuses on fixing the mistakes of previous models. It's like learning from your errors iteratively.\n",
    "\n",
    "**2. Feature Standardization:**\n",
    "- **Why it matters**: Some algorithms (like SVM and KNN) are sensitive to the scale of features. If one feature ranges from 0-1 and another from 0-1000, the algorithm might give undue importance to the larger-scale feature.\n",
    "- **Which models need it**: Distance-based models (KNN, SVM) and gradient-based models (Logistic Regression, Neural Networks)\n",
    "- **Which models don't**: Tree-based models (Decision Trees, Random Forests, Gradient Boosting) naturally handle different scales\n",
    "\n",
    "**3. Cross-Validation:**\n",
    "- **Purpose**: Provides a more reliable estimate of model performance than a single train/test split\n",
    "- **How it works**: Divides training data into K parts (folds), trains K times using different folds for validation each time\n",
    "- **Why we use it**: Reduces the risk of getting lucky (or unlucky) with a particular train/test split\n",
    "\n",
    "**4. Out-of-Bag (OOB) Evaluation:**\n",
    "- **Unique to bagging methods**: When creating bootstrap samples, ~37% of data is left out of each sample\n",
    "- **Free validation**: These left-out samples can be used to estimate model performance without needing a separate validation set\n",
    "\n",
    "### Quick Self-Check:\n",
    "Before running the next cells, think about:\n",
    "- Which algorithm might perform best on the Wine dataset and why?\n",
    "- Do you expect linear models or tree-based models to work better for this classification task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "\n",
    "We'll load the Wine dataset from scikit-learn, which is a well-known classification dataset perfect for demonstrating ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset\n",
    "\n",
    "The next cell loads the Wine dataset and displays essential information. As you run it, pay attention to:\n",
    "\n",
    "**1. Class Balance:**\n",
    "- Are the three wine classes equally represented?\n",
    "- Imbalanced classes can bias models toward the majority class\n",
    "- This dataset is reasonably balanced, which makes accuracy a reliable metric\n",
    "\n",
    "**2. Feature Information:**\n",
    "- Number of features (13 chemical properties like alcohol content, acidity, etc.)\n",
    "- All features are continuous numerical values (no categorical variables)\n",
    "- This is ideal for ensemble methods which work well with numerical data\n",
    "\n",
    "**3. Dataset Size:**\n",
    "- With 178 samples, this is a small dataset\n",
    "- Ensemble methods shine even on small datasets by reducing overfitting\n",
    "- Cross-validation becomes especially important with limited data\n",
    "\n",
    "**What to look for in the output:**\n",
    "- Total samples and how they split across classes\n",
    "- Feature names (these are chemical measurements from wine analysis)\n",
    "- First few rows to see the scale and range of values\n",
    "\n",
    "**Try this:** After running the cell, use `df.describe()` and `df.info()` in a new cell to explore the data distributions further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(pd.Series(y).value_counts().sort_index())\n",
    "\n",
    "print(\"\\nFeature names:\")\n",
    "print(wine.feature_names)\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "pd.Series(y).value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Wine Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df.iloc[:, :4].boxplot()\n",
    "plt.title('Feature Distributions (First 4 Features)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "\n",
    "This cell creates two important visualizations:\n",
    "\n",
    "**Left Plot - Class Distribution:**\n",
    "- Shows how many samples we have for each wine class\n",
    "- Helps identify if we have class imbalance (we don't in this case!)\n",
    "- Balanced classes mean we can trust accuracy as our primary metric\n",
    "\n",
    "**Right Plot - Feature Distributions (First 4 Features):**\n",
    "- Box plots show the spread and outliers for each feature\n",
    "- Notice the different scales across features (this is why we'll standardize later)\n",
    "- Outliers appear as individual points beyond the whiskers\n",
    "- The wide range of scales reinforces why distance-based algorithms need standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Split the data into training and test sets, and standardize the features for better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting and Scaling Explained\n",
    "\n",
    "This cell performs two critical preprocessing steps:\n",
    "\n",
    "**1. Train/Test Split (70%/30%):**\n",
    "- **Training set (70%)**: Used to train our models and tune parameters\n",
    "- **Test set (30%)**: Held aside to evaluate final performance (simulates new, unseen data)\n",
    "- **Stratified splitting**: Ensures each split maintains the same class proportions as the original dataset\n",
    "  - Example: If the original data is 40% class 0, 30% class 1, 30% class 2, both train and test sets will have the same proportions\n",
    "  - This is crucial for small datasets to avoid accidentally creating imbalanced splits\n",
    "\n",
    "**2. Feature Standardization (StandardScaler):**\n",
    "- **What it does**: Transforms each feature to have mean=0 and standard deviation=1\n",
    "- **Why it matters**: \n",
    "  - Prevents features with larger scales from dominating the model\n",
    "  - Essential for: SVM, KNN, Logistic Regression, and Neural Networks\n",
    "  - Not needed for: Decision Trees, Random Forests, and Gradient Boosting (they use splits, not distances)\n",
    "- **Important**: We fit the scaler ONLY on training data, then apply it to test data\n",
    "  - This prevents \"data leakage\" - we don't want the test set influencing our preprocessing\n",
    "\n",
    "**Which models in this lab need scaling?**\n",
    "- ✅ Need scaling: SVM, KNN, Logistic Regression\n",
    "- ❌ Don't need scaling: Decision Trees, Random Forests, Gradient Boosting\n",
    "\n",
    "**Mini-experiment idea**: Try training a KNN model with and without scaling to see the difference in performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features (important for some algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Multiple Models Together\n",
    "\n",
    "Before diving into ensemble methods, let's first train several individual models to establish baselines. We'll compare their performance and then see how combining them improves results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing Baseline Performance\n",
    "\n",
    "**Why start with individual models?**\n",
    "Ensemble methods combine multiple models, so we need to understand how well individual models perform first. This gives us:\n",
    "- A **baseline** to compare against - how much improvement do ensembles provide?\n",
    "- Insight into **which models work well** for this dataset\n",
    "- Understanding of **model diversity** - do different models make different mistakes?\n",
    "\n",
    "**What this cell measures:**\n",
    "\n",
    "1. **Test Accuracy**: How well the model performs on unseen data (our test set)\n",
    "   - This is what you'd report as the final model performance\n",
    "\n",
    "2. **Cross-Validation (CV) Score**: More reliable than a single test score\n",
    "   - **CV Mean**: Average performance across 5 different train/validation splits\n",
    "   - **CV Std (Standard Deviation)**: How much performance varies between folds\n",
    "     - Low std = stable, consistent model\n",
    "     - High std = model is sensitive to which data it sees\n",
    "\n",
    "**The 5 Models We're Testing:**\n",
    "- **Decision Tree**: Fast, interpretable, but prone to overfitting\n",
    "- **Logistic Regression**: Linear model, works well when classes are linearly separable\n",
    "- **SVM (Support Vector Machine)**: Finds optimal decision boundary, powerful but slower\n",
    "- **K-Nearest Neighbors**: Classifies based on similarity to nearby training examples\n",
    "- **Naive Bayes**: Fast probabilistic model, assumes feature independence\n",
    "\n",
    "**After running, ask yourself:**\n",
    "- Which model performs best? Why might that be?\n",
    "- Which models show high variance (large CV std)? This suggests instability.\n",
    "- Do you see any models overfitting (train accuracy much higher than test)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple individual models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Store results\n",
    "individual_results = {}\n",
    "\n",
    "print(\"Training individual models...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    individual_results[name] = {\n",
    "        'model': model,\n",
    "        'test_accuracy': accuracy,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  CV Score: {cv_mean:.4f} (+/- {cv_std:.4f})\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual model performance\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(individual_results.keys()),\n",
    "    'Test Accuracy': [r['test_accuracy'] for r in individual_results.values()],\n",
    "    'CV Mean': [r['cv_mean'] for r in individual_results.values()],\n",
    "    'CV Std': [r['cv_std'] for r in individual_results.values()]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(results_df['Model'], results_df['Test Accuracy'])\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Individual Model Performance')\n",
    "plt.xlim(0.8, 1.0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.errorbar(results_df['CV Mean'], results_df['Model'], \n",
    "             xerr=results_df['CV Std'], fmt='o', markersize=8)\n",
    "plt.xlabel('Cross-Validation Score')\n",
    "plt.title('CV Performance with Standard Deviation')\n",
    "plt.xlim(0.8, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Performance\n",
    "\n",
    "This visualization helps you compare models at a glance:\n",
    "\n",
    "**Left Plot - Test Accuracy:**\n",
    "- Simple bar chart showing final test performance\n",
    "- Easy to identify the best and worst performers\n",
    "- But this only shows one metric from one train/test split...\n",
    "\n",
    "**Right Plot - Cross-Validation with Error Bars:**\n",
    "- **Center point**: Mean performance across 5 folds\n",
    "- **Error bars**: Show the standard deviation (spread of scores)\n",
    "- **What good error bars tell you**:\n",
    "  - Short bars = consistent, reliable model\n",
    "  - Long bars = unstable model, performance varies with data\n",
    "  \n",
    "**Key insight**: A model with slightly lower mean but smaller error bars might be more trustworthy than one with higher mean but large variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Voting Ensemble\n",
    "\n",
    "Now let's combine these models using a **Voting Classifier**. This ensemble method combines predictions from multiple models using either:\n",
    "- **Hard voting**: Each model votes for a class, and the majority wins\n",
    "- **Soft voting**: Predictions are weighted by class probabilities (usually performs better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Voting Ensembles Work\n",
    "\n",
    "Now we're combining our individual models! Think of this like a panel of experts voting on the correct answer.\n",
    "\n",
    "**Two Voting Strategies:**\n",
    "\n",
    "**1. Hard Voting (Majority Vote):**\n",
    "- Each model predicts a class (0, 1, or 2)\n",
    "- The class with the most votes wins\n",
    "- Example: If 3 models predict class 1, and 2 models predict class 2, the final prediction is class 1\n",
    "- Simple and interpretable\n",
    "\n",
    "**2. Soft Voting (Probability Weighted):**\n",
    "- Each model outputs probabilities for each class (e.g., [0.2, 0.7, 0.1])\n",
    "- Probabilities are averaged across all models\n",
    "- The class with the highest average probability wins\n",
    "- Generally performs better because it considers confidence levels\n",
    "- **Requirement**: All models must support `predict_proba()` method\n",
    "\n",
    "**Why Voting Ensembles Work:**\n",
    "- **Diversity of mistakes**: Different algorithms make different errors\n",
    "  - Linear models struggle with non-linear boundaries\n",
    "  - KNN can be fooled by noisy data\n",
    "  - Decision trees might overfit to certain patterns\n",
    "- When combined, the errors of one model can be corrected by others\n",
    "- The \"wisdom of crowds\" principle in action!\n",
    "\n",
    "**What you should see**: Voting ensemble performance typically matches or exceeds the best individual model, with more stable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create voting classifiers\n",
    "estimators = [(name, model) for name, model in models.items()]\n",
    "\n",
    "# Hard voting\n",
    "voting_hard = VotingClassifier(estimators=estimators, voting='hard')\n",
    "voting_hard.fit(X_train_scaled, y_train)\n",
    "y_pred_hard = voting_hard.predict(X_test_scaled)\n",
    "acc_hard = accuracy_score(y_test, y_pred_hard)\n",
    "\n",
    "# Soft voting\n",
    "voting_soft = VotingClassifier(estimators=estimators, voting='soft')\n",
    "voting_soft.fit(X_train_scaled, y_train)\n",
    "y_pred_soft = voting_soft.predict(X_test_scaled)\n",
    "acc_soft = accuracy_score(y_test, y_pred_soft)\n",
    "\n",
    "print(\"\\nVoting Ensemble Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hard Voting Accuracy: {acc_hard:.4f}\")\n",
    "print(f\"Soft Voting Accuracy: {acc_soft:.4f}\")\n",
    "print(\"\\nComparison with best individual model:\")\n",
    "best_individual = max(individual_results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "print(f\"Best Individual Model: {best_individual[0]}\")\n",
    "print(f\"Best Individual Accuracy: {best_individual[1]['test_accuracy']:.4f}\")\n",
    "print(f\"\\nImprovement (Soft Voting): {(acc_soft - best_individual[1]['test_accuracy']):.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Forests and Gradient Boosted Trees\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "Random Forests use **Bootstrap Aggregation (Bagging)** combined with random feature selection. Each tree is trained on a different bootstrap sample, and at each split, only a random subset of features is considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Random Forests\n",
    "\n",
    "**What is a Random Forest?**\n",
    "A Random Forest is an ensemble of Decision Trees, but with two key sources of randomness:\n",
    "\n",
    "1. **Bootstrap Sampling**: Each tree is trained on a different random sample of the data (with replacement)\n",
    "2. **Random Feature Selection**: At each split, only a random subset of features is considered\n",
    "   - For classification: typically √(n_features) are considered\n",
    "   - This prevents trees from being too similar to each other\n",
    "\n",
    "**Why Randomness Helps:**\n",
    "- Without randomness, all trees would be identical (they'd see the same data and same features)\n",
    "- Randomness creates **diverse** trees that make different mistakes\n",
    "- When we average predictions, the errors cancel out, but the correct predictions reinforce each other\n",
    "\n",
    "**The Experiment:**\n",
    "This cell trains Random Forests with different numbers of trees (10, 50, 100, 200, 500) to answer:\n",
    "- Does more trees = better performance?\n",
    "- Is there a point of diminishing returns?\n",
    "- Can Random Forests overfit with too many trees?\n",
    "\n",
    "**What to watch for:**\n",
    "- **Training accuracy**: Will be high (maybe too high) because decision trees can memorize data\n",
    "- **Test accuracy**: Should improve initially, then plateau\n",
    "- **The gap between train and test**: Should remain stable (Random Forests resist overfitting!)\n",
    "\n",
    "**Computational trade-off**: More trees = better performance but slower training and prediction. In practice, 100-500 trees is a good range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with different numbers of trees\n",
    "n_trees_list = [10, 50, 100, 200, 500]\n",
    "rf_results = []\n",
    "\n",
    "print(\"Training Random Forests with different number of trees...\\n\")\n",
    "\n",
    "for n_trees in n_trees_list:\n",
    "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = rf.score(X_train, y_train)\n",
    "    test_acc = rf.score(X_test, y_test)\n",
    "    \n",
    "    rf_results.append({\n",
    "        'n_trees': n_trees,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"n_estimators={n_trees:3d} | Train: {train_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "rf_results_df = pd.DataFrame(rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Forest performance vs number of trees\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(rf_results_df['n_trees'], rf_results_df['train_accuracy'], \n",
    "         marker='o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(rf_results_df['n_trees'], rf_results_df['test_accuracy'], \n",
    "         marker='s', label='Test Accuracy', linewidth=2)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest Performance vs Number of Trees')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.85, 1.05)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Performance improves with more trees initially\")\n",
    "print(\"- Returns diminish after a certain point\")\n",
    "print(\"- Random Forests are resistant to overfitting due to averaging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Random Forest Learning Curve\n",
    "\n",
    "**What this plot shows:**\n",
    "- **X-axis**: Number of trees in the forest\n",
    "- **Blue line**: Training accuracy (how well the forest fits the training data)\n",
    "- **Orange line**: Test accuracy (how well it generalizes to new data)\n",
    "\n",
    "**Key observations to make:**\n",
    "\n",
    "1. **Initial improvement**: Performance jumps dramatically from 10 to 50 trees\n",
    "   - More trees = more diverse opinions = better ensemble decisions\n",
    "\n",
    "2. **Diminishing returns**: The curve flattens after ~100-200 trees\n",
    "   - Additional trees help less and less\n",
    "   - The ensemble has already captured most patterns\n",
    "\n",
    "3. **No overfitting!**: Notice that test accuracy doesn't decrease as we add trees\n",
    "   - Unlike single decision trees, Random Forests are resistant to overfitting\n",
    "   - More trees increase computational cost but don't hurt generalization\n",
    "\n",
    "4. **Train vs Test gap**: Training accuracy is near perfect, test is lower\n",
    "   - This gap is expected and acceptable\n",
    "   - The gap doesn't grow as we add trees (good sign!)\n",
    "\n",
    "**Practical takeaway**: For most problems, 100-500 trees offers the best accuracy-speed trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from Random Forest\n",
    "rf_final = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_final.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': wine.feature_names,\n",
    "    'importance': rf_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance: Understanding What Matters\n",
    "\n",
    "One of the most valuable aspects of Random Forests is **feature importance** - which features contribute most to predictions.\n",
    "\n",
    "**How it's calculated:**\n",
    "- For each tree, we track how much each feature reduces impurity (error) when used for splitting\n",
    "- Average this across all trees in the forest\n",
    "- Normalize to sum to 1.0 (or 100%)\n",
    "\n",
    "**Why this matters:**\n",
    "- **Interpretability**: Understand which chemical properties most distinguish wine types\n",
    "- **Feature selection**: Could we build a simpler model using only the top features?\n",
    "- **Domain insights**: Does the model's ranking align with wine expert knowledge?\n",
    "\n",
    "**What to look for in the results:**\n",
    "- A few dominant features vs. evenly distributed importance\n",
    "- Features with near-zero importance (candidates for removal)\n",
    "- Whether top features make intuitive sense for wine classification\n",
    "\n",
    "**Important note**: Feature importance shows correlation with predictions, not causation! High importance means the feature is useful for prediction, not that it causes the wine type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees (GBTs)\n",
    "\n",
    "Unlike Random Forests which build trees independently, Gradient Boosting builds trees **sequentially**. Each tree corrects the errors of the previous trees.\n",
    "\n",
    "We'll compare:\n",
    "- **Scikit-learn GradientBoosting**\n",
    "- **XGBoost** (eXtreme Gradient Boosting)\n",
    "- **LightGBM** (Light Gradient Boosting Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting: Learning from Mistakes\n",
    "\n",
    "**Fundamental Difference from Random Forests:**\n",
    "- **Random Forest (Bagging)**: Build all trees independently in parallel, then average\n",
    "- **Gradient Boosting**: Build trees sequentially, where each new tree corrects the errors of previous trees\n",
    "\n",
    "**How Gradient Boosting Works:**\n",
    "1. Start with a simple model (often just predicting the mean)\n",
    "2. Calculate the errors (residuals) this model makes\n",
    "3. Train a new tree to predict these errors\n",
    "4. Add this new tree's predictions to the ensemble (scaled by learning rate)\n",
    "5. Repeat steps 2-4 for n_estimators iterations\n",
    "\n",
    "**Key Parameters:**\n",
    "- **n_estimators**: Number of boosting iterations (trees to build)\n",
    "  - More trees = more opportunities to reduce error\n",
    "  - But can overfit if too many!\n",
    "  \n",
    "- **learning_rate**: How much each tree contributes (typically 0.01 to 0.3)\n",
    "  - Lower = more conservative, needs more trees but often better results\n",
    "  - Higher = faster learning but may overshoot optimal solution\n",
    "  \n",
    "- **max_depth**: Complexity of each individual tree (typically 3-6)\n",
    "  - Shallow trees prevent overfitting to each iteration's errors\n",
    "\n",
    "**Three Implementations We'll Test:**\n",
    "1. **Scikit-learn GradientBoosting**: Standard implementation, reliable and well-documented\n",
    "2. **XGBoost**: Industry standard, highly optimized, includes regularization\n",
    "3. **LightGBM**: Microsoft's implementation, very fast, uses leaf-wise growth\n",
    "4. **AdaBoost**: Earlier boosting algorithm, simpler but still effective\n",
    "\n",
    "**What to expect**: Boosting often achieves the highest accuracy but requires more careful tuning than Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different gradient boosting implementations\n",
    "print(\"Training Gradient Boosting Models...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scikit-learn Gradient Boosting\n",
    "gb_sklearn = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                        max_depth=3, random_state=42)\n",
    "gb_sklearn.fit(X_train, y_train)\n",
    "gb_sklearn_acc = gb_sklearn.score(X_test, y_test)\n",
    "print(f\"Scikit-learn GradientBoosting: {gb_sklearn_acc:.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                              max_depth=3, random_state=42, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_acc = xgb_model.score(X_test, y_test)\n",
    "print(f\"XGBoost:                       {xgb_acc:.4f}\")\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                               max_depth=3, random_state=42, verbose=-1)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_acc = lgb_model.score(X_test, y_test)\n",
    "print(f\"LightGBM:                      {lgb_acc:.4f}\")\n",
    "\n",
    "# AdaBoost (another boosting variant)\n",
    "ada_model = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "ada_acc = ada_model.score(X_test, y_test)\n",
    "print(f\"AdaBoost:                      {ada_acc:.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Random Forest vs Gradient Boosting\n",
    "comparison_data = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Sklearn GB', 'XGBoost', 'LightGBM', 'AdaBoost'],\n",
    "    'Accuracy': [rf_final.score(X_test, y_test), gb_sklearn_acc, xgb_acc, lgb_acc, ada_acc],\n",
    "    'Type': ['Bagging', 'Boosting', 'Boosting', 'Boosting', 'Boosting']\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['blue' if t == 'Bagging' else 'orange' for t in comparison_data['Type']]\n",
    "plt.barh(comparison_data['Model'], comparison_data['Accuracy'], color=colors)\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Random Forest (Bagging) vs Gradient Boosting Methods')\n",
    "plt.xlim(0.85, 1.0)\n",
    "plt.axvline(x=0.95, color='red', linestyle='--', alpha=0.5, label='95% threshold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging vs Boosting: Visual Comparison\n",
    "\n",
    "This chart directly compares the two major ensemble paradigms:\n",
    "\n",
    "**Blue (Bagging - Random Forest):**\n",
    "- Builds trees independently\n",
    "- Reduces variance by averaging\n",
    "- Parallel training (faster on multi-core systems)\n",
    "- Very resistant to overfitting\n",
    "- Good general-purpose choice\n",
    "\n",
    "**Orange (Boosting - GB/XGBoost/LightGBM/AdaBoost):**\n",
    "- Builds trees sequentially\n",
    "- Reduces bias by focusing on errors\n",
    "- Sequential training (slower to train)\n",
    "- Can overfit if not careful\n",
    "- Often achieves highest accuracy with proper tuning\n",
    "\n",
    "**Questions to consider:**\n",
    "- Which approach performs better on this dataset?\n",
    "- Are the differences significant or marginal?\n",
    "- Would the ranking change with different hyperparameters?\n",
    "- Which would you choose for a production system and why?\n",
    "\n",
    "**The 95% threshold line**: This reference helps visualize which models meet a high performance standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Understanding Bootstrap Aggregation (Bagging)\n",
    "\n",
    "Let's dive deeper into how **Bootstrap Aggregation** works:\n",
    "\n",
    "1. Create multiple bootstrap samples (random sampling with replacement)\n",
    "2. Train a model on each bootstrap sample\n",
    "3. Aggregate predictions (majority vote for classification, average for regression)\n",
    "\n",
    "We'll demonstrate this manually and compare it to scikit-learn's BaggingClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Magic of Bootstrap Sampling\n",
    "\n",
    "Let's understand the foundation of bagging by examining **bootstrap sampling** in detail.\n",
    "\n",
    "**What is Bootstrap Sampling?**\n",
    "- Random sampling **with replacement** from the training data\n",
    "- Each bootstrap sample has the same size as the original dataset\n",
    "- Some samples appear multiple times, others don't appear at all\n",
    "\n",
    "**The Mathematics:**\n",
    "- Probability a specific sample is NOT selected in one draw: (n-1)/n\n",
    "- Probability it's NOT selected in n draws: ((n-1)/n)^n\n",
    "- As n gets large, this approaches 1/e ≈ 0.368 (36.8%)\n",
    "- Therefore, ~63.2% of samples are included (with possible repeats)\n",
    "\n",
    "**The Out-of-Bag (OOB) Samples:**\n",
    "- The ~36.8% of samples NOT selected are called \"out-of-bag\"\n",
    "- These are like a free validation set for each tree!\n",
    "- Each tree can be evaluated on its OOB samples\n",
    "- Average OOB predictions across all trees gives an unbiased performance estimate\n",
    "\n",
    "**Why This Matters:**\n",
    "- Creates diverse training sets → diverse models → better ensemble\n",
    "- OOB samples provide validation without reducing training data\n",
    "- This is unique to bagging; boosting doesn't have this benefit\n",
    "\n",
    "**Watch the percentages** in the output closely - you'll see they consistently hover around 63% unique samples and 37% OOB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate bootstrap sampling\n",
    "n_samples = len(X_train)\n",
    "n_bootstrap = 3\n",
    "\n",
    "print(\"Demonstrating Bootstrap Sampling:\\n\")\n",
    "print(f\"Original training set size: {n_samples}\")\n",
    "print(f\"\\nCreating {n_bootstrap} bootstrap samples...\\n\")\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Create bootstrap sample (sampling with replacement)\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    unique_indices = len(np.unique(indices))\n",
    "    \n",
    "    # Out-of-bag samples (samples not selected)\n",
    "    oob_indices = set(range(n_samples)) - set(indices)\n",
    "    \n",
    "    print(f\"Bootstrap Sample {i+1}:\")\n",
    "    print(f\"  Total samples: {len(indices)}\")\n",
    "    print(f\"  Unique samples: {unique_indices} ({unique_indices/n_samples*100:.1f}%)\")\n",
    "    print(f\"  Out-of-bag samples: {len(oob_indices)} ({len(oob_indices)/n_samples*100:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "print(\"Key Insight: Each bootstrap sample uses ~63.2% unique samples\")\n",
    "print(\"The remaining ~36.8% are out-of-bag (OOB) samples used for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model vs Bagging ensemble\n",
    "print(\"\\nComparing Single Model vs Bagging Ensemble:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Single Decision Tree\n",
    "single_tree = DecisionTreeClassifier(random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "single_tree_acc = single_tree.score(X_test, y_test)\n",
    "print(f\"Single Decision Tree: {single_tree_acc:.4f}\")\n",
    "\n",
    "# Bagging with different numbers of estimators\n",
    "n_estimators_list = [10, 50, 100, 200]\n",
    "bagging_results = []\n",
    "\n",
    "for n_est in n_estimators_list:\n",
    "    bagging = BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(),\n",
    "        n_estimators=n_est,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    bagging.fit(X_train, y_train)\n",
    "    bagging_acc = bagging.score(X_test, y_test)\n",
    "    bagging_results.append({'n_estimators': n_est, 'accuracy': bagging_acc})\n",
    "    print(f\"Bagging ({n_est:3d} trees): {bagging_acc:.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "improvement = bagging_results[-1]['accuracy'] - single_tree_acc\n",
    "print(f\"\\nImprovement from Bagging: {improvement:.4f} ({improvement/single_tree_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging in Action: From One Tree to Many\n",
    "\n",
    "This experiment demonstrates the core value proposition of bagging.\n",
    "\n",
    "**The Setup:**\n",
    "- **Baseline**: A single Decision Tree (known for high variance and overfitting)\n",
    "- **Bagging Ensembles**: Multiple decision trees trained on bootstrap samples (10, 50, 100, 200 trees)\n",
    "\n",
    "**What We're Testing:**\n",
    "- Does combining multiple unstable models create a stable ensemble?\n",
    "- How many trees are needed for good performance?\n",
    "- What's the improvement over a single tree?\n",
    "\n",
    "**Decision Trees: The Perfect Base Learner for Bagging**\n",
    "- **High variance**: Different training data → very different trees\n",
    "- **Low bias**: Can fit complex patterns (even overfit)\n",
    "- **Bagging fixes the variance problem** by averaging predictions!\n",
    "\n",
    "**Expected Results:**\n",
    "- Single tree: Decent performance but unstable\n",
    "- Bagging (10 trees): Noticeable improvement\n",
    "- Bagging (50-100 trees): Substantial improvement, plateau\n",
    "- Bagging (200 trees): Marginal additional gains\n",
    "\n",
    "**The key insight**: Many weak, diverse models can combine to create a strong, stable ensemble!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of ensemble size\n",
    "bagging_df = pd.DataFrame(bagging_results)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bagging_df['n_estimators'], bagging_df['accuracy'], \n",
    "         marker='o', linewidth=2, markersize=8, label='Bagging')\n",
    "plt.axhline(y=single_tree_acc, color='red', linestyle='--', \n",
    "            label='Single Tree', linewidth=2)\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Bagging Performance vs Ensemble Size')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Bagging's Impact\n",
    "\n",
    "**The Story This Plot Tells:**\n",
    "\n",
    "**Red dashed line (baseline)**: Performance of a single decision tree\n",
    "- This is our starting point\n",
    "- Prone to overfitting and high variance\n",
    "\n",
    "**Blue line (Bagging curve)**: Accuracy as we add more trees\n",
    "- Sharp initial rise: Adding the first few trees has huge impact\n",
    "- Gradual plateau: Returns diminish after ~50-100 trees\n",
    "- Stays well above baseline: Clear, consistent improvement\n",
    "\n",
    "**Key Observations:**\n",
    "1. Even 10 trees provide substantial improvement\n",
    "2. The curve never dips back down (more trees never hurt performance)\n",
    "3. The gap between bagging and single tree shows the power of ensembling\n",
    "4. There's a sweet spot around 100 trees (good performance without excessive computation)\n",
    "\n",
    "**Why the plateau?**\n",
    "- After enough trees, you've already captured the diversity in bootstrap samples\n",
    "- Additional trees are learning similar patterns\n",
    "- More trees still help a little (why Random Forests use 100-500), but diminishing returns set in\n",
    "\n",
    "**Practical lesson**: Even simple bagging with modest ensemble sizes (50-100) can dramatically improve model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag (OOB) Evaluation\n",
    "\n",
    "One of the benefits of bagging is **OOB evaluation** - we can estimate model performance without a separate validation set using the samples that weren't selected in each bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate OOB scoring\n",
    "bagging_oob = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging_oob.fit(X_train, y_train)\n",
    "\n",
    "print(\"Out-of-Bag Evaluation:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"OOB Score (internal validation): {bagging_oob.oob_score_:.4f}\")\n",
    "print(f\"Test Score:                      {bagging_oob.score(X_test, y_test):.4f}\")\n",
    "print(\"\\nThe OOB score provides an unbiased estimate without needing a separate\")\n",
    "print(\"validation set, saving data for training!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Scoring: Free Validation!\n",
    "\n",
    "This cell demonstrates one of bagging's most elegant features: **built-in validation without using any extra data**.\n",
    "\n",
    "**How OOB Scoring Works:**\n",
    "1. For each tree in the ensemble, ~37% of training samples were not used (out-of-bag)\n",
    "2. Use each tree to predict on its OOB samples\n",
    "3. For each training sample, average predictions from all trees where it was OOB\n",
    "4. Compare OOB predictions to true labels → OOB score\n",
    "\n",
    "**Why This is Powerful:**\n",
    "- **No data loss**: Don't need to set aside a separate validation set\n",
    "- **Unbiased estimate**: OOB samples are truly unseen by each tree\n",
    "- **Free cross-validation**: Similar to cross-validation but happens automatically\n",
    "- **Particularly valuable for small datasets** where every training sample counts\n",
    "\n",
    "**Comparing OOB vs Test Score:**\n",
    "- OOB score: Estimated from internal validation during training\n",
    "- Test score: True held-out performance\n",
    "- They should be close! If OOB >> Test, you might have a problem with your test set\n",
    "- If OOB ≈ Test, it confirms your model will generalize well\n",
    "\n",
    "**When to use OOB scoring:**\n",
    "- Quick model evaluation during development\n",
    "- Hyperparameter tuning without extra validation split\n",
    "- Confidence in generalization before final testing\n",
    "\n",
    "**Important**: OOB is only available with bagging methods (BaggingClassifier, RandomForest), not with boosting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combining Heterogeneous Models (Stacking)\n",
    "\n",
    "**Stacking** is an advanced ensemble technique that combines different types of models:\n",
    "\n",
    "1. Train multiple diverse base models (level 0)\n",
    "2. Use their predictions as features for a meta-model (level 1)\n",
    "3. The meta-model learns how to best combine the base model predictions\n",
    "\n",
    "This is different from voting, which uses a fixed combination rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking: The Ultimate Ensemble\n",
    "\n",
    "**What Makes Stacking Different?**\n",
    "- **Voting**: Fixed combination rule (majority vote or average probabilities)\n",
    "- **Stacking**: Learns the optimal way to combine model predictions using a meta-model\n",
    "\n",
    "**How Stacking Works:**\n",
    "\n",
    "**Level 0 (Base Models):**\n",
    "1. Train diverse base models on the training data\n",
    "2. Each uses a different algorithm with different strengths\n",
    "   - Random Forest: Handles non-linearity and interactions\n",
    "   - Gradient Boosting: Reduces bias, high accuracy\n",
    "   - SVM: Finds optimal decision boundaries\n",
    "   - KNN: Captures local patterns\n",
    "\n",
    "**Level 1 (Meta-Model):**\n",
    "1. Use base model predictions as features\n",
    "2. Train a meta-model (here, Logistic Regression) to learn optimal combination\n",
    "3. The meta-model discovers which base models to trust for which types of predictions\n",
    "\n",
    "**The Cross-Validation Trick (cv=5):**\n",
    "- To avoid overfitting, we don't use simple training predictions as meta-features\n",
    "- Instead, we use 5-fold cross-validation:\n",
    "  - Split training data into 5 parts\n",
    "  - For each part, train base models on other 4 parts, predict on this part\n",
    "  - This ensures meta-features are from \"unseen\" predictions\n",
    "  - Prevents the meta-model from just memorizing training data\n",
    "\n",
    "**Why Choose Stacking?**\n",
    "- ✅ Often achieves best performance by combining strengths of diverse models\n",
    "- ✅ Meta-model learns complex combination rules (not just averaging)\n",
    "- ✅ Can weight models differently for different scenarios\n",
    "- ❌ More complex to implement and tune\n",
    "- ❌ Longer training time (train base models + meta-model)\n",
    "- ❌ Less interpretable than simpler ensembles\n",
    "\n",
    "**When to use Stacking:**\n",
    "- When you need maximum accuracy and have compute resources\n",
    "- When you have diverse base models that capture different aspects of data\n",
    "- In competitions or high-stakes applications\n",
    "\n",
    "**Expected Result:** Stacking should match or exceed any single base model and often beats simple voting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models (diverse set of algorithms)\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(probability=True, random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "]\n",
    "\n",
    "# Define meta-model (final estimator)\n",
    "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # Use cross-validation to generate meta-features\n",
    ")\n",
    "\n",
    "print(\"Training Stacking Ensemble...\\n\")\n",
    "stacking.fit(X_train_scaled, y_train)\n",
    "stacking_acc = stacking.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Stacking Results:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nBase Models Performance:\")\n",
    "for name, model in base_models:\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    acc = model.score(X_test_scaled, y_test)\n",
    "    print(f\"  {name:3s}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nStacking Ensemble: {stacking_acc:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different ensemble strategies\n",
    "ensemble_comparison = pd.DataFrame({\n",
    "    'Method': ['Voting (Hard)', 'Voting (Soft)', 'Bagging', 'Random Forest', \n",
    "               'Gradient Boosting', 'Stacking'],\n",
    "    'Accuracy': [\n",
    "        acc_hard,\n",
    "        acc_soft,\n",
    "        bagging_oob.score(X_test, y_test),\n",
    "        rf_final.score(X_test, y_test),\n",
    "        xgb_acc,\n",
    "        stacking_acc\n",
    "    ],\n",
    "    'Strategy': ['Voting', 'Voting', 'Bagging', 'Bagging', 'Boosting', 'Stacking']\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = {'Voting': 'skyblue', 'Bagging': 'lightgreen', \n",
    "          'Boosting': 'orange', 'Stacking': 'purple'}\n",
    "bar_colors = [colors[s] for s in ensemble_comparison['Strategy']]\n",
    "\n",
    "plt.barh(ensemble_comparison['Method'], ensemble_comparison['Accuracy'], color=bar_colors)\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Comparison of Different Ensemble Strategies')\n",
    "plt.xlim(0.85, 1.0)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=strategy) \n",
    "                   for strategy, color in colors.items()]\n",
    "plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEnsemble Method Comparison:\")\n",
    "print(ensemble_comparison.sort_values('Accuracy', ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing All Ensemble Strategies\n",
    "\n",
    "This comprehensive comparison brings together everything we've learned!\n",
    "\n",
    "**The Four Ensemble Strategies (Color-Coded):**\n",
    "\n",
    "**Sky Blue - Voting Ensembles:**\n",
    "- Combines heterogeneous models (different algorithms)\n",
    "- Simple, interpretable, easy to implement\n",
    "- Soft voting usually beats hard voting\n",
    "\n",
    "**Light Green - Bagging Ensembles:**\n",
    "- Combines homogeneous models (same algorithm, different data)\n",
    "- Random Forest adds feature randomness to bagging\n",
    "- Great for reducing variance of high-variance models\n",
    "\n",
    "**Orange - Boosting Ensembles:**\n",
    "- Sequential training, each model corrects previous errors\n",
    "- Often achieves highest accuracy\n",
    "- Requires more careful tuning\n",
    "\n",
    "**Purple - Stacking:**\n",
    "- Meta-learning approach\n",
    "- Learns optimal combination weights\n",
    "- Most sophisticated but most complex\n",
    "\n",
    "**What to Look For:**\n",
    "- Which strategy achieved highest accuracy?\n",
    "- Is the difference significant or marginal?\n",
    "- Is the complexity/performance trade-off worth it?\n",
    "- Which would you deploy in production considering interpretability, speed, and accuracy?\n",
    "\n",
    "**Key Insight**: On this dataset, you'll likely see that all ensemble methods substantially outperform individual models (from Section 1), validating the ensemble approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluating Ensembles of Methods\n",
    "\n",
    "Let's perform a comprehensive evaluation of our best ensemble models using multiple metrics:\n",
    "- Accuracy\n",
    "- Precision, Recall, F1-score\n",
    "- Confusion Matrix\n",
    "- Cross-validation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Performance Analysis\n",
    "\n",
    "Now let's go beyond simple accuracy and examine our best models using multiple evaluation metrics.\n",
    "\n",
    "**Why Accuracy Alone Isn't Enough:**\n",
    "- **Class imbalance**: If 90% of samples are class A, predicting \"always A\" gives 90% accuracy but is useless\n",
    "- **Different costs**: Misclassifying wine type might have different consequences for each class\n",
    "- **Per-class performance**: A model might excel at some classes but fail at others\n",
    "\n",
    "**The Classification Report Includes:**\n",
    "\n",
    "**Precision**: Of all samples predicted as class X, what fraction actually were class X?\n",
    "- Precision = True Positives / (True Positives + False Positives)\n",
    "- High precision = Few false alarms\n",
    "- Important when false positives are costly\n",
    "\n",
    "**Recall (Sensitivity)**: Of all actual class X samples, what fraction did we correctly identify?\n",
    "- Recall = True Positives / (True Positives + False Negatives)\n",
    "- High recall = Few missed cases\n",
    "- Important when false negatives are costly\n",
    "\n",
    "**F1-Score**: Harmonic mean of precision and recall\n",
    "- F1 = 2 × (Precision × Recall) / (Precision + Recall)\n",
    "- Balances precision and recall\n",
    "- Useful single metric when you care about both\n",
    "\n",
    "**Support**: Number of samples in each class\n",
    "- Helps you weight the importance of each class's metrics\n",
    "- Small support → less reliable metrics for that class\n",
    "\n",
    "**What to Look For:**\n",
    "- Are precision and recall balanced, or does the model favor one?\n",
    "- Which classes are easier/harder to predict?\n",
    "- Do all models struggle with the same classes?\n",
    "- How do ensemble methods compare on per-class metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best models for detailed evaluation\n",
    "best_models = {\n",
    "    'Random Forest': rf_final,\n",
    "    'XGBoost': xgb_model,\n",
    "    'Stacking': stacking,\n",
    "    'Voting (Soft)': voting_soft\n",
    "}\n",
    "\n",
    "# Generate predictions for each model\n",
    "print(\"Detailed Classification Reports:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Use scaled or unscaled data based on model type\n",
    "    if name in ['Stacking', 'Voting (Soft)']:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=wine.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(best_models.items()):\n",
    "    # Use scaled or unscaled data based on model type\n",
    "    if name in ['Stacking', 'Voting (Soft)']:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=wine.target_names,\n",
    "                yticklabels=wine.target_names)\n",
    "    axes[idx].set_title(f'{name} Confusion Matrix')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Confusion Matrices\n",
    "\n",
    "Confusion matrices show the complete picture of model predictions vs. reality.\n",
    "\n",
    "**How to Read a Confusion Matrix:**\n",
    "- **Rows**: True labels (actual wine class)\n",
    "- **Columns**: Predicted labels (what the model said)\n",
    "- **Diagonal (top-left to bottom-right)**: Correct predictions\n",
    "- **Off-diagonal**: Mistakes\n",
    "\n",
    "**Example Interpretation:**\n",
    "If cell [row=1, col=2] = 3, it means:\n",
    "- 3 samples that were actually class 1\n",
    "- Were incorrectly predicted as class 2\n",
    "\n",
    "**What Makes a Good Confusion Matrix:**\n",
    "- **Dark diagonal**: Most predictions on the diagonal (correct)\n",
    "- **Light off-diagonal**: Few mistakes (off-diagonal cells)\n",
    "- **Symmetric mistakes**: If the model confuses class A with B, does it also confuse B with A?\n",
    "\n",
    "**Patterns to Look For:**\n",
    "- Which classes are most confused with each other?\n",
    "- Are errors symmetric or directional?\n",
    "- Does the ensemble reduce specific types of errors?\n",
    "- Compare matrices across models - do they make different mistakes?\n",
    "\n",
    "**Wine-Specific Insight**: If certain wine types are chemically similar, you'd expect more confusion between them. The confusion matrix reveals these relationships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation comparison\n",
    "print(\"\\nCross-Validation Performance Comparison:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    # Use scaled or unscaled data based on model type\n",
    "    if name in ['Stacking', 'Voting (Soft)']:\n",
    "        X_cv = X_train_scaled\n",
    "    else:\n",
    "        X_cv = X_train\n",
    "    \n",
    "    scores = cross_val_score(model, X_cv, y_train, cv=5, scoring='accuracy')\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'Mean CV Score': scores.mean(),\n",
    "        'Std CV Score': scores.std(),\n",
    "        'Min Score': scores.min(),\n",
    "        'Max Score': scores.max()\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "    print(f\"  Range: [{scores.min():.4f}, {scores.max():.4f}]\")\n",
    "    print()\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation: Measuring Reliability\n",
    "\n",
    "A single test set only shows one snapshot of performance. Cross-validation gives us a more complete picture.\n",
    "\n",
    "**What This Cell Does:**\n",
    "- Performs 5-fold cross-validation on each of our best models\n",
    "- Reports mean score, standard deviation, minimum, and maximum across folds\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "**Mean CV Score**: Average performance across all 5 folds\n",
    "- This is your best estimate of expected performance\n",
    "- More reliable than a single test score\n",
    "\n",
    "**Standard Deviation (Std)**: Variability across folds\n",
    "- **Low std**: Model is stable, consistent predictions regardless of data split\n",
    "- **High std**: Model is sensitive to training data, less reliable\n",
    "- Example: Mean=0.95, Std=0.01 is better than Mean=0.96, Std=0.05\n",
    "\n",
    "**Min and Max Scores**: Range of performance\n",
    "- Shows best-case and worst-case scenarios\n",
    "- Large range (high max-min) suggests instability\n",
    "- Helps identify if model got lucky or is genuinely good\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Model selection**: Choose models with good mean AND low variance\n",
    "- **Trust**: A model with lower mean but higher stability might be preferable for production\n",
    "- **Understanding limitations**: Know the range of expected performance\n",
    "\n",
    "**What Ensembles Should Show:**\n",
    "- Generally lower variance than individual models\n",
    "- This is one of the key benefits of ensembling - more stable predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV performance with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(cv_results_df['Mean CV Score'], cv_results_df['Model'],\n",
    "             xerr=cv_results_df['Std CV Score'], fmt='o', markersize=10,\n",
    "             capsize=5, capthick=2, linewidth=2)\n",
    "plt.xlabel('Cross-Validation Score')\n",
    "plt.title('Cross-Validation Performance Comparison (with Standard Deviation)')\n",
    "plt.xlim(0.90, 1.0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Stability\n",
    "\n",
    "This error bar plot reveals model consistency at a glance.\n",
    "\n",
    "**How to Interpret:**\n",
    "- **Center point (circle)**: Mean cross-validation score\n",
    "- **Horizontal error bars**: Standard deviation (uncertainty)\n",
    "- **Left edge of bar**: Roughly the worst expected performance\n",
    "- **Right edge of bar**: Roughly the best expected performance\n",
    "\n",
    "**Comparing Models:**\n",
    "\n",
    "**Ideal model**: Far right (high mean) with short bars (low variance)\n",
    "- High performance AND consistent\n",
    "\n",
    "**Risky model**: Far right but with long bars\n",
    "- Sometimes great, sometimes mediocre\n",
    "- Unreliable for production use\n",
    "\n",
    "**Stable but limited model**: Middle position with short bars\n",
    "- Predictable but not exceptional\n",
    "- Might be acceptable if consistency matters more than peak performance\n",
    "\n",
    "**What to Look For:**\n",
    "- Do any models have bars that don't overlap? That's a significant performance difference!\n",
    "- Which model has the shortest bars (most stable)?\n",
    "- Is there a model that's both high-performing AND stable?\n",
    "\n",
    "**The Ensemble Advantage**: Typically, ensemble methods should show tighter error bars than individual models - they're more robust to data variations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Performance Summary\n",
    "\n",
    "Let's create a comprehensive summary of all ensemble methods we've explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Grand Finale: Complete Performance Summary\n",
    "\n",
    "Let's bring it all together and see the complete picture of what we've learned!\n",
    "\n",
    "**What the Summary Shows:**\n",
    "- **Every method** we've explored, from baseline to advanced ensembles\n",
    "- Sorted by test accuracy (best at top)\n",
    "- Categorized by ensemble strategy\n",
    "\n",
    "**The Complete Journey:**\n",
    "1. **Baseline**: Single decision tree - our starting point\n",
    "2. **Voting**: Simple combination of diverse models\n",
    "3. **Bagging**: Bootstrap aggregation with decision trees\n",
    "4. **Boosting**: Sequential error correction\n",
    "5. **Stacking**: Meta-learning optimal combinations\n",
    "\n",
    "**Questions to Reflect On:**\n",
    "\n",
    "**Performance:**\n",
    "- How much improvement did we gain from ensembling?\n",
    "- Which strategy worked best for this dataset?\n",
    "- Are the top performers significantly better, or clustered close together?\n",
    "\n",
    "**Complexity vs. Accuracy:**\n",
    "- Is the best model worth its complexity?\n",
    "- Could a simpler model (Random Forest) be \"good enough\"?\n",
    "- What's the performance/interpretability trade-off?\n",
    "\n",
    "**Practical Considerations:**\n",
    "- Training time: Stacking > Boosting > Bagging > Voting > Single model\n",
    "- Prediction speed: Single model > Voting ≈ Bagging > Boosting > Stacking\n",
    "- Interpretability: Single model > Random Forest > other ensembles\n",
    "- Robustness: Ensembles > Single model\n",
    "\n",
    "**Real-World Decision Making:**\n",
    "If you were deploying this in production, which model would you choose and why? Consider:\n",
    "- Required accuracy level\n",
    "- Computational budget\n",
    "- Need for interpretability\n",
    "- Latency requirements\n",
    "- Maintenance complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary table\n",
    "final_summary = pd.DataFrame({\n",
    "    'Ensemble Method': [\n",
    "        'Single Decision Tree (Baseline)',\n",
    "        'Voting - Hard',\n",
    "        'Voting - Soft',\n",
    "        'Bagging (100 trees)',\n",
    "        'Random Forest (200 trees)',\n",
    "        'Gradient Boosting (sklearn)',\n",
    "        'XGBoost',\n",
    "        'LightGBM',\n",
    "        'AdaBoost',\n",
    "        'Stacking'\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        single_tree_acc,\n",
    "        acc_hard,\n",
    "        acc_soft,\n",
    "        bagging_oob.score(X_test, y_test),\n",
    "        rf_final.score(X_test, y_test),\n",
    "        gb_sklearn_acc,\n",
    "        xgb_acc,\n",
    "        lgb_acc,\n",
    "        ada_acc,\n",
    "        stacking_acc\n",
    "    ],\n",
    "    'Category': [\n",
    "        'Baseline',\n",
    "        'Voting',\n",
    "        'Voting',\n",
    "        'Bagging',\n",
    "        'Bagging',\n",
    "        'Boosting',\n",
    "        'Boosting',\n",
    "        'Boosting',\n",
    "        'Boosting',\n",
    "        'Stacking'\n",
    "    ]\n",
    "})\n",
    "\n",
    "final_summary = final_summary.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(final_summary.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "best_model = final_summary.iloc[0]\n",
    "improvement = (best_model['Test Accuracy'] - single_tree_acc) / single_tree_acc * 100\n",
    "print(f\"\\nBest Model: {best_model['Ensemble Method']}\")\n",
    "print(f\"Improvement over baseline: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Final Summary Table\n",
    "\n",
    "This cell compiles all results into one comprehensive comparison table and calculates the total improvement achieved through ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "category_colors = {\n",
    "    'Baseline': 'gray',\n",
    "    'Voting': 'skyblue',\n",
    "    'Bagging': 'lightgreen',\n",
    "    'Boosting': 'orange',\n",
    "    'Stacking': 'purple'\n",
    "}\n",
    "\n",
    "colors = [category_colors[cat] for cat in final_summary['Category']]\n",
    "\n",
    "plt.barh(final_summary['Ensemble Method'], final_summary['Test Accuracy'], color=colors)\n",
    "plt.xlabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Comprehensive Ensemble Methods Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlim(0.85, 1.0)\n",
    "plt.axvline(x=single_tree_acc, color='red', linestyle='--', linewidth=2, label='Baseline', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for idx, row in final_summary.iterrows():\n",
    "    plt.text(row['Test Accuracy'], row['Ensemble Method'], \n",
    "             f\" {row['Test Accuracy']:.4f}\", \n",
    "             va='center', fontsize=9)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=category) \n",
    "                   for category, color in category_colors.items()]\n",
    "plt.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Visual Comparison\n",
    "\n",
    "This comprehensive bar chart is your visual summary of everything we've learned!\n",
    "\n",
    "**Color Coding (The Ensemble Taxonomy):**\n",
    "- **Gray**: Baseline (where we started)\n",
    "- **Sky Blue**: Voting ensembles (simple combinations)\n",
    "- **Light Green**: Bagging ensembles (bootstrap aggregation)\n",
    "- **Orange**: Boosting ensembles (sequential error correction)\n",
    "- **Purple**: Stacking (meta-learning)\n",
    "\n",
    "**Reading the Chart:**\n",
    "- **Bar length**: Test accuracy (longer = better)\n",
    "- **Red dashed line**: Baseline performance (single decision tree)\n",
    "- **Numeric labels**: Exact accuracy values\n",
    "- **Vertical ordering**: Best performer at top\n",
    "\n",
    "**The Story It Tells:**\n",
    "1. **Baseline gap**: Notice how far all ensembles are from the baseline\n",
    "2. **Within-category comparison**: How do different boosting methods compare?\n",
    "3. **Across-category comparison**: Does one ensemble strategy dominate?\n",
    "4. **Practical significance**: Are small differences meaningful, or just noise?\n",
    "\n",
    "**Key Insights to Extract:**\n",
    "- **Ensemble benefit**: All ensemble methods beat the baseline\n",
    "- **Method comparison**: Which category performs best overall?\n",
    "- **Diminishing returns**: Top models cluster together - is the complexity worth marginal gains?\n",
    "- **Consistency**: Methods in the same category perform similarly (validating the concepts!)\n",
    "\n",
    "**Your Takeaway:**\n",
    "Based on this dataset, which ensemble approach would you recommend for a new project, and why? Consider not just accuracy, but the full context of implementation and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Key Takeaways\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through ensemble learning. Let's consolidate what you've learned.\n",
    "\n",
    "## Ensemble Methods Overview\n",
    "\n",
    "### 1. **Using Multiple Models Together (Voting)**\n",
    "**What you learned:**\n",
    "- Combining diverse models often performs better than any individual model\n",
    "- Different algorithms make different mistakes - when combined, errors cancel out\n",
    "- **Hard voting**: Simple majority vote (each model gets one vote)\n",
    "- **Soft voting**: Weighted by confidence (uses probabilities, usually better)\n",
    "\n",
    "**When to use:** Quick improvement over single models, you already have diverse trained models\n",
    "\n",
    "**Key insight:** The \"wisdom of crowds\" applies to machine learning!\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Random Forests and Gradient Boosted Trees**\n",
    "\n",
    "#### Random Forests (Bagging Approach)\n",
    "**What you learned:**\n",
    "- Build many decision trees independently in parallel\n",
    "- Each tree sees a random subset of data (bootstrap) and features\n",
    "- Average predictions to reduce variance\n",
    "- Resistant to overfitting (can add more trees safely)\n",
    "- Provides feature importance for interpretability\n",
    "\n",
    "**Strengths:**\n",
    "- Good default choice for many problems\n",
    "- Handles non-linear relationships naturally\n",
    "- Works well out-of-the-box with minimal tuning\n",
    "- Resistant to overfitting\n",
    "- Fast to train (parallelizable)\n",
    "\n",
    "**When to use:** Your first choice for most classification/regression problems\n",
    "\n",
    "#### Gradient Boosted Trees (Boosting Approach)\n",
    "**What you learned:**\n",
    "- Build trees sequentially, not in parallel\n",
    "- Each new tree corrects errors of previous trees\n",
    "- Reduces bias by iteratively improving predictions\n",
    "- More prone to overfitting than Random Forests (need to tune carefully)\n",
    "- Three excellent implementations: sklearn, XGBoost, LightGBM\n",
    "\n",
    "**Strengths:**\n",
    "- Often achieves highest accuracy with proper tuning\n",
    "- Powerful for complex patterns\n",
    "- XGBoost/LightGBM optimized for speed and performance\n",
    "\n",
    "**Weaknesses:**\n",
    "- Requires more careful hyperparameter tuning\n",
    "- Can overfit if not monitored\n",
    "- Sequential training (slower than parallel methods)\n",
    "\n",
    "**When to use:** When you need maximum accuracy and have time to tune parameters\n",
    "\n",
    "**Key difference:** Bagging reduces variance, Boosting reduces bias\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Bootstrap Aggregation (Bagging) - Deep Dive**\n",
    "\n",
    "**What you learned:**\n",
    "- Bootstrap sampling: Random sampling **with replacement**\n",
    "- Each sample uses ~63.2% unique data (mathematical result)\n",
    "- Remaining ~36.8% are \"out-of-bag\" (OOB) samples\n",
    "- OOB samples provide free validation without reducing training data\n",
    "\n",
    "**How it works:**\n",
    "1. Create multiple bootstrap samples from training data\n",
    "2. Train a model on each bootstrap sample\n",
    "3. Aggregate predictions (vote for classification, average for regression)\n",
    "\n",
    "**Why it works:**\n",
    "- Creates diversity through different training sets\n",
    "- Reduces variance by averaging predictions\n",
    "- Each model sees different data, learns different patterns\n",
    "- Errors are random and cancel out, correct predictions reinforce\n",
    "\n",
    "**Perfect for:** High-variance models (Decision Trees, Neural Networks, KNN)\n",
    "\n",
    "**The OOB advantage:** Get validation score without using separate validation set - especially valuable for small datasets!\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Combining Heterogeneous Models (Stacking)**\n",
    "\n",
    "**What you learned:**\n",
    "- Most sophisticated ensemble approach\n",
    "- **Level 0 (Base models):** Train diverse algorithms (RF, GB, SVM, KNN, etc.)\n",
    "- **Level 1 (Meta-model):** Learns how to optimally combine base model predictions\n",
    "- Uses cross-validation to avoid overfitting (critical!)\n",
    "\n",
    "**How it's different from voting:**\n",
    "- Voting: Fixed combination rule (average or majority)\n",
    "- Stacking: Learns the optimal combination through a meta-model\n",
    "\n",
    "**Advantages:**\n",
    "- Can capture complementary strengths of different algorithms\n",
    "- Meta-model learns which models to trust in which situations\n",
    "- Often achieves best performance\n",
    "\n",
    "**Disadvantages:**\n",
    "- More complex to implement and tune\n",
    "- Longer training time (train base models + meta-model)\n",
    "- Less interpretable\n",
    "- Can overfit if not using cross-validation properly\n",
    "\n",
    "**When to use:** Maximum accuracy needed, have computational resources, competitions\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Evaluating Ensembles Properly**\n",
    "\n",
    "**What you learned:**\n",
    "- **Multiple metrics matter:** Don't rely on accuracy alone\n",
    "  - **Precision:** Of predicted positives, how many were correct? (minimize false alarms)\n",
    "  - **Recall:** Of actual positives, how many did we find? (minimize missed cases)\n",
    "  - **F1-Score:** Balance between precision and recall\n",
    "  \n",
    "- **Confusion matrix:** Shows exactly which classes are confused with each other\n",
    "\n",
    "- **Cross-validation:** More reliable than single train/test split\n",
    "  - Mean score: Expected performance\n",
    "  - Standard deviation: Stability/consistency\n",
    "  - Ensembles should show lower variance!\n",
    "\n",
    "- **Consider the full picture:**\n",
    "  - Accuracy (overall correctness)\n",
    "  - Per-class performance (some classes harder than others?)\n",
    "  - Stability across folds (reliable predictions?)\n",
    "  - Computational cost (training time, prediction speed)\n",
    "  - Interpretability (can you explain predictions?)\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices You Should Follow\n",
    "\n",
    "### 1. **Start Simple, Then Ensemble**\n",
    "- Begin with a single well-tuned model (baseline)\n",
    "- Try Random Forest as your first ensemble (great default)\n",
    "- If you need more accuracy, try Gradient Boosting\n",
    "- Only use stacking if you have time and need maximum performance\n",
    "\n",
    "### 2. **Ensure Model Diversity**\n",
    "For voting and stacking to work, base models should:\n",
    "- Use different algorithms (linear, tree-based, distance-based)\n",
    "- Make different types of errors\n",
    "- Have decent individual performance (bad models → bad ensemble)\n",
    "\n",
    "### 3. **Validate Properly**\n",
    "- Always use cross-validation for robust estimates\n",
    "- Use OOB scoring for bagging methods (free validation!)\n",
    "- Keep a separate test set for final evaluation\n",
    "- Watch for overfitting (train vs validation performance)\n",
    "\n",
    "### 4. **Tune Hyperparameters**\n",
    "- Random Forest: `n_estimators`, `max_depth`, `min_samples_split`\n",
    "- Gradient Boosting: `n_estimators`, `learning_rate`, `max_depth`\n",
    "- Stacking: Choice of base models and meta-model\n",
    "\n",
    "### 5. **Consider Practical Constraints**\n",
    "- **Training time:** Single < Voting < Bagging < Boosting < Stacking\n",
    "- **Prediction speed:** Single ≈ Voting < Bagging < Boosting < Stacking\n",
    "- **Memory:** Ensembles use more memory (store multiple models)\n",
    "- **Interpretability:** Single > Random Forest (feature importance) > other ensembles\n",
    "\n",
    "### 6. **Monitor Complexity vs. Performance**\n",
    "- More complex ensembles aren't always better\n",
    "- A 2% accuracy gain might not justify 10× training time\n",
    "- Consider the full system context, not just accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Guide: When to Use Each Method\n",
    "\n",
    "### Random Forest\n",
    "**Use when:**\n",
    "- ✅ You need a strong baseline quickly\n",
    "- ✅ You have tabular data with many features\n",
    "- ✅ You want built-in feature importance\n",
    "- ✅ You don't have time for extensive tuning\n",
    "- ✅ You want robust, stable predictions\n",
    "\n",
    "**Best for:** Default choice for most classification/regression problems\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Boosting (XGBoost/LightGBM)\n",
    "**Use when:**\n",
    "- ✅ You need maximum accuracy\n",
    "- ✅ You have time to tune hyperparameters\n",
    "- ✅ You have structured/tabular data\n",
    "- ✅ You can monitor for overfitting\n",
    "\n",
    "**Best for:** Kaggle competitions, high-stakes applications where accuracy is paramount\n",
    "\n",
    "---\n",
    "\n",
    "### Bagging (General)\n",
    "**Use when:**\n",
    "- ✅ You have a high-variance base model (Decision Trees, Neural Networks)\n",
    "- ✅ You want to reduce overfitting\n",
    "- ✅ You have limited training data (OOB scoring helps)\n",
    "- ✅ You can train models in parallel\n",
    "\n",
    "**Best for:** Stabilizing unstable models\n",
    "\n",
    "---\n",
    "\n",
    "### Voting\n",
    "**Use when:**\n",
    "- ✅ You already have several trained models\n",
    "- ✅ You want quick ensemble without retraining\n",
    "- ✅ Models are diverse (different algorithms)\n",
    "- ✅ You want interpretable combination\n",
    "\n",
    "**Best for:** Combining pre-existing models, quick wins\n",
    "\n",
    "---\n",
    "\n",
    "### Stacking\n",
    "**Use when:**\n",
    "- ✅ You need absolute maximum performance\n",
    "- ✅ You have diverse base models available\n",
    "- ✅ You have computational resources for training\n",
    "- ✅ Accuracy justifies complexity\n",
    "\n",
    "**Best for:** Competitions, critical applications, squeezing out last % of accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls to Avoid\n",
    "\n",
    "❌ **Using identical base models in voting/stacking**\n",
    "- Diversity is key! Use different algorithms\n",
    "\n",
    "❌ **Not using cross-validation in stacking**\n",
    "- Meta-model will overfit to training predictions\n",
    "\n",
    "❌ **Adding too many trees without checking**\n",
    "- More trees = more compute, diminishing returns after a point\n",
    "\n",
    "❌ **Forgetting to scale features for distance-based models**\n",
    "- SVM, KNN, Logistic Regression need standardized features\n",
    "- Trees don't need scaling\n",
    "\n",
    "❌ **Relying only on accuracy**\n",
    "- Check precision, recall, confusion matrix\n",
    "- Some errors might be more costly than others\n",
    "\n",
    "❌ **Not considering deployment constraints**\n",
    "- Model accuracy is useless if it's too slow/large for production\n",
    "\n",
    "---\n",
    "\n",
    "## What You Should Remember\n",
    "\n",
    "🎯 **Core Principle:** Ensemble methods work by combining diverse models to reduce errors\n",
    "\n",
    "🎯 **Bagging:** Reduces variance by averaging independent models (Random Forest)\n",
    "\n",
    "🎯 **Boosting:** Reduces bias by sequentially correcting errors (Gradient Boosting)\n",
    "\n",
    "🎯 **Voting:** Simple combination of diverse algorithms\n",
    "\n",
    "🎯 **Stacking:** Meta-learning optimal combinations\n",
    "\n",
    "🎯 **Practical wisdom:** Random Forest is your go-to, Gradient Boosting when you need more, Stacking when you need the absolute best\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps and Further Learning\n",
    "\n",
    "**Practice exercises to deepen understanding:**\n",
    "1. Apply these methods to a different dataset (imbalanced classes, regression, etc.)\n",
    "2. Implement hyperparameter tuning with GridSearchCV\n",
    "3. Build a stacking ensemble manually (without StackingClassifier)\n",
    "4. Compare training times and prediction speeds\n",
    "5. Try ensemble methods on a large dataset (>100K samples)\n",
    "\n",
    "**Advanced topics to explore:**\n",
    "- Ensemble diversity metrics\n",
    "- Weighted voting strategies\n",
    "- Multi-level stacking\n",
    "- Ensemble pruning (removing weak models)\n",
    "- Online learning with ensembles\n",
    "- Deep learning ensembles\n",
    "\n",
    "**Real-world applications:**\n",
    "- Medical diagnosis (where accuracy and reliability matter)\n",
    "- Fraud detection (imbalanced classes)\n",
    "- Recommendation systems (combining collaborative and content-based)\n",
    "- Financial forecasting (ensemble predictions are more stable)\n",
    "\n",
    "---\n",
    "\n",
    "## Final Reflection Questions\n",
    "\n",
    "Take a moment to think about:\n",
    "\n",
    "1. Which ensemble method surprised you most in its performance?\n",
    "2. How would you explain the difference between bagging and boosting to a colleague?\n",
    "3. In a real project with tight deadlines, which method would you choose first and why?\n",
    "4. What trade-offs would you consider when moving a model to production?\n",
    "5. How could you determine if the complexity of stacking is justified for your use case?\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have a solid foundation in ensemble learning. These techniques are among the most powerful tools in modern machine learning. Practice applying them to different problems, and you'll develop intuition for when and how to use each approach effectively.\n",
    "\n",
    "Remember: **The best model is not always the most complex one, but the one that best balances your accuracy requirements, computational constraints, and interpretability needs for your specific problem.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
