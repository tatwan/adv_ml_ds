{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Undersampling\n",
    "\n",
    "\n",
    "Random undersampling consists in extracting at random samples from the majority class, until they reach a certain proportion compared to the minority class, typically 50:50.\n",
    "\n",
    "- **Criteria for data exclusion**: Random\n",
    "- **Final Dataset size**: 2 x minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. What random undersampling is and when to use it\n",
    "2. How to implement random undersampling using imbalanced-learn\n",
    "3. The advantages and disadvantages of this technique\n",
    "4. How undersampling affects machine learning model performance\n",
    "\n",
    "## Why Random Undersampling?\n",
    "\n",
    "When dealing with imbalanced datasets, machine learning algorithms often:\n",
    "- **Bias towards the majority class** (predicting the most frequent class)\n",
    "- **Poor performance on minority class** (missing important cases)\n",
    "- **High accuracy but low practical value** (accuracy paradox)\n",
    "\n",
    "Random undersampling is one of the simplest techniques to address class imbalance by reducing the majority class size.\n",
    "\n",
    "## Import Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries for data manipulation, visualization, and machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data\n",
    "\n",
    "We will create data where the classes have different degrees of separateness.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(sep):\n",
    "    \n",
    "    # returns arrays\n",
    "    X, y = make_classification(n_samples=1000,\n",
    "                           n_features=2,\n",
    "                           n_redundant=0,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[0.99],\n",
    "                           class_sep=sep,# how separate the classes are\n",
    "                           random_state=1)\n",
    "    \n",
    "    # trasform arrays into pandas df and series\n",
    "    X = pd.DataFrame(X, columns =['varA', 'varB'])\n",
    "    y = pd.Series(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Class Separability\n",
    "\n",
    "Before we dive into undersampling, let's understand how **class separability** affects our data. We'll create synthetic datasets with different degrees of overlap between classes to see how this impacts the effectiveness of undersampling techniques.\n",
    "\n",
    "The `make_classification` function allows us to control:\n",
    "- **class_sep**: How well-separated the classes are (higher = easier to distinguish)\n",
    "- **weights**: The proportion of each class (creating imbalance)\n",
    "- **n_samples**: Total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make datasets with different class separateness\n",
    "# and plot\n",
    "\n",
    "for sep in [0.1, 1., 2.]:\n",
    "    \n",
    "    X, y = make_data(sep)\n",
    "    \n",
    "    print(y.value_counts())\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=X, x=\"varA\", y=\"varB\", hue=y\n",
    "    )\n",
    "    \n",
    "    plt.title('Separation: {}'.format(sep))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Different Levels of Class Separability\n",
    "\n",
    "Let's create and visualize datasets with different separation levels to understand how class overlap affects our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the parameter **sep**, the minority and majority class show less degree of overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from the Plots\n",
    "\n",
    "**What you should notice:**\n",
    "- **Separation = 0.1**: Classes heavily overlap (harder to distinguish)\n",
    "- **Separation = 1.0**: Moderate overlap (some difficulty in classification)  \n",
    "- **Separation = 2.0**: Well-separated classes (easier to classify)\n",
    "\n",
    "**Class Distribution**: Notice that we have approximately 990 samples of class 0 (majority) and only 10 samples of class 1 (minority) - a 99:1 imbalance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Undersampling\n",
    "\n",
    "[RandomUnderSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html)\n",
    "\n",
    "Selects samples from the majority class at random, until we have as many observations as those in the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "\n",
    "X, y = make_data(sep=2)\n",
    "\n",
    "# set up the random undersampling class\n",
    "\n",
    "rus = RandomUnderSampler(\n",
    "    sampling_strategy='auto',  # samples only the majority class\n",
    "    random_state=0,  # for reproducibility\n",
    "    replacement=True # if it should resample with replacement\n",
    ")  \n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Random Undersampling\n",
    "\n",
    "Now let's apply random undersampling to our well-separated dataset (sep=2). We'll use the `RandomUnderSampler` from imbalanced-learn:\n",
    "\n",
    "**Key Parameters:**\n",
    "- **sampling_strategy='auto'**: Automatically balances by sampling majority class to match minority class size\n",
    "- **random_state**: Ensures reproducible results\n",
    "- **replacement**: Whether to sample with replacement (allows duplicate selections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of original data\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Dataset Size Changes\n",
    "\n",
    "Let's examine how the dataset size changes after undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of undersampled data\n",
    "\n",
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of minority class observations\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of original data\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=X, x=\"varA\", y=\"varB\", hue=y\n",
    ")\n",
    "\n",
    "plt.title('Original dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparison: Before and After Undersampling\n",
    "\n",
    "Let's visualize the original imbalanced dataset vs. the balanced dataset after undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot undersampled data\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=X_resampled, x=\"varA\", y=\"varB\", hue=y_resampled\n",
    ")\n",
    "\n",
    "plt.title('Undersampled dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples show a similar observation as they do in the original dataset. This is the product of removing data at random.\n",
    "\n",
    "**HOMEWORK**: \n",
    "\n",
    "- change the degree of separateness when creating the data, and re-run the cells above. Then compare the plots.\n",
    "\n",
    "## Changing the balancing ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, I will resample the data, so that I obtain\n",
    "# twice as many observations from the majority as\n",
    "# those from the minority\n",
    "\n",
    "rus = RandomUnderSampler(\n",
    "    sampling_strategy= 0.5,  # remember balancing ratio = x min / x maj\n",
    "    random_state=0,  \n",
    "    replacement=False # if it should resample with replacement\n",
    ")  \n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the Balance Ratio\n",
    "\n",
    "Random undersampling doesn't always need to create a 50:50 balance. We can specify different ratios using the `sampling_strategy` parameter:\n",
    "\n",
    "**sampling_strategy = 0.5** means: minority_samples / majority_samples = 0.5\n",
    "- If we have 10 minority samples, we'll keep 20 majority samples (10/20 = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of undersampled data\n",
    "\n",
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see that we have twice as many of the majority now\n",
    "\n",
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can also specify how many observations we want\n",
    "# from each class\n",
    "\n",
    "rus = RandomUnderSampler(\n",
    "    sampling_strategy= {0:100, 1:15},  # remember balancing ratio = x min / x maj\n",
    "    random_state=0,  \n",
    "    replacement=False # if it should resample with replacement\n",
    ")  \n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Exact Sample Counts\n",
    "\n",
    "For even more precise control, we can specify exactly how many samples we want from each class using a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of undersampled data\n",
    "\n",
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have what we asked for :)\n",
    "\n",
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfect Control:**\n",
    "- **Class 0 (majority)**: Exactly 100 samples as requested\n",
    "- **Class 1 (minority)**: Exactly 15 samples as requested  \n",
    "- **Total samples**: 115 samples\n",
    "- **Custom ratio**: 100:15 ≈ 6.7:1 ratio\n",
    "\n",
    "This approach is useful when you have domain knowledge about the optimal class distribution for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('../kdd2004.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Application: Credit Risk Dataset\n",
    "\n",
    "Now let's apply random undersampling to a real-world dataset - the KDD 2004 credit risk dataset. This dataset contains features related to credit applications and whether they were approved or rejected.\n",
    "\n",
    "**Why this dataset?**\n",
    "- **Real imbalanced data**: Reflects actual business scenarios\n",
    "- **High dimensionality**: 74 features to work with\n",
    "- **Practical relevance**: Credit risk is a common use case for imbalanced learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced target\n",
    "data.target.value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Class Imbalance\n",
    "\n",
    "Let's examine the severity of class imbalance in this real-world dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Severe Imbalance Detected:**\n",
    "- **Class 0 (rejected)**: ~91.4% of all applications \n",
    "- **Class 1 (approved)**: ~8.6% of all applications\n",
    "- **Imbalance ratio**: Approximately 10.6:1\n",
    "- **Problem**: Models will likely predict \"rejection\" for most cases, missing many valid approvals\n",
    "\n",
    "This level of imbalance is typical in credit risk scenarios where most applications are rejected for various reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataset into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),  # drop the target\n",
    "    data['target'],  # just the target\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train/Test Split\n",
    "\n",
    "**Important principle:** Always split your data BEFORE applying any resampling technique to avoid data leakage.\n",
    "\n",
    "**Why this order matters:**\n",
    "1. **Split first**: Ensures test set represents real-world distribution\n",
    "2. **Resample training only**: Test set remains untouched for unbiased evaluation\n",
    "3. **Prevent data leakage**: No information from test set influences training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Undersampling\n",
    "\n",
    "[RandomUnderSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(\n",
    "    sampling_strategy='auto',  # samples only from majority class\n",
    "    random_state=0,  # for reproducibility\n",
    "    replacement=True # if it should resample with replacement\n",
    ")  \n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Random Undersampling to Real Data\n",
    "\n",
    "Now let's apply random undersampling to our credit risk training data. We'll use `replacement=True` to allow sampling with replacement, which can be useful when the minority class is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of undersampled data\n",
    "\n",
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of positive class in original dataset\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data size is 2 times the number of observations\n",
    "# with positive class:\n",
    "\n",
    "y_train.value_counts()[1] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data\n",
    "\n",
    "Let's compare how the data looks before and after the undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data.sample(1784, random_state=0),\n",
    "                x=\"0\",\n",
    "                y=\"1\",\n",
    "                hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing High-Dimensional Data\n",
    "\n",
    "Since we're dealing with 74 features, we can only visualize 2 dimensions at a time. Let's examine features 0 and 1 to see how the class distributions change:\n",
    "\n",
    "**Note**: We're sampling 1,784 points from the original data to match the undersampled dataset size for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [str(i) for i in range(74)] +['target']\n",
    "\n",
    "data_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "data_resampled.columns = col_names\n",
    "\n",
    "sns.scatterplot(data=data_resampled, x=\"0\", y=\"1\", hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions are similar to that of the original data. The reason you see more purple dots, is because now they are not covered by the pink ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data.sample(1784, random_state=0),\n",
    "                x=\"4\",\n",
    "                y=\"5\",\n",
    "                hue=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data_resampled, x=\"4\", y=\"5\", hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning performance comparison\n",
    "\n",
    "Let's compare model performance with and without undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train random forests and evaluate the performance\n",
    "\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ultimate Test: Machine Learning Performance\n",
    "\n",
    "Now comes the crucial question: **Does random undersampling actually improve model performance?**\n",
    "\n",
    "We'll compare a Random Forest classifier trained on:\n",
    "1. **Original imbalanced dataset** \n",
    "2. **Undersampled balanced dataset**\n",
    "\n",
    "**Evaluation metric**: We'll use ROC-AUC because it's robust to class imbalance and measures the model's ability to distinguish between classes.\n",
    "\n",
    "Let's define a function to train and evaluate Random Forest models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance of algorithm built\n",
    "# using imbalanced dataset\n",
    "\n",
    "run_randomForests(X_train,\n",
    "                  X_test,\n",
    "                  y_train,\n",
    "                  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on Original Imbalanced Data\n",
    "\n",
    "First, let's see how well our model performs when trained on the original, highly imbalanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance of algorithm built\n",
    "# using undersampled dataset\n",
    "\n",
    "run_randomForests(X_resampled,\n",
    "                  X_test,\n",
    "                  y_resampled,\n",
    "                  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on Undersampled Balanced Data\n",
    "\n",
    "Now let's see the performance improvement when training on the balanced dataset created through random undersampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big jump in model performance.\n",
    "\n",
    "**HOMEWORK**\n",
    "\n",
    "- Try random undersampling with and without replacement, and with different machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎉 Remarkable Performance Improvement!\n",
    "\n",
    "**Key Results:**\n",
    "- **Training set improvement**: Significant increase in ROC-AUC score\n",
    "- **Test set improvement**: Substantial boost in generalization performance  \n",
    "- **Success**: Random undersampling successfully addressed the class imbalance problem!\n",
    "\n",
    "**Why does this work?**\n",
    "1. **Balanced learning**: Model sees equal examples of both classes during training\n",
    "2. **Better decision boundaries**: Algorithm can learn to distinguish minority class patterns\n",
    "3. **Reduced bias**: Less tendency to always predict the majority class\n",
    "4. **Improved sensitivity**: Better at identifying positive cases (approved applications)\n",
    "\n",
    "**Trade-offs to consider:**\n",
    "- ✅ **Pros**: Simple, fast, often effective, preserves data distribution\n",
    "- ❌ **Cons**: Potential information loss, smaller training set, may remove important samples\n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "**What we learned:**\n",
    "- Random undersampling is effective for addressing class imbalance\n",
    "- It can significantly improve model performance on minority class detection\n",
    "- The technique is simple to implement and computationally efficient\n",
    "- Proper train/test splitting is crucial to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
