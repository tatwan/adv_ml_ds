{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75c2315",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Data in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd3aa7",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive guide to understanding and handling imbalanced datasets in machine learning. We will explore various techniques, including oversampling, undersampling, and advanced methods like SMOTE and ensemble learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5131c62c",
   "metadata": {},
   "source": [
    "## 1. Introduction to Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf94ac3",
   "metadata": {},
   "source": [
    "An imbalanced dataset is a dataset where the number of observations per class is not equally distributed. This is a common problem in many real-world scenarios such as fraud detection, medical diagnosis, and spam filtering. When one class (the majority class) significantly outnumbers another class (the minority class), machine learning models can become biased towards the majority class, leading to poor performance on the minority class, which is often the class of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c227302",
   "metadata": {},
   "source": [
    "### 1.1 Why is it a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca25ad",
   "metadata": {},
   "source": [
    "Most standard machine learning algorithms assume that the class distribution is balanced. When this assumption is violated, the model may achieve high accuracy by simply predicting the majority class for all instances, while completely ignoring the minority class. This is problematic because in many applications, correctly identifying the minority class is crucial (e.g., detecting a rare disease or a fraudulent transaction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99f197",
   "metadata": {},
   "source": [
    "### 1.2 Overview of Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dddbd26",
   "metadata": {},
   "source": [
    "There are several techniques to handle imbalanced data, which can be broadly categorized into:\n",
    "\n",
    "*   **Data-level methods:** These techniques modify the training data to create a balanced class distribution. This includes oversampling the minority class, undersampling the majority class, or a combination of both.\n",
    "*   **Algorithm-level methods:** These techniques modify the learning algorithm to be more sensitive to the minority class. This often involves assigning different weights to the classes or using cost-sensitive learning.\n",
    "*   **Ensemble methods:** These techniques combine multiple models to improve performance. Specialized ensemble methods have been developed for imbalanced learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d27a79",
   "metadata": {},
   "source": [
    "## 2. Setup and Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d7b56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:23.161944Z",
     "iopub.status.busy": "2025-10-22T19:19:23.161684Z",
     "iopub.status.idle": "2025-10-22T19:19:31.059935Z",
     "shell.execute_reply": "2025-10-22T19:19:31.059120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier, BalancedBaggingClassifier\n",
    "from collections import Counter\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d37aa2",
   "metadata": {},
   "source": [
    "### 2.1 Create a Synthetic Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb37c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:31.063294Z",
     "iopub.status.busy": "2025-10-22T19:19:31.062895Z",
     "iopub.status.idle": "2025-10-22T19:19:31.513003Z",
     "shell.execute_reply": "2025-10-22T19:19:31.512296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0,\n",
    "                           n_classes=2, n_clusters_per_class=1, weights=[0.95, 0.05],\n",
    "                           flip_y=0, random_state=42)\n",
    "\n",
    "# Visualize the class distribution\n",
    "counter = Counter(y)\n",
    "print(f'Original dataset shape {X.shape}')\n",
    "print(f'Original dataset samples per class {counter}')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y)\n",
    "plt.title('Original Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afdfafe",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics for Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37978f",
   "metadata": {},
   "source": [
    "As discussed earlier, accuracy is not a suitable metric for evaluating models trained on imbalanced data. Instead, we should use metrics that provide a better picture of the model's performance on the minority class. Some of these metrics are:\n",
    "\n",
    "*   **Confusion Matrix:** A table that summarizes the performance of a classification model.\n",
    "*   **Precision:** The ratio of correctly predicted positive observations to the total predicted positive observations. `Precision = TP / (TP + FP)`\n",
    "*   **Recall (Sensitivity):** The ratio of correctly predicted positive observations to all observations in the actual class. `Recall = TP / (TP + FN)`\n",
    "*   **F1-Score:** The harmonic mean of precision and recall. `F1 Score = 2 * (Recall * Precision) / (Recall + Precision)`\n",
    "*   **ROC-AUC:** The Area Under the Receiver Operating Characteristic Curve. It measures the ability of a classifier to distinguish between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6f91e",
   "metadata": {},
   "source": [
    "### 3.1 Helper Function for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d58af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:31.515941Z",
     "iopub.status.busy": "2025-10-22T19:19:31.515699Z",
     "iopub.status.idle": "2025-10-22T19:19:31.520460Z",
     "shell.execute_reply": "2025-10-22T19:19:31.519750Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(X_test, y_test, model):\n",
    "    # Predict probabilities\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Predict classes\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print classification report\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Print ROC-AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
    "    \n",
    "    # Plotting Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad80f9f",
   "metadata": {},
   "source": [
    "## 4. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc0e7d",
   "metadata": {},
   "source": [
    "Let's first train a simple Logistic Regression model on the original, imbalanced dataset. This will serve as our baseline to see how the different data handling techniques improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf22724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:31.523044Z",
     "iopub.status.busy": "2025-10-22T19:19:31.522842Z",
     "iopub.status.idle": "2025-10-22T19:19:31.666118Z",
     "shell.execute_reply": "2025-10-22T19:19:31.665435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Train a baseline logistic regression model\n",
    "baseline_model = LogisticRegression(random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the baseline model\n",
    "print('--- Baseline Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061de156",
   "metadata": {},
   "source": [
    "## 5. Oversampling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61275b",
   "metadata": {},
   "source": [
    "Oversampling techniques increase the number of instances in the minority class to balance the dataset. Let's explore some popular oversampling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228465ab",
   "metadata": {},
   "source": [
    "### 5.1 Random Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b468ec",
   "metadata": {},
   "source": [
    "Random oversampling involves randomly duplicating examples from the minority class and adding them to the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59cde6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:31.668996Z",
     "iopub.status.busy": "2025-10-22T19:19:31.668773Z",
     "iopub.status.idle": "2025-10-22T19:19:31.811573Z",
     "shell.execute_reply": "2025-10-22T19:19:31.810817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Random Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the class distribution\n",
    "print(f'Resampled dataset shape {X_ros.shape}')\n",
    "print(f'Resampled dataset samples per class {Counter(y_ros)}')\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "ros_model = LogisticRegression(random_state=42)\n",
    "ros_model.fit(X_ros, y_ros)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- Random Oversampling Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, ros_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ccc7c",
   "metadata": {},
   "source": [
    "### 5.2 SMOTE (Synthetic Minority Oversampling Technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda8f79",
   "metadata": {},
   "source": [
    "SMOTE is a more sophisticated oversampling technique. Instead of duplicating minority class instances, it creates synthetic instances by interpolating between existing minority class instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6c6a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:31.813682Z",
     "iopub.status.busy": "2025-10-22T19:19:31.813472Z",
     "iopub.status.idle": "2025-10-22T19:19:31.958670Z",
     "shell.execute_reply": "2025-10-22T19:19:31.957905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the class distribution\n",
    "print(f'Resampled dataset shape {X_smote.shape}')\n",
    "print(f'Resampled dataset samples per class {Counter(y_smote)}')\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "smote_model = LogisticRegression(random_state=42)\n",
    "smote_model.fit(X_smote, y_smote)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- SMOTE Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, smote_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00481cbb",
   "metadata": {},
   "source": [
    "### 5.3 Borderline SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9b329",
   "metadata": {},
   "source": [
    "Borderline SMOTE is a variant of SMOTE that focuses on generating synthetic samples along the decision boundary between the minority and majority classes. It identifies minority class samples that are difficult to classify (i.e., those with many majority class neighbors) and generates synthetic samples from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76435d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:31.961324Z",
     "iopub.status.busy": "2025-10-22T19:19:31.961106Z",
     "iopub.status.idle": "2025-10-22T19:19:32.103248Z",
     "shell.execute_reply": "2025-10-22T19:19:32.102260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Borderline SMOTE\n",
    "bsmote = BorderlineSMOTE(random_state=42)\n",
    "X_bsmote, y_bsmote = bsmote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the class distribution\n",
    "print(f'Resampled dataset shape {X_bsmote.shape}')\n",
    "print(f'Resampled dataset samples per class {Counter(y_bsmote)}')\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "bsmote_model = LogisticRegression(random_state=42)\n",
    "bsmote_model.fit(X_bsmote, y_bsmote)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- Borderline SMOTE Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, bsmote_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab3e6c",
   "metadata": {},
   "source": [
    "### 5.4 ADASYN (Adaptive Synthetic Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd59a0",
   "metadata": {},
   "source": [
    "ADASYN is another adaptive oversampling technique. It generates more synthetic data for minority class samples that are harder to learn, based on the density of majority class samples in their neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54b20c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:32.105465Z",
     "iopub.status.busy": "2025-10-22T19:19:32.105243Z",
     "iopub.status.idle": "2025-10-22T19:19:32.259531Z",
     "shell.execute_reply": "2025-10-22T19:19:32.258264Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_adasyn, y_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the class distribution\n",
    "print(f'Resampled dataset shape {X_adasyn.shape}')\n",
    "print(f'Resampled dataset samples per class {Counter(y_adasyn)}')\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "adasyn_model = LogisticRegression(random_state=42)\n",
    "adasyn_model.fit(X_adasyn, y_adasyn)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- ADASYN Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, adasyn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb645d9",
   "metadata": {},
   "source": [
    "## 6. Undersampling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdfcf9",
   "metadata": {},
   "source": [
    "Undersampling techniques reduce the number of instances in the majority class to balance the dataset. This can be useful when the dataset is very large and training time is a concern. However, it can also lead to loss of important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee283857",
   "metadata": {},
   "source": [
    "### 6.1 Random Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f9575",
   "metadata": {},
   "source": [
    "Random undersampling involves randomly removing examples from the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d84c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:32.261795Z",
     "iopub.status.busy": "2025-10-22T19:19:32.261588Z",
     "iopub.status.idle": "2025-10-22T19:19:32.406756Z",
     "shell.execute_reply": "2025-10-22T19:19:32.406013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the class distribution\n",
    "print(f'Resampled dataset shape {X_rus.shape}')\n",
    "print(f'Resampled dataset samples per class {Counter(y_rus)}')\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "rus_model = LogisticRegression(random_state=42)\n",
    "rus_model.fit(X_rus, y_rus)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- Random Undersampling Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, rus_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5402da8",
   "metadata": {},
   "source": [
    "### 6.2 Tomek Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173bc59",
   "metadata": {},
   "source": [
    "Tomek links are pairs of instances of opposite classes that are their own nearest neighbors. In the context of undersampling, Tomek links can be used to remove majority class instances that are close to minority class instances, which helps to clean the class boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee28ce1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:32.409071Z",
     "iopub.status.busy": "2025-10-22T19:19:32.408860Z",
     "iopub.status.idle": "2025-10-22T19:19:32.570124Z",
     "shell.execute_reply": "2025-10-22T19:19:32.569386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Tomek Links\n",
    "tl = TomekLinks()\n",
    "X_tl, y_tl = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the class distribution\n",
    "print(f'Resampled dataset shape {X_tl.shape}')\n",
    "print(f'Resampled dataset samples per class {Counter(y_tl)}')\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "tl_model = LogisticRegression(random_state=42)\n",
    "tl_model.fit(X_tl, y_tl)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- Tomek Links Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, tl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8a397",
   "metadata": {},
   "source": [
    "### 6.3 NearMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d93a1",
   "metadata": {},
   "source": [
    "NearMiss is an undersampling technique that selects majority class samples based on their distance to minority class samples. There are three versions of NearMiss:\n",
    "\n",
    "*   **NearMiss-1:** Selects majority class samples with the smallest average distance to the *three* closest minority class samples.\n",
    "*   **NearMiss-2:** Selects majority class samples with the smallest average distance to the *three* farthest minority class samples.\n",
    "*   **NearMiss-3:** Selects a given number of the closest majority class samples for each minority class sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c82b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:32.573358Z",
     "iopub.status.busy": "2025-10-22T19:19:32.573117Z",
     "iopub.status.idle": "2025-10-22T19:19:32.723309Z",
     "shell.execute_reply": "2025-10-22T19:19:32.722556Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply NearMiss\n",
    "nm = NearMiss()\n",
    "X_nm, y_nm = nm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the class distribution\n",
    "print(f'Resampled dataset shape {X_nm.shape}')\n",
    "print(f'Resampled dataset samples per class {Counter(y_nm)}')\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "nm_model = LogisticRegression(random_state=42)\n",
    "nm_model.fit(X_nm, y_nm)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- NearMiss Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, nm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec76ff",
   "metadata": {},
   "source": [
    "## 7. Combination Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e9e30",
   "metadata": {},
   "source": [
    "Combination techniques, also known as hybrid methods, combine oversampling and undersampling techniques to achieve a better-balanced dataset. These methods can often provide better results than using either oversampling or undersampling alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24d820",
   "metadata": {},
   "source": [
    "### 7.1 SMOTE + Tomek Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145aef9e",
   "metadata": {},
   "source": [
    "This method first uses SMOTE to oversample the minority class, and then uses Tomek Links to remove instances from the majority class that are close to the minority class instances. This helps to clean the class boundary and remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b4edf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:32.725702Z",
     "iopub.status.busy": "2025-10-22T19:19:32.725485Z",
     "iopub.status.idle": "2025-10-22T19:19:32.886008Z",
     "shell.execute_reply": "2025-10-22T19:19:32.885106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply SMOTE + Tomek Links\n",
    "smt = SMOTETomek(random_state=42)\n",
    "X_smt, y_smt = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the class distribution\n",
    "print(f'Resampled dataset shape {X_smt.shape}')\n",
    "print(f'Resampled dataset samples per class {Counter(y_smt)}')\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "smt_model = LogisticRegression(random_state=42)\n",
    "smt_model.fit(X_smt, y_smt)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- SMOTE + Tomek Links Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, smt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8d997",
   "metadata": {},
   "source": [
    "### 7.2 SMOTE + ENN (Edited Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418dbdcb",
   "metadata": {},
   "source": [
    "This method is similar to SMOTE + Tomek Links, but it uses Edited Nearest Neighbors (ENN) for cleaning. ENN removes any instance whose class label differs from the class label of at least two of its three nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21e839",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:32.888094Z",
     "iopub.status.busy": "2025-10-22T19:19:32.887879Z",
     "iopub.status.idle": "2025-10-22T19:19:33.073788Z",
     "shell.execute_reply": "2025-10-22T19:19:33.072950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply SMOTE + ENN\n",
    "sme = SMOTEENN(random_state=42)\n",
    "X_sme, y_sme = sme.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the class distribution\n",
    "print(f'Resampled dataset shape {X_sme.shape}')\n",
    "print(f'Resampled dataset samples per class {Counter(y_sme)}')\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "sme_model = LogisticRegression(random_state=42)\n",
    "sme_model.fit(X_sme, y_sme)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- SMOTE + ENN Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, sme_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e9483",
   "metadata": {},
   "source": [
    "## 8. Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b7f9f",
   "metadata": {},
   "source": [
    "Ensemble methods combine the predictions of several base estimators to improve the overall performance. Some ensemble methods are specifically designed to handle imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d8418",
   "metadata": {},
   "source": [
    "### 8.1 BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0e564",
   "metadata": {},
   "source": [
    "This is a variant of the Random Forest algorithm where each tree is trained on a balanced bootstrap sample of the data. It randomly undersamples the majority class for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec933e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:33.075919Z",
     "iopub.status.busy": "2025-10-22T19:19:33.075702Z",
     "iopub.status.idle": "2025-10-22T19:19:33.451827Z",
     "shell.execute_reply": "2025-10-22T19:19:33.450756Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply BalancedRandomForestClassifier\n",
    "brf = BalancedRandomForestClassifier(random_state=42)\n",
    "brf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- BalancedRandomForestClassifier Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, brf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636263e5",
   "metadata": {},
   "source": [
    "### 8.2 BalancedBaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394cdfdd",
   "metadata": {},
   "source": [
    "This is a bagging classifier that uses a balanced bootstrap sample for each base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbc717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:33.454209Z",
     "iopub.status.busy": "2025-10-22T19:19:33.453953Z",
     "iopub.status.idle": "2025-10-22T19:19:33.636478Z",
     "shell.execute_reply": "2025-10-22T19:19:33.635461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply BalancedBaggingClassifier\n",
    "bbc = BalancedBaggingClassifier(random_state=42)\n",
    "bbc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- BalancedBaggingClassifier Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, bbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b39ebf",
   "metadata": {},
   "source": [
    "### 8.3 EasyEnsembleClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c7493",
   "metadata": {},
   "source": [
    "The EasyEnsemble classifier trains an ensemble of classifiers on different balanced bootstrap samples of the data. It is a form of undersampling-based ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73042eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T19:19:33.638540Z",
     "iopub.status.busy": "2025-10-22T19:19:33.638336Z",
     "iopub.status.idle": "2025-10-22T19:19:34.610830Z",
     "shell.execute_reply": "2025-10-22T19:19:34.609883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply EasyEnsembleClassifier\n",
    "eec = EasyEnsembleClassifier(random_state=42)\n",
    "eec.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- EasyEnsembleClassifier Model Evaluation ---')\n",
    "evaluate_model(X_test, y_test, eec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210247d5",
   "metadata": {},
   "source": [
    "## 9. Final Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80aeeb",
   "metadata": {},
   "source": [
    "After applying various techniques, we can compare their performance to see which one worked best for our synthetic dataset. The choice of the best technique will depend on the specific dataset and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f439ee",
   "metadata": {},
   "source": [
    "## 10. Practical Tips and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d6b53",
   "metadata": {},
   "source": [
    "Here are some practical tips and best practices for handling imbalanced data:\n",
    "\n",
    "*   **Choose the right evaluation metric:** As we've seen, accuracy is not a good metric for imbalanced datasets. Use metrics like Precision, Recall, F1-score, and ROC-AUC.\n",
    "*   **Don't test on the resampled data:** Always resample only the training data and test your model on the original, untouched test set. This prevents data leakage and gives a more realistic estimate of the model's performance on unseen data.\n",
    "*   **Cross-validation:** Use stratified cross-validation to ensure that the class distribution in each fold is representative of the original dataset.\n",
    "*   **Consider class weights:** Many machine learning algorithms have a `class_weight` parameter that can be set to `balanced` to automatically adjust for class imbalance. This is a simple and often effective technique.\n",
    "*   **Experiment with different techniques:** There is no one-size-fits-all solution for imbalanced data. It's important to experiment with different techniques to find the one that works best for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
